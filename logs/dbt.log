[0m13:46:50.526052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d33dd9ab490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d33dd9fa2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d33dd9fa290>]}


============================== 13:46:50.528736 | 3d87c2ab-ff65-4a9e-9558-68916ff51b36 ==============================
[0m13:46:50.528736 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:46:50.529173 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:46:50.536176 [info ] [MainThread]: dbt version: 1.9.4
[0m13:46:50.536545 [info ] [MainThread]: python version: 3.11.11
[0m13:46:50.536805 [info ] [MainThread]: python path: /home/filpill/projects/chess_analysis/dbt/.venv/bin/python3
[0m13:46:50.537052 [info ] [MainThread]: os info: Linux-6.14.5-arch1-1-x86_64-with-glibc2.41
[0m13:46:51.065915 [info ] [MainThread]: Using profiles dir at /home/filpill/.dbt
[0m13:46:51.066296 [info ] [MainThread]: Using profiles.yml file at /home/filpill/.dbt/profiles.yml
[0m13:46:51.066549 [info ] [MainThread]: Using dbt_project.yml file at /home/filpill/projects/chess_analysis/dbt/pipeline/dbt_project.yml
[0m13:46:51.066789 [info ] [MainThread]: adapter type: bigquery
[0m13:46:51.067013 [info ] [MainThread]: adapter version: 1.9.1
[0m13:46:51.141212 [info ] [MainThread]: Configuration:
[0m13:46:51.141576 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:46:51.141820 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:46:51.142053 [info ] [MainThread]: Required dependencies:
[0m13:46:51.142300 [debug] [MainThread]: Executing "git --help"
[0m13:46:51.147558 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:46:51.148470 [debug] [MainThread]: STDERR: "b''"
[0m13:46:51.148839 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:46:51.149203 [info ] [MainThread]: Connection:
[0m13:46:51.149515 [info ] [MainThread]:   method: oauth
[0m13:46:51.149772 [info ] [MainThread]:   database: checkmate-453316
[0m13:46:51.150030 [info ] [MainThread]:   execution_project: checkmate-453316
[0m13:46:51.150287 [info ] [MainThread]:   schema: dbt_filiplivancic
[0m13:46:51.150545 [info ] [MainThread]:   location: EU
[0m13:46:51.150797 [info ] [MainThread]:   priority: interactive
[0m13:46:51.151045 [info ] [MainThread]:   maximum_bytes_billed: None
[0m13:46:51.151292 [info ] [MainThread]:   impersonate_service_account: None
[0m13:46:51.151548 [info ] [MainThread]:   job_retry_deadline_seconds: None
[0m13:46:51.151794 [info ] [MainThread]:   job_retries: 1
[0m13:46:51.152039 [info ] [MainThread]:   job_creation_timeout_seconds: None
[0m13:46:51.152284 [info ] [MainThread]:   job_execution_timeout_seconds: 300
[0m13:46:51.152538 [info ] [MainThread]:   timeout_seconds: 300
[0m13:46:51.152786 [info ] [MainThread]:   client_id: None
[0m13:46:51.153030 [info ] [MainThread]:   token_uri: None
[0m13:46:51.153280 [info ] [MainThread]:   dataproc_region: None
[0m13:46:51.153534 [info ] [MainThread]:   dataproc_cluster_name: None
[0m13:46:51.153779 [info ] [MainThread]:   gcs_bucket: None
[0m13:46:51.154024 [info ] [MainThread]:   dataproc_batch: None
[0m13:46:51.154396 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:46:51.329478 [debug] [MainThread]: Acquiring new bigquery connection 'debug'
[0m13:46:51.329813 [debug] [MainThread]: On debug: select 1 as id
[0m13:46:51.330075 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:46:52.490093 [debug] [MainThread]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:847870fb-90b0-4b44-a4ff-73cb759d02ae&page=queryresults
[0m13:46:52.803071 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:46:52.803720 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:46:52.804479 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 2.3221333, "process_in_blocks": "8", "process_kernel_time": 0.316646, "process_mem_max_rss": "229160", "process_out_blocks": "272", "process_user_time": 2.762961}
[0m13:46:52.804938 [debug] [MainThread]: Command `dbt debug` succeeded at 13:46:52.804837 after 2.32 seconds
[0m13:46:52.805263 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:46:52.805643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d33e1477d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d33e1ae7e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d33a61bb750>]}
[0m13:46:52.806043 [debug] [MainThread]: Flushing usage events
[0m13:46:53.150189 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:54:08.426517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de20a9750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de20fa010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de20f9cd0>]}


============================== 13:54:08.429919 | b45c796b-dcc6-4dbb-97f2-7671d01d63b3 ==============================
[0m13:54:08.429919 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:54:08.431396 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:54:09.081552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8daa1b3f90>]}
[0m13:54:09.134328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de4a49890>]}
[0m13:54:09.134944 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:54:09.257903 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:54:09.258477 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:54:09.258777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8daa25b790>]}
[0m13:54:10.357161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8db0b49c50>]}
[0m13:54:10.420746 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:54:10.422927 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:54:10.437098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8da3c2c090>]}
[0m13:54:10.437529 [info ] [MainThread]: Found 2 models, 4 data tests, 491 macros
[0m13:54:10.437820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8da9db6bd0>]}
[0m13:54:10.439339 [info ] [MainThread]: 
[0m13:54:10.439667 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:54:10.439923 [info ] [MainThread]: 
[0m13:54:10.440342 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:54:10.444292 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:54:10.445025 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:54:11.355879 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dbt_filiplivancic)
[0m13:54:11.356695 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dbt_filiplivancic"
"
[0m13:54:11.371685 [debug] [ThreadPool]: On create_checkmate-453316_dbt_filiplivancic: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dbt_filiplivancic"} */
create schema if not exists `checkmate-453316`.`dbt_filiplivancic`
  
[0m13:54:11.372305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:54:11.757377 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3dbea9ca-8844-4c25-852f-fd6fafafc172&page=queryresults
[0m13:54:14.022970 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dbt_filiplivancic, now list_checkmate-453316_dbt_filiplivancic)
[0m13:54:14.023685 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:54:14.414332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8da9db6bd0>]}
[0m13:54:14.415193 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:54:14.418713 [debug] [Thread-1 (]: Began running node model.pipeline.my_first_dbt_model
[0m13:54:14.419333 [info ] [Thread-1 (]: 1 of 2 START sql table model dbt_filiplivancic.my_first_dbt_model .............. [RUN]
[0m13:54:14.419811 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.my_first_dbt_model)
[0m13:54:14.420191 [debug] [Thread-1 (]: Began compiling node model.pipeline.my_first_dbt_model
[0m13:54:14.426436 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.my_first_dbt_model"
[0m13:54:14.427086 [debug] [Thread-1 (]: Began executing node model.pipeline.my_first_dbt_model
[0m13:54:14.456509 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.my_first_dbt_model"
[0m13:54:14.457268 [debug] [Thread-1 (]: On model.pipeline.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.my_first_dbt_model"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`my_first_dbt_model`
      
    
    

    OPTIONS()
    as (
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
    );
  
[0m13:54:14.457670 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:54:15.464280 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:1c416b22-4773-40ee-a505-b637ee3db914&page=queryresults
[0m13:54:17.429749 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8da31046d0>]}
[0m13:54:17.430520 [info ] [Thread-1 (]: 1 of 2 OK created sql table model dbt_filiplivancic.my_first_dbt_model ......... [[32mCREATE TABLE (2.0 rows, 0 processed)[0m in 3.01s]
[0m13:54:17.431183 [debug] [Thread-1 (]: Finished running node model.pipeline.my_first_dbt_model
[0m13:54:17.432316 [debug] [Thread-3 (]: Began running node model.pipeline.my_second_dbt_model
[0m13:54:17.433092 [info ] [Thread-3 (]: 2 of 2 START sql view model dbt_filiplivancic.my_second_dbt_model .............. [RUN]
[0m13:54:17.433967 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.my_second_dbt_model'
[0m13:54:17.434620 [debug] [Thread-3 (]: Began compiling node model.pipeline.my_second_dbt_model
[0m13:54:17.439499 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.my_second_dbt_model"
[0m13:54:17.440341 [debug] [Thread-3 (]: Began executing node model.pipeline.my_second_dbt_model
[0m13:54:17.472609 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.my_second_dbt_model"
[0m13:54:17.473571 [debug] [Thread-3 (]: On model.pipeline.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.my_second_dbt_model"} */


  create or replace view `checkmate-453316`.`dbt_filiplivancic`.`my_second_dbt_model`
  OPTIONS()
  as -- Use the `ref` function to select from other models

select *
from `checkmate-453316`.`dbt_filiplivancic`.`my_first_dbt_model`
where id = 1;


[0m13:54:17.474259 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:54:18.419330 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7a43def0-eda1-4c5e-9bc5-8a15354dc07d&page=queryresults
[0m13:54:18.999013 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b45c796b-dcc6-4dbb-97f2-7671d01d63b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8da3c65110>]}
[0m13:54:19.000098 [info ] [Thread-3 (]: 2 of 2 OK created sql view model dbt_filiplivancic.my_second_dbt_model ......... [[32mCREATE VIEW (0 processed)[0m in 1.57s]
[0m13:54:19.001114 [debug] [Thread-3 (]: Finished running node model.pipeline.my_second_dbt_model
[0m13:54:19.002958 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:54:19.003876 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:54:19.004324 [debug] [MainThread]: Connection 'model.pipeline.my_first_dbt_model' was properly closed.
[0m13:54:19.004773 [debug] [MainThread]: Connection 'model.pipeline.my_second_dbt_model' was properly closed.
[0m13:54:19.005247 [info ] [MainThread]: 
[0m13:54:19.005767 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 8.57 seconds (8.57s).
[0m13:54:19.006999 [debug] [MainThread]: Command end result
[0m13:54:19.041856 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:54:19.043878 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:54:19.052210 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m13:54:19.052704 [info ] [MainThread]: 
[0m13:54:19.053311 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:54:19.053712 [info ] [MainThread]: 
[0m13:54:19.054062 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:54:19.054739 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.674137, "process_in_blocks": "0", "process_kernel_time": 0.334238, "process_mem_max_rss": "239892", "process_out_blocks": "3184", "process_user_time": 4.194261}
[0m13:54:19.055192 [debug] [MainThread]: Command `dbt run` succeeded at 13:54:19.055092 after 10.67 seconds
[0m13:54:19.055582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de22e9f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de2408310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8de2097ed0>]}
[0m13:54:19.055938 [debug] [MainThread]: Flushing usage events
[0m13:54:19.403505 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:10:31.786273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f627497590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f62748cbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f6276de110>]}


============================== 14:10:31.789442 | d7fa78b4-1073-48ac-bcad-c6dda45c036a ==============================
[0m14:10:31.789442 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:10:31.789958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:10:32.419864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5ef769b90>]}
[0m14:10:32.467454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f629edd790>]}
[0m14:10:32.468005 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:10:32.586649 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:10:32.718251 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:10:32.718662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f627671c10>]}
[0m14:10:33.681338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5ef5f3b10>]}
[0m14:10:33.738643 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:10:33.740713 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:10:33.749071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5ed2afb10>]}
[0m14:10:33.749506 [info ] [MainThread]: Found 2 models, 4 data tests, 491 macros
[0m14:10:33.749781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5ef25d710>]}
[0m14:10:33.751181 [info ] [MainThread]: 
[0m14:10:33.751480 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:10:33.751720 [info ] [MainThread]: 
[0m14:10:33.752095 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:10:33.755771 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:10:33.756476 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:10:34.720735 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dbt_filiplivancic)
[0m14:10:34.721515 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:10:34.880674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5ef289bd0>]}
[0m14:10:34.881376 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:10:34.884808 [debug] [Thread-1 (]: Began running node model.pipeline.my_first_dbt_model
[0m14:10:34.885653 [info ] [Thread-1 (]: 1 of 2 START sql table model dbt_filiplivancic.my_first_dbt_model .............. [RUN]
[0m14:10:34.886189 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.my_first_dbt_model)
[0m14:10:34.886599 [debug] [Thread-1 (]: Began compiling node model.pipeline.my_first_dbt_model
[0m14:10:34.894248 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.my_first_dbt_model"
[0m14:10:34.894763 [debug] [Thread-1 (]: Began executing node model.pipeline.my_first_dbt_model
[0m14:10:34.906848 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:35.185341 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.my_first_dbt_model"
[0m14:10:35.186373 [debug] [Thread-1 (]: On model.pipeline.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.my_first_dbt_model"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`my_first_dbt_model`
      
    
    

    OPTIONS()
    as (
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
    );
  
[0m14:10:35.711166 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f12a5c8f-1089-4b44-b363-3bcf2f5a9fb7&page=queryresults
[0m14:10:37.554317 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5ef2e15d0>]}
[0m14:10:37.557007 [info ] [Thread-1 (]: 1 of 2 OK created sql table model dbt_filiplivancic.my_first_dbt_model ......... [[32mCREATE TABLE (2.0 rows, 0 processed)[0m in 2.67s]
[0m14:10:37.558099 [debug] [Thread-1 (]: Finished running node model.pipeline.my_first_dbt_model
[0m14:10:37.559499 [debug] [Thread-3 (]: Began running node model.pipeline.my_second_dbt_model
[0m14:10:37.560317 [info ] [Thread-3 (]: 2 of 2 START sql table model dbt_filiplivancic.my_second_dbt_model ............. [RUN]
[0m14:10:37.561170 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.my_second_dbt_model'
[0m14:10:37.561736 [debug] [Thread-3 (]: Began compiling node model.pipeline.my_second_dbt_model
[0m14:10:37.566025 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.my_second_dbt_model"
[0m14:10:37.566814 [debug] [Thread-3 (]: Began executing node model.pipeline.my_second_dbt_model
[0m14:10:37.570706 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:10:37.834990 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.my_second_dbt_model"
[0m14:10:37.835937 [debug] [Thread-3 (]: On model.pipeline.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.my_second_dbt_model"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`my_second_dbt_model`
      
    
    

    OPTIONS()
    as (
      -- Use the `ref` function to select from other models

select *
from `checkmate-453316`.`dbt_filiplivancic`.`my_first_dbt_model`
where id = 1
    );
  
[0m14:10:38.185531 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a9dfc8e2-5bb5-4be0-be49-9b8854615f65&page=queryresults
[0m14:10:39.711060 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd7fa78b4-1073-48ac-bcad-c6dda45c036a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f5e6d97a10>]}
[0m14:10:39.712072 [info ] [Thread-3 (]: 2 of 2 OK created sql table model dbt_filiplivancic.my_second_dbt_model ........ [[32mCREATE TABLE (1.0 rows, 8.0 Bytes processed)[0m in 2.15s]
[0m14:10:39.712982 [debug] [Thread-3 (]: Finished running node model.pipeline.my_second_dbt_model
[0m14:10:39.714731 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:10:39.715393 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:10:39.715718 [debug] [MainThread]: Connection 'model.pipeline.my_first_dbt_model' was properly closed.
[0m14:10:39.716014 [debug] [MainThread]: Connection 'model.pipeline.my_second_dbt_model' was properly closed.
[0m14:10:39.716385 [info ] [MainThread]: 
[0m14:10:39.716735 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 5.96 seconds (5.96s).
[0m14:10:39.717422 [debug] [MainThread]: Command end result
[0m14:10:39.739922 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:10:39.741057 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:10:39.748458 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:10:39.748976 [info ] [MainThread]: 
[0m14:10:39.749559 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:10:39.750015 [info ] [MainThread]: 
[0m14:10:39.750515 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:10:39.751452 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.007074, "process_in_blocks": "0", "process_kernel_time": 0.336259, "process_mem_max_rss": "238744", "process_out_blocks": "3064", "process_user_time": 4.095342}
[0m14:10:39.752087 [debug] [MainThread]: Command `dbt run` succeeded at 14:10:39.751943 after 8.01 seconds
[0m14:10:39.752635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f627483f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f62b517d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f62af17cd0>]}
[0m14:10:39.753171 [debug] [MainThread]: Flushing usage events
[0m14:10:40.113478 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:38:35.746493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e28914d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e28e3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e28e2290>]}


============================== 14:38:35.748984 | d0ba4366-446b-486e-8bbb-b5fb9e0a0232 ==============================
[0m14:38:35.748984 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:38:35.749425 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:38:36.370731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541ab285850>]}
[0m14:38:36.418836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e2f9fc90>]}
[0m14:38:36.419400 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:38:36.539533 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:38:36.685830 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 1 files added, 1 files changed.
[0m14:38:36.686266 [debug] [MainThread]: Partial parsing: added file: pipeline://models/calendar.sql
[0m14:38:36.686696 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m14:38:36.686936 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/my_second_dbt_model.sql
[0m14:38:36.687163 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/my_first_dbt_model.sql
[0m14:38:36.940667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541aaa56d90>]}
[0m14:38:36.987348 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:38:36.989502 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:38:36.997914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541a83946d0>]}
[0m14:38:36.998324 [info ] [MainThread]: Found 1 model, 491 macros
[0m14:38:36.998603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541a83b6fd0>]}
[0m14:38:36.999775 [info ] [MainThread]: 
[0m14:38:37.000053 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:38:37.000288 [info ] [MainThread]: 
[0m14:38:37.000675 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:38:37.001793 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:38:37.002478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:37.959321 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dbt_filiplivancic)
[0m14:38:37.960106 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:38:38.125390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541a83b70d0>]}
[0m14:38:38.125909 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:38.128803 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:38:38.129701 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_filiplivancic.calendar ........................ [RUN]
[0m14:38:38.130485 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.calendar)
[0m14:38:38.131136 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:38:38.144046 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:38:38.144888 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:38:38.181158 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:38:38.181880 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`calendar`
      
    
    

    OPTIONS()
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number  
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1 
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:38:38.182348 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:38.670393 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:70e207c6-b816-4f7d-8364-fad2296a11cd&page=queryresults
[0m14:38:40.571641 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0ba4366-446b-486e-8bbb-b5fb9e0a0232', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541a826c0d0>]}
[0m14:38:40.572426 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_filiplivancic.calendar ................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.44s]
[0m14:38:40.573082 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:38:40.574809 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:38:40.575858 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:38:40.576318 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:38:40.576848 [info ] [MainThread]: 
[0m14:38:40.577372 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.58 seconds (3.58s).
[0m14:38:40.578355 [debug] [MainThread]: Command end result
[0m14:38:40.613215 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:38:40.615254 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:38:40.623501 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:38:40.624045 [info ] [MainThread]: 
[0m14:38:40.624870 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:38:40.625252 [info ] [MainThread]: 
[0m14:38:40.625647 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:38:40.626379 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.925422, "process_in_blocks": "0", "process_kernel_time": 0.325718, "process_mem_max_rss": "237252", "process_out_blocks": "3032", "process_user_time": 3.39048}
[0m14:38:40.626841 [debug] [MainThread]: Command `dbt run` succeeded at 14:38:40.626741 after 4.93 seconds
[0m14:38:40.627227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e28bd650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e35a2f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7541e288b590>]}
[0m14:38:40.627614 [debug] [MainThread]: Flushing usage events
[0m14:38:41.010125 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:42:20.442630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845b895b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845bade110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845bb97d90>]}


============================== 14:42:20.445838 | 5ff4d416-0f14-481f-9d2f-d56a81803185 ==============================
[0m14:42:20.445838 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:42:20.446331 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:42:21.085973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845b914e10>]}
[0m14:42:21.134626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845e2cd750>]}
[0m14:42:21.135195 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:42:21.255556 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:42:21.394600 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:42:21.395126 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/calendar.sql
[0m14:42:21.655752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758423bcf910>]}
[0m14:42:21.702303 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:42:21.704463 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:42:21.712642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75842142ae50>]}
[0m14:42:21.713063 [info ] [MainThread]: Found 1 model, 491 macros
[0m14:42:21.713353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7584237243d0>]}
[0m14:42:21.714631 [info ] [MainThread]: 
[0m14:42:21.714935 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:42:21.715174 [info ] [MainThread]: 
[0m14:42:21.715575 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:42:21.716748 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:42:21.717423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:42:22.596019 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dbt_filiplivancic)
[0m14:42:22.596768 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:42:22.760665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758423694e90>]}
[0m14:42:22.761182 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:42:22.764000 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:42:22.764681 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_filiplivancic.calendar ........................ [RUN]
[0m14:42:22.765196 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.calendar)
[0m14:42:22.765589 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:42:22.773619 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:42:22.774167 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:42:22.786761 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:42:23.009786 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:42:23.011074 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`calendar`
      
    
    

    OPTIONS()
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:42:23.622598 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:8e468032-aa9f-4721-b8f7-c2c931340029&page=queryresults
[0m14:42:25.966518 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ff4d416-0f14-481f-9d2f-d56a81803185', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758421499ad0>]}
[0m14:42:25.967916 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_filiplivancic.calendar ................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.20s]
[0m14:42:25.968983 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:42:25.970915 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:42:25.971988 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:42:25.972536 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:42:25.972965 [info ] [MainThread]: 
[0m14:42:25.973425 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.26 seconds (4.26s).
[0m14:42:25.974250 [debug] [MainThread]: Command end result
[0m14:42:25.994572 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:42:25.995807 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:42:26.000766 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:42:26.001062 [info ] [MainThread]: 
[0m14:42:26.001422 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:42:26.001686 [info ] [MainThread]: 
[0m14:42:26.001968 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:42:26.002553 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.60809, "process_in_blocks": "0", "process_kernel_time": 0.339006, "process_mem_max_rss": "237712", "process_out_blocks": "3024", "process_user_time": 3.404215}
[0m14:42:26.002928 [debug] [MainThread]: Command `dbt run` succeeded at 14:42:26.002845 after 5.61 seconds
[0m14:42:26.003261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845b8e7050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845b8ee650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75845b883c10>]}
[0m14:42:26.003590 [debug] [MainThread]: Flushing usage events
[0m14:42:26.361025 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:59:18.538267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b101c94510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b101dbb490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b101eda490>]}


============================== 15:59:18.540801 | 97117fe9-2fc7-4bac-9822-640a16b6d408 ==============================
[0m15:59:18.540801 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:59:18.541251 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --full-refresh', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:59:19.174122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b0ca0dea10>]}
[0m15:59:19.225912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b1046cd690>]}
[0m15:59:19.226535 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:59:19.346995 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:59:19.483537 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:59:19.483907 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:59:19.508658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b0c9b329d0>]}
[0m15:59:19.561870 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:59:19.564166 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:59:19.572862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b0c9c43f10>]}
[0m15:59:19.573304 [info ] [MainThread]: Found 1 model, 491 macros
[0m15:59:19.573609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b10411aa50>]}
[0m15:59:19.574874 [info ] [MainThread]: 
[0m15:59:19.575169 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:59:19.575437 [info ] [MainThread]: 
[0m15:59:19.575866 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:59:19.577001 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:59:19.577589 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:20.551673 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dbt_filiplivancic)
[0m15:59:20.552288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:59:20.725944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b0c9f9ef10>]}
[0m15:59:20.726621 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:59:20.730238 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m15:59:20.731032 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_filiplivancic.calendar ........................ [RUN]
[0m15:59:20.731581 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.calendar)
[0m15:59:20.731965 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m15:59:20.740464 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m15:59:20.740997 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m15:59:20.753817 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:59:20.954635 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m15:59:20.957881 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`calendar`
      
    
    

    OPTIONS()
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m15:59:21.455461 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:fc27de49-3a39-48b3-92a8-3c3e89071811&page=queryresults
[0m15:59:23.339739 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97117fe9-2fc7-4bac-9822-640a16b6d408', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b0c9b30990>]}
[0m15:59:23.340938 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_filiplivancic.calendar ................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.61s]
[0m15:59:23.341979 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m15:59:23.343966 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:59:23.344898 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:59:23.345346 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m15:59:23.345833 [info ] [MainThread]: 
[0m15:59:23.346322 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.77 seconds (3.77s).
[0m15:59:23.347305 [debug] [MainThread]: Command end result
[0m15:59:23.379952 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:59:23.381953 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:59:23.390959 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:59:23.391487 [info ] [MainThread]: 
[0m15:59:23.392091 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:59:23.392568 [info ] [MainThread]: 
[0m15:59:23.393068 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:59:23.394012 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.8983884, "process_in_blocks": "0", "process_kernel_time": 0.340249, "process_mem_max_rss": "232344", "process_out_blocks": "2024", "process_user_time": 3.189801}
[0m15:59:23.394660 [debug] [MainThread]: Command `dbt run` succeeded at 15:59:23.394523 after 4.90 seconds
[0m15:59:23.395034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b101cec7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b101ff84d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77b101c83e50>]}
[0m15:59:23.395417 [debug] [MainThread]: Flushing usage events
[0m15:59:23.772268 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:09:59.070850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77713cf80650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77713cf81350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77713cf80290>]}


============================== 16:09:59.073362 | 31d32302-757f-4be0-9856-e77031704dd1 ==============================
[0m16:09:59.073362 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:09:59.073814 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:09:59.703213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777139b38a90>]}
[0m16:09:59.751435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77713f975b50>]}
[0m16:09:59.752028 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:09:59.875860 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:09:59.998084 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m16:09:59.998499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7771098a6b90>]}
[0m16:10:00.884928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7771098cbe50>]}
[0m16:10:00.930633 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:10:00.932818 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:10:00.941233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7770eed711d0>]}
[0m16:10:00.941656 [info ] [MainThread]: Found 1 model, 491 macros
[0m16:10:00.941943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777108d02990>]}
[0m16:10:00.943124 [info ] [MainThread]: 
[0m16:10:00.943404 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:10:00.943659 [info ] [MainThread]: 
[0m16:10:00.944050 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:10:00.945201 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:10:00.945898 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:10:02.446548 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dbt_filiplivancic)
[0m16:10:02.447256 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:10:02.641056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777108f3a310>]}
[0m16:10:02.641790 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:10:02.645345 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:10:02.646218 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_filiplivancic.calendar ........................ [RUN]
[0m16:10:02.646947 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.calendar)
[0m16:10:02.647521 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:10:02.659713 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:10:02.660620 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:10:02.675495 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:10:02.888604 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m16:10:02.889875 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m16:10:03.373068 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:fe5f4954-8334-4b06-b09b-a39888cea875&page=queryresults
[0m16:10:05.584004 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31d32302-757f-4be0-9856-e77031704dd1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777108de4390>]}
[0m16:10:05.585210 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_filiplivancic.calendar ................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.94s]
[0m16:10:05.586288 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:10:05.588246 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:10:05.589190 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:10:05.589671 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m16:10:05.590147 [info ] [MainThread]: 
[0m16:10:05.590645 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.65 seconds (4.65s).
[0m16:10:05.591644 [debug] [MainThread]: Command end result
[0m16:10:05.624797 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:10:05.626811 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:10:05.635104 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:10:05.635539 [info ] [MainThread]: 
[0m16:10:05.635956 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:10:05.636271 [info ] [MainThread]: 
[0m16:10:05.636624 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:10:05.637296 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.6116366, "process_in_blocks": "0", "process_kernel_time": 0.333825, "process_mem_max_rss": "236964", "process_out_blocks": "3024", "process_user_time": 4.0142}
[0m16:10:05.637740 [debug] [MainThread]: Command `dbt run` succeeded at 16:10:05.637657 after 6.61 seconds
[0m16:10:05.638024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77713cf7bc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77713cf6ff90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x777140977ed0>]}
[0m16:10:05.638310 [debug] [MainThread]: Flushing usage events
[0m16:10:06.004945 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:32:46.211025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4e997810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4e9978d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4e994ad0>]}


============================== 16:32:46.213753 | 05fe3ef9-d054-49c4-a8fe-6820db475c2c ==============================
[0m16:32:46.213753 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:32:46.214195 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:32:46.837227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4af02350>]}
[0m16:32:46.886198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a51375a90>]}
[0m16:32:46.886782 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:32:47.007803 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:32:47.145159 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m16:32:47.145615 [debug] [MainThread]: Partial parsing: added file: pipeline://models/test.sql
[0m16:32:47.145997 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m16:32:47.491665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a146a2650>]}
[0m16:32:47.550956 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:32:47.553069 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:32:47.562107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a14754d50>]}
[0m16:32:47.562563 [info ] [MainThread]: Found 2 models, 1 source, 491 macros
[0m16:32:47.562838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a147164d0>]}
[0m16:32:47.564218 [info ] [MainThread]: 
[0m16:32:47.564502 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:32:47.564745 [info ] [MainThread]: 
[0m16:32:47.565138 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:32:47.568869 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:32:47.569511 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:32:48.581166 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dbt_filiplivancic)
[0m16:32:48.581689 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:32:48.781495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a166fd990>]}
[0m16:32:48.782204 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:32:48.785662 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:32:48.786181 [debug] [Thread-2 (]: Began running node model.pipeline.test
[0m16:32:48.786769 [info ] [Thread-1 (]: 1 of 2 START sql table model dbt_filiplivancic.calendar ........................ [RUN]
[0m16:32:48.787304 [info ] [Thread-2 (]: 2 of 2 START sql table model dbt_filiplivancic.test ............................ [RUN]
[0m16:32:48.787814 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic, now model.pipeline.calendar)
[0m16:32:48.788322 [debug] [Thread-2 (]: Acquiring new bigquery connection 'model.pipeline.test'
[0m16:32:48.788694 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:32:48.789037 [debug] [Thread-2 (]: Began compiling node model.pipeline.test
[0m16:32:48.799067 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:32:48.801558 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.test"
[0m16:32:48.802064 [debug] [Thread-2 (]: Began executing node model.pipeline.test
[0m16:32:48.802376 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:32:48.825376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:32:48.839009 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.test"
[0m16:32:48.839567 [debug] [Thread-2 (]: On model.pipeline.test: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`test`
      
    
    

    OPTIONS(
      description="""test table"""
    )
    as (
      

SELECT DISTINCT opening FROM `checkmate-453316`.`chess_raw`.`games`
    );
  
[0m16:32:48.839894 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:32:49.021394 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m16:32:49.022626 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m16:32:49.417138 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f659a584-d0c8-4681-838f-193681d78353&page=queryresults
[0m16:32:49.510367 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c6084f38-38f5-4abc-9a34-1d18a2af67dc&page=queryresults
[0m16:32:51.698068 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a165e8890>]}
[0m16:32:51.698799 [info ] [Thread-1 (]: 1 of 2 OK created sql table model dbt_filiplivancic.calendar ................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.91s]
[0m16:32:51.699442 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:32:51.789986 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05fe3ef9-d054-49c4-a8fe-6820db475c2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a14582450>]}
[0m16:32:51.791036 [info ] [Thread-2 (]: 2 of 2 OK created sql table model dbt_filiplivancic.test ....................... [[32mCREATE TABLE (37.5k rows, 66.2 MiB processed)[0m in 3.00s]
[0m16:32:51.792049 [debug] [Thread-2 (]: Finished running node model.pipeline.test
[0m16:32:51.793825 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:32:51.794723 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:32:51.795196 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m16:32:51.795676 [debug] [MainThread]: Connection 'model.pipeline.test' was properly closed.
[0m16:32:51.796252 [info ] [MainThread]: 
[0m16:32:51.796807 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 4.23 seconds (4.23s).
[0m16:32:51.798116 [debug] [MainThread]: Command end result
[0m16:32:51.833153 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:32:51.835180 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:32:51.842130 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:32:51.842515 [info ] [MainThread]: 
[0m16:32:51.842903 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:32:51.843247 [info ] [MainThread]: 
[0m16:32:51.843607 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:32:51.844274 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.6770287, "process_in_blocks": "0", "process_kernel_time": 0.346111, "process_mem_max_rss": "239448", "process_out_blocks": "3056", "process_user_time": 3.544898}
[0m16:32:51.844725 [debug] [MainThread]: Command `dbt run` succeeded at 16:32:51.844631 after 5.68 seconds
[0m16:32:51.845025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4e98c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4e9e7b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748a4ec97d90>]}
[0m16:32:51.845306 [debug] [MainThread]: Flushing usage events
[0m16:32:52.214305 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:57:31.825881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a185438bc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a185438aa50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1854330b10>]}


============================== 16:57:31.828953 | 8c408934-32ae-40dc-8f44-e85e60e3bf12 ==============================
[0m16:57:31.828953 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:57:31.829389 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ls --select pipeline --output path', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:57:32.474036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8c408934-32ae-40dc-8f44-e85e60e3bf12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1820c25fd0>]}
[0m16:57:32.522914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8c408934-32ae-40dc-8f44-e85e60e3bf12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1856d75a10>]}
[0m16:57:32.523484 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:57:32.644768 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:57:32.775677 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m16:57:32.776076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8c408934-32ae-40dc-8f44-e85e60e3bf12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a185547fd50>]}
[0m16:57:33.601286 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m16:57:33.734395 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.pipeline.aggregate
- models.pipeline.staging
[0m16:57:33.743505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8c408934-32ae-40dc-8f44-e85e60e3bf12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1820333010>]}
[0m16:57:33.799118 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:57:33.801199 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:57:33.809437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8c408934-32ae-40dc-8f44-e85e60e3bf12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1819f4bb50>]}
[0m16:57:33.809850 [info ] [MainThread]: Found 1 model, 1 source, 491 macros
[0m16:57:33.810126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8c408934-32ae-40dc-8f44-e85e60e3bf12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a181a02ab10>]}
[0m16:57:33.810567 [info ] [MainThread]: models/universal/calendar.sql
[0m16:57:33.811089 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 2.02765, "process_in_blocks": "0", "process_kernel_time": 0.334654, "process_mem_max_rss": "231368", "process_out_blocks": "2032", "process_user_time": 3.88336}
[0m16:57:33.811462 [debug] [MainThread]: Command `dbt ls` succeeded at 16:57:33.811358 after 2.03 seconds
[0m16:57:33.811759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1854330310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a1854330b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a18201402d0>]}
[0m16:57:33.812046 [debug] [MainThread]: Flushing usage events
[0m16:57:34.188396 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:57:50.401672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919ee7d3810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919ee7808d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919ee781510>]}


============================== 16:57:50.404047 | bbed5c90-6d09-4d2c-84b3-9086018c2015 ==============================
[0m16:57:50.404047 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:57:50.404506 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ls --select pipeline --output name', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:57:51.026212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bbed5c90-6d09-4d2c-84b3-9086018c2015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919b6a5b750>]}
[0m16:57:51.073959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bbed5c90-6d09-4d2c-84b3-9086018c2015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919f1175b50>]}
[0m16:57:51.074525 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:57:51.196449 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:57:51.344026 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:57:51.344366 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:57:51.349065 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.pipeline.aggregate
- models.pipeline.staging
[0m16:57:51.374430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bbed5c90-6d09-4d2c-84b3-9086018c2015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919eee938d0>]}
[0m16:57:51.434485 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:57:51.436658 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:57:51.443844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bbed5c90-6d09-4d2c-84b3-9086018c2015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919ee78d990>]}
[0m16:57:51.444247 [info ] [MainThread]: Found 1 model, 1 source, 491 macros
[0m16:57:51.444550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bbed5c90-6d09-4d2c-84b3-9086018c2015', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919eeef0910>]}
[0m16:57:51.444974 [info ] [MainThread]: calendar
[0m16:57:51.445492 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 1.0879446, "process_in_blocks": "0", "process_kernel_time": 0.331686, "process_mem_max_rss": "225228", "process_out_blocks": "1000", "process_user_time": 2.941266}
[0m16:57:51.445950 [debug] [MainThread]: Command `dbt ls` succeeded at 16:57:51.445869 after 1.09 seconds
[0m16:57:51.446241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919ee7784d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919f2177e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7919ee78e810>]}
[0m16:57:51.446527 [debug] [MainThread]: Flushing usage events
[0m16:57:51.790189 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:58:12.597782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c0f19bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c0f190850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c0f190e90>]}


============================== 16:58:12.600912 | 922d4cc2-c751-4739-a518-7104dd7987d5 ==============================
[0m16:58:12.600912 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:58:12.601397 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ls --select pipeline --output json', 'send_anonymous_usage_stats': 'True'}
[0m16:58:13.228713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '922d4cc2-c751-4739-a518-7104dd7987d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4bd73ef490>]}
[0m16:58:13.276218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '922d4cc2-c751-4739-a518-7104dd7987d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c11bcdd50>]}
[0m16:58:13.276771 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:58:13.393840 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:58:13.537789 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:58:13.538118 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:58:13.542667 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.pipeline.staging
- models.pipeline.aggregate
[0m16:58:13.567055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '922d4cc2-c751-4739-a518-7104dd7987d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c1001c510>]}
[0m16:58:13.626549 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:58:13.628701 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:58:13.635885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '922d4cc2-c751-4739-a518-7104dd7987d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4bd704b590>]}
[0m16:58:13.636287 [info ] [MainThread]: Found 1 model, 1 source, 491 macros
[0m16:58:13.636588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '922d4cc2-c751-4739-a518-7104dd7987d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4bd73feb10>]}
[0m16:58:13.637143 [info ] [MainThread]: {"name": "calendar", "resource_type": "model", "package_name": "pipeline", "original_file_path": "models/universal/calendar.sql", "unique_id": "model.pipeline.calendar", "alias": "calendar", "config": {"enabled": true, "alias": null, "schema": "dev_universal", "database": null, "tags": [], "meta": {}, "group": null, "materialized": "table", "incremental_strategy": null, "batch_size": null, "lookback": 1, "begin": null, "persist_docs": {"relation": true, "columns": true}, "post-hook": [], "pre-hook": [], "quoting": {}, "column_types": {}, "full_refresh": null, "unique_key": null, "on_schema_change": "ignore", "on_configuration_change": "apply", "grants": {}, "packages": [], "docs": {"show": true, "node_color": null}, "contract": {"enforced": false, "alias_types": true}, "event_time": null, "concurrent_batches": null, "access": "protected"}, "tags": [], "depends_on": {"macros": [], "nodes": []}}
[0m16:58:13.637741 [debug] [MainThread]: Resource report: {"command_name": "list", "command_success": true, "command_wall_clock_time": 1.082136, "process_in_blocks": "0", "process_kernel_time": 0.306129, "process_mem_max_rss": "225164", "process_out_blocks": "1008", "process_user_time": 2.956356}
[0m16:58:13.638214 [debug] [MainThread]: Command `dbt ls` succeeded at 16:58:13.638130 after 1.08 seconds
[0m16:58:13.638515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c12dca1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c0f187fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a4c0f1cc790>]}
[0m16:58:13.638801 [debug] [MainThread]: Flushing usage events
[0m16:58:13.992161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:14:38.109934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a579b090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a579be90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a579b3d0>]}


============================== 17:14:38.113059 | 70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b ==============================
[0m17:14:38.113059 [info ] [MainThread]: Running with dbt=1.9.4
[0m17:14:38.113508 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:14:38.748414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046d93f1d0>]}
[0m17:14:38.796555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a81cd6d0>]}
[0m17:14:38.797120 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m17:14:38.916866 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m17:14:39.046890 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m17:14:39.047291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046d64e850>]}
[0m17:14:39.871574 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m17:14:40.005755 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m17:14:40.014985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046daa26d0>]}
[0m17:14:40.071975 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m17:14:40.074087 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m17:14:40.082432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72044733af50>]}
[0m17:14:40.082886 [info ] [MainThread]: Found 2 models, 1 source, 491 macros
[0m17:14:40.083190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204475f0e50>]}
[0m17:14:40.084673 [info ] [MainThread]: 
[0m17:14:40.084998 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:14:40.085264 [info ] [MainThread]: 
[0m17:14:40.085697 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:14:40.089454 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m17:14:40.090362 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m17:14:40.090929 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:14:40.091515 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:14:41.241021 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dbt_filiplivancic_dbt_dev_staging)
[0m17:14:41.241722 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dbt_filiplivancic_dbt_dev_universal)
[0m17:14:41.242372 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dbt_filiplivancic_dbt_dev_staging"
"
[0m17:14:41.242923 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dbt_filiplivancic_dbt_dev_universal"
"
[0m17:14:41.261304 [debug] [ThreadPool]: On create_checkmate-453316_dbt_filiplivancic_dbt_dev_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dbt_filiplivancic_dbt_dev_staging"} */
create schema if not exists `checkmate-453316`.`dbt_filiplivancic_dbt_dev_staging`
  
[0m17:14:41.268137 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:14:41.268929 [debug] [ThreadPool]: On create_checkmate-453316_dbt_filiplivancic_dbt_dev_universal: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dbt_filiplivancic_dbt_dev_universal"} */
create schema if not exists `checkmate-453316`.`dbt_filiplivancic_dbt_dev_universal`
  
[0m17:14:41.270171 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:14:41.732701 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6b1f2f33-f8a2-4abb-8655-2fb72069494b&page=queryresults
[0m17:14:41.745347 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3daba9c4-3114-40ce-a1a6-01e3d18404c1&page=queryresults
[0m17:14:45.766749 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dbt_filiplivancic_dbt_dev_universal, now list_checkmate-453316_dbt_filiplivancic_dbt_dev_staging)
[0m17:14:45.767612 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dbt_filiplivancic_dbt_dev_staging, now list_checkmate-453316_dbt_filiplivancic_dbt_dev_universal)
[0m17:14:45.768198 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:14:45.768670 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:14:46.168347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72046d5b59d0>]}
[0m17:14:46.169112 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:14:46.172594 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m17:14:46.173358 [info ] [Thread-1 (]: 1 of 2 START sql table model dbt_filiplivancic_dbt_dev_universal.calendar ...... [RUN]
[0m17:14:46.174048 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dbt_filiplivancic_dbt_dev_universal, now model.pipeline.calendar)
[0m17:14:46.174636 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m17:14:46.186450 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m17:14:46.187423 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m17:14:46.217338 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m17:14:46.218040 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic_dbt_dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m17:14:46.218494 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:14:47.217587 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:5e40df3e-8312-49da-ad4d-546f2f8f6a3a&page=queryresults
[0m17:14:50.267753 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204460a0a90>]}
[0m17:14:50.268766 [info ] [Thread-1 (]: 1 of 2 OK created sql table model dbt_filiplivancic_dbt_dev_universal.calendar . [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 4.09s]
[0m17:14:50.269731 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m17:14:50.270845 [debug] [Thread-3 (]: Began running node model.pipeline.player_games
[0m17:14:50.271967 [info ] [Thread-3 (]: 2 of 2 START sql table model dbt_filiplivancic_dbt_dev_staging.player_games .... [RUN]
[0m17:14:50.272804 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.player_games'
[0m17:14:50.273388 [debug] [Thread-3 (]: Began compiling node model.pipeline.player_games
[0m17:14:50.279190 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.player_games"
[0m17:14:50.280155 [debug] [Thread-3 (]: Began executing node model.pipeline.player_games
[0m17:14:50.284392 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.player_games"
[0m17:14:50.285713 [debug] [Thread-3 (]: On model.pipeline.player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.player_games"} */

  
    

    create or replace table `checkmate-453316`.`dbt_filiplivancic_dbt_dev_staging`.`player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316.chess_raw.games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dbt_filiplivancic_dbt_dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      AND cal_date BETWEEN current_date-7 and current_date

)

SELECT * FROM cte_base
    );
  
[0m17:14:50.286734 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m17:14:51.323962 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ba20c127-b67b-4eba-8a3d-71b2be2206a0&page=queryresults
[0m17:14:53.471862 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '70b5c79f-d90a-4b9a-91ac-2dcb12fd3f5b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x720446040b50>]}
[0m17:14:53.472820 [info ] [Thread-3 (]: 2 of 2 OK created sql table model dbt_filiplivancic_dbt_dev_staging.player_games  [[32mCREATE TABLE (0.0 rows, 85.6 KiB processed)[0m in 3.20s]
[0m17:14:53.473743 [debug] [Thread-3 (]: Finished running node model.pipeline.player_games
[0m17:14:53.475457 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:14:53.476303 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:14:53.476754 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m17:14:53.477177 [debug] [MainThread]: Connection 'list_checkmate-453316_dbt_filiplivancic_dbt_dev_staging' was properly closed.
[0m17:14:53.477648 [debug] [MainThread]: Connection 'model.pipeline.player_games' was properly closed.
[0m17:14:53.478474 [info ] [MainThread]: 
[0m17:14:53.479103 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 13.39 seconds (13.39s).
[0m17:14:53.480392 [debug] [MainThread]: Command end result
[0m17:14:53.515938 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m17:14:53.517922 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m17:14:53.526048 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m17:14:53.526446 [info ] [MainThread]: 
[0m17:14:53.526843 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:14:53.527164 [info ] [MainThread]: 
[0m17:14:53.527515 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:14:53.528058 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.467207, "process_in_blocks": "0", "process_kernel_time": 0.361077, "process_mem_max_rss": "239576", "process_out_blocks": "3112", "process_user_time": 4.184766}
[0m17:14:53.528392 [debug] [MainThread]: Command `dbt run` succeeded at 17:14:53.528317 after 15.47 seconds
[0m17:14:53.528686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a59e5e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a5793f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7204a93ffd90>]}
[0m17:14:53.528964 [debug] [MainThread]: Flushing usage events
[0m17:14:53.898814 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:26:16.757877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eaa23983750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eaa23983990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eaa239802d0>]}


============================== 17:26:16.760930 | fd7c7a2b-e0a4-4402-8254-35aeadd3977a ==============================
[0m17:26:16.760930 [info ] [MainThread]: Running with dbt=1.9.4
[0m17:26:16.761381 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --target prod', 'send_anonymous_usage_stats': 'True'}
[0m17:26:16.764939 [error] [MainThread]: Encountered an error:
Runtime Error
  The profile 'pipeline' does not have a target named 'prod'. The valid target names for this profile are:
   - dev
[0m17:26:16.765608 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.051830277, "process_in_blocks": "0", "process_kernel_time": 0.180459, "process_mem_max_rss": "104608", "process_out_blocks": "16", "process_user_time": 1.004157}
[0m17:26:16.765969 [debug] [MainThread]: Command `dbt run` failed at 17:26:16.765886 after 0.05 seconds
[0m17:26:16.766278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eaa23981610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eaa23978250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7eaa239d8ed0>]}
[0m17:26:16.766625 [debug] [MainThread]: Flushing usage events
[0m17:26:17.130160 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:27:32.315483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53aedda90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53ac8c890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53ac8f9d0>]}


============================== 18:27:32.318448 | c2eb1829-b3db-4c12-b4bd-55595765a5e9 ==============================
[0m18:27:32.318448 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:27:32.318931 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m18:27:32.974981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53bb04190>]}
[0m18:27:33.021888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53d6cd750>]}
[0m18:27:33.022454 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:27:33.140084 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:27:33.269684 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m18:27:33.270094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53be8b990>]}
[0m18:27:34.166969 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m18:27:34.314626 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m18:27:34.324782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db502b0d450>]}
[0m18:27:34.386505 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:27:34.388828 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:27:34.397646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db5008c0d10>]}
[0m18:27:34.398098 [info ] [MainThread]: Found 2 models, 1 source, 491 macros
[0m18:27:34.398390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db5009e59d0>]}
[0m18:27:34.399851 [info ] [MainThread]: 
[0m18:27:34.400148 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:27:34.400426 [info ] [MainThread]: 
[0m18:27:34.400829 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:27:34.404715 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:27:34.405456 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:27:34.405864 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:27:34.406347 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:27:35.743046 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_fallback_unused_dbt_dev_staging)
[0m18:27:35.743752 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_fallback_unused_dbt_dev_universal)
[0m18:27:35.744561 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "fallback_unused_dbt_dev_staging"
"
[0m18:27:35.745271 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "fallback_unused_dbt_dev_universal"
"
[0m18:27:35.759837 [debug] [ThreadPool]: On create_checkmate-453316_fallback_unused_dbt_dev_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_fallback_unused_dbt_dev_staging"} */
create schema if not exists `checkmate-453316`.`fallback_unused_dbt_dev_staging`
  
[0m18:27:35.765014 [debug] [ThreadPool]: On create_checkmate-453316_fallback_unused_dbt_dev_universal: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_fallback_unused_dbt_dev_universal"} */
create schema if not exists `checkmate-453316`.`fallback_unused_dbt_dev_universal`
  
[0m18:27:35.765737 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:27:35.766306 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:27:36.105135 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3e22f871-4599-4860-8a7d-03b462f90ac7&page=queryresults
[0m18:27:36.267925 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:80710373-891d-41be-a518-91006d1b707a&page=queryresults
[0m18:27:39.962850 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_fallback_unused_dbt_dev_staging, now list_checkmate-453316_fallback_unused_dbt_dev_staging)
[0m18:27:39.963683 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_fallback_unused_dbt_dev_universal, now list_checkmate-453316_fallback_unused_dbt_dev_universal)
[0m18:27:39.964281 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:27:39.964782 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:27:40.313362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db50073c650>]}
[0m18:27:40.314150 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:27:40.317646 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:27:40.318570 [info ] [Thread-1 (]: 1 of 2 START sql table model fallback_unused_dbt_dev_universal.calendar ........ [RUN]
[0m18:27:40.319255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_fallback_unused_dbt_dev_staging, now model.pipeline.calendar)
[0m18:27:40.319818 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:27:40.332350 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:27:40.333286 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:27:40.364019 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:27:40.364729 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`fallback_unused_dbt_dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:27:40.365186 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:27:41.371913 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d239baac-a0f1-4171-b00f-f64c0447da29&page=queryresults
[0m18:27:44.719707 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db500813e50>]}
[0m18:27:44.720806 [info ] [Thread-1 (]: 1 of 2 OK created sql table model fallback_unused_dbt_dev_universal.calendar ... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 4.40s]
[0m18:27:44.721928 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:27:44.723171 [debug] [Thread-3 (]: Began running node model.pipeline.player_games
[0m18:27:44.723977 [info ] [Thread-3 (]: 2 of 2 START sql table model fallback_unused_dbt_dev_staging.player_games ...... [RUN]
[0m18:27:44.724826 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.player_games'
[0m18:27:44.725368 [debug] [Thread-3 (]: Began compiling node model.pipeline.player_games
[0m18:27:44.731071 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.player_games"
[0m18:27:44.731919 [debug] [Thread-3 (]: Began executing node model.pipeline.player_games
[0m18:27:44.736032 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.player_games"
[0m18:27:44.737354 [debug] [Thread-3 (]: On model.pipeline.player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.player_games"} */

  
    

    create or replace table `checkmate-453316`.`fallback_unused_dbt_dev_staging`.`player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316.chess_raw.games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`fallback_unused_dbt_dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      AND cal_date BETWEEN current_date-7 and current_date

)

SELECT * FROM cte_base
    );
  
[0m18:27:44.738598 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:27:45.771841 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bdf710ae-f0fe-4ff1-ad95-a227d16a24a8&page=queryresults
[0m18:27:48.216391 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c2eb1829-b3db-4c12-b4bd-55595765a5e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db5003874d0>]}
[0m18:27:48.217348 [info ] [Thread-3 (]: 2 of 2 OK created sql table model fallback_unused_dbt_dev_staging.player_games . [[32mCREATE TABLE (0.0 rows, 85.6 KiB processed)[0m in 3.49s]
[0m18:27:48.218895 [debug] [Thread-3 (]: Finished running node model.pipeline.player_games
[0m18:27:48.220680 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:27:48.221469 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:27:48.221913 [debug] [MainThread]: Connection 'list_checkmate-453316_fallback_unused_dbt_dev_universal' was properly closed.
[0m18:27:48.222341 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:27:48.222780 [debug] [MainThread]: Connection 'model.pipeline.player_games' was properly closed.
[0m18:27:48.223318 [info ] [MainThread]: 
[0m18:27:48.223847 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 13.82 seconds (13.82s).
[0m18:27:48.225071 [debug] [MainThread]: Command end result
[0m18:27:48.259770 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:27:48.261774 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:27:48.270257 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:27:48.270629 [info ] [MainThread]: 
[0m18:27:48.271022 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:27:48.271338 [info ] [MainThread]: 
[0m18:27:48.271689 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:27:48.272343 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.000805, "process_in_blocks": "0", "process_kernel_time": 0.361656, "process_mem_max_rss": "239180", "process_out_blocks": "3112", "process_user_time": 4.300187}
[0m18:27:48.272798 [debug] [MainThread]: Command `dbt run` succeeded at 18:27:48.272699 after 16.00 seconds
[0m18:27:48.273168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53ace5d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53aceed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7db53b9a7210>]}
[0m18:27:48.273555 [debug] [MainThread]: Flushing usage events
[0m18:27:48.640044 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:29:38.649662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b095d499190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b095d498dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b095d499310>]}


============================== 18:29:38.652792 | 872d6091-89ac-4a65-9630-340ef53d20d5 ==============================
[0m18:29:38.652792 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:29:38.653227 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:29:39.216286 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "pipeline", target "dev" invalid: Runtime Error
    Must specify schema
[0m18:29:39.216982 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6098783, "process_in_blocks": "0", "process_kernel_time": 0.255156, "process_mem_max_rss": "218008", "process_out_blocks": "8", "process_user_time": 2.536138}
[0m18:29:39.217488 [debug] [MainThread]: Command `dbt run` failed at 18:29:39.217395 after 0.61 seconds
[0m18:29:39.217803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b095d499610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b095d4ebc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b092b47d410>]}
[0m18:29:39.218130 [debug] [MainThread]: Flushing usage events
[0m18:29:39.600135 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:30:02.693681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781622d66990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781621f212d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781620378990>]}


============================== 18:30:02.696758 | d7029b39-c4a9-40c7-baad-ce6316d2089f ==============================
[0m18:30:02.696758 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:30:02.697203 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:30:03.241840 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "pipeline", target "dev" invalid: Runtime Error
    Must specify schema
[0m18:30:03.242544 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.59237057, "process_in_blocks": "0", "process_kernel_time": 0.269655, "process_mem_max_rss": "218360", "process_out_blocks": "8", "process_user_time": 2.519035}
[0m18:30:03.243053 [debug] [MainThread]: Command `dbt run` failed at 18:30:03.242964 after 0.59 seconds
[0m18:30:03.243373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7816203d81d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7816203781d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7815ed6b5410>]}
[0m18:30:03.243709 [debug] [MainThread]: Flushing usage events
[0m18:30:03.593137 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:30:40.917333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee46975d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee4696ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee4697110>]}


============================== 18:30:40.920184 | 8beedd69-d910-4a94-ad51-04325e2a0dac ==============================
[0m18:30:40.920184 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:30:40.920642 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:30:41.567270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deb0995010>]}
[0m18:30:41.680580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deb09a2dd0>]}
[0m18:30:41.681143 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:30:41.802989 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:30:41.874161 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m18:30:41.874630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee4dbe210>]}
[0m18:30:42.733064 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m18:30:42.871948 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m18:30:42.881525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deb07312d0>]}
[0m18:30:42.939290 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:30:42.941459 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:30:42.950045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deb08a47d0>]}
[0m18:30:42.950473 [info ] [MainThread]: Found 2 models, 1 source, 491 macros
[0m18:30:42.950745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deaa50cd10>]}
[0m18:30:42.952129 [info ] [MainThread]: 
[0m18:30:42.952419 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:30:42.952661 [info ] [MainThread]: 
[0m18:30:42.953040 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:30:42.956726 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:30:42.957233 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:30:42.957669 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:30:42.958120 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:30:44.051181 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_dbt_dev_universal)
[0m18:30:44.051911 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_dbt_dev_staging)
[0m18:30:44.052711 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_dbt_dev_universal"
"
[0m18:30:44.053421 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_dbt_dev_staging"
"
[0m18:30:44.068227 [debug] [ThreadPool]: On create_checkmate-453316_dev_dbt_dev_universal: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_dbt_dev_universal"} */
create schema if not exists `checkmate-453316`.`dev_dbt_dev_universal`
  
[0m18:30:44.071603 [debug] [ThreadPool]: On create_checkmate-453316_dev_dbt_dev_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_dbt_dev_staging"} */
create schema if not exists `checkmate-453316`.`dev_dbt_dev_staging`
  
[0m18:30:44.072266 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:30:44.072874 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:30:44.435125 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:9117cd71-9970-4ff0-86d8-f233e47d71fd&page=queryresults
[0m18:30:44.485924 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bb3c10fb-c9d5-4d66-92f7-63637142f726&page=queryresults
[0m18:30:47.551432 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_dbt_dev_staging, now list_checkmate-453316_dev_dbt_dev_universal)
[0m18:30:47.552235 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_dbt_dev_universal, now list_checkmate-453316_dev_dbt_dev_staging)
[0m18:30:47.552940 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:30:47.553552 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:30:47.903068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deb06e3a90>]}
[0m18:30:47.903860 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:30:47.907303 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:30:47.908133 [info ] [Thread-1 (]: 1 of 2 START sql table model dev_dbt_dev_universal.calendar .................... [RUN]
[0m18:30:47.908729 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_dbt_dev_universal, now model.pipeline.calendar)
[0m18:30:47.909118 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:30:47.915975 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:30:47.916494 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:30:47.945666 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:30:47.946432 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_dbt_dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:30:47.946916 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:30:48.960619 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0ce7a7c3-00c0-46d6-9648-3d0b9245c3e3&page=queryresults
[0m18:30:51.811042 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78deb053bd10>]}
[0m18:30:51.812115 [info ] [Thread-1 (]: 1 of 2 OK created sql table model dev_dbt_dev_universal.calendar ............... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.90s]
[0m18:30:51.813169 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:30:51.814460 [debug] [Thread-3 (]: Began running node model.pipeline.player_games
[0m18:30:51.815437 [info ] [Thread-3 (]: 2 of 2 START sql table model dev_dbt_dev_staging.player_games .................. [RUN]
[0m18:30:51.816278 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.player_games'
[0m18:30:51.816860 [debug] [Thread-3 (]: Began compiling node model.pipeline.player_games
[0m18:30:51.822890 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.player_games"
[0m18:30:51.823832 [debug] [Thread-3 (]: Began executing node model.pipeline.player_games
[0m18:30:51.829694 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.player_games"
[0m18:30:51.830980 [debug] [Thread-3 (]: On model.pipeline.player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_dbt_dev_staging`.`player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316.chess_raw.games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_dbt_dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      AND cal_date BETWEEN current_date-7 and current_date

)

SELECT * FROM cte_base
    );
  
[0m18:30:51.831975 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:30:53.165718 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2d632234-9aa5-4f57-95f4-908aaaf12545&page=queryresults
[0m18:30:55.111203 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8beedd69-d910-4a94-ad51-04325e2a0dac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dea9054b90>]}
[0m18:30:55.112183 [info ] [Thread-3 (]: 2 of 2 OK created sql table model dev_dbt_dev_staging.player_games ............. [[32mCREATE TABLE (0.0 rows, 85.6 KiB processed)[0m in 3.30s]
[0m18:30:55.113115 [debug] [Thread-3 (]: Finished running node model.pipeline.player_games
[0m18:30:55.115042 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:30:55.115974 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:30:55.116443 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_dbt_dev_staging' was properly closed.
[0m18:30:55.116875 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:30:55.117296 [debug] [MainThread]: Connection 'model.pipeline.player_games' was properly closed.
[0m18:30:55.117842 [info ] [MainThread]: 
[0m18:30:55.118333 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 12.16 seconds (12.16s).
[0m18:30:55.119568 [debug] [MainThread]: Command end result
[0m18:30:55.154645 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:30:55.156894 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:30:55.165482 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:30:55.165861 [info ] [MainThread]: 
[0m18:30:55.166256 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:30:55.166582 [info ] [MainThread]: 
[0m18:30:55.166861 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:30:55.167370 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.292559, "process_in_blocks": "0", "process_kernel_time": 0.364294, "process_mem_max_rss": "240104", "process_out_blocks": "3112", "process_user_time": 4.196627}
[0m18:30:55.167720 [debug] [MainThread]: Command `dbt run` succeeded at 18:30:55.167645 after 14.29 seconds
[0m18:30:55.168004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee468d790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee468c290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78dee48da010>]}
[0m18:30:55.168285 [debug] [MainThread]: Flushing usage events
[0m18:30:55.537532 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:33:14.053605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40cf9bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40d1e6110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40d2cefd0>]}


============================== 18:33:14.057020 | f6a89e2a-78da-4457-8d26-26775bf976c7 ==============================
[0m18:33:14.057020 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:33:14.057517 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m18:33:14.714134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3da235410>]}
[0m18:33:14.829505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40f9ccc10>]}
[0m18:33:14.830145 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:33:14.951129 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:33:15.022120 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m18:33:15.022548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40d6c20d0>]}
[0m18:33:15.922706 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m18:33:16.057431 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m18:33:16.066545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3bab93e10>]}
[0m18:33:16.126730 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:33:16.128908 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:33:16.137270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3ba91e410>]}
[0m18:33:16.137731 [info ] [MainThread]: Found 2 models, 1 source, 491 macros
[0m18:33:16.138016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3baa4d9d0>]}
[0m18:33:16.139581 [info ] [MainThread]: 
[0m18:33:16.139944 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:33:16.140210 [info ] [MainThread]: 
[0m18:33:16.140646 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:33:16.145052 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:33:16.146106 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:33:16.146756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:33:16.147433 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:33:17.216055 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev__staging)
[0m18:33:17.216706 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev__universal)
[0m18:33:17.217318 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev__staging"
"
[0m18:33:17.217879 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev__universal"
"
[0m18:33:17.233033 [debug] [ThreadPool]: On create_checkmate-453316_dev__staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev__staging"} */
create schema if not exists `checkmate-453316`.`dev__staging`
  
[0m18:33:17.235646 [debug] [ThreadPool]: On create_checkmate-453316_dev__universal: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev__universal"} */
create schema if not exists `checkmate-453316`.`dev__universal`
  
[0m18:33:17.236129 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:33:17.236676 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:33:18.530044 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6c0315b7-7a9f-4447-a500-a6c9ef3ed1c6&page=queryresults
[0m18:33:18.902756 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:264fec45-3284-4292-9932-d70463764181&page=queryresults
[0m18:33:21.650444 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev__universal, now list_checkmate-453316_dev__staging)
[0m18:33:21.651268 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev__staging, now list_checkmate-453316_dev__universal)
[0m18:33:21.651900 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:33:21.652484 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:33:21.995508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3bab3e310>]}
[0m18:33:21.996801 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:33:22.001548 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:33:22.002168 [info ] [Thread-1 (]: 1 of 2 START sql table model dev__universal.calendar ........................... [RUN]
[0m18:33:22.002538 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev__universal, now model.pipeline.calendar)
[0m18:33:22.002828 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:33:22.009372 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:33:22.009897 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:33:22.041213 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:33:22.041978 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev__universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:33:22.042462 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:33:23.065456 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:34d03f45-e153-4b66-8d5f-29568e5ee14e&page=queryresults
[0m18:33:25.919791 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3ba9a9750>]}
[0m18:33:25.920575 [info ] [Thread-1 (]: 1 of 2 OK created sql table model dev__universal.calendar ...................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.92s]
[0m18:33:25.921577 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:33:25.922809 [debug] [Thread-3 (]: Began running node model.pipeline.player_games
[0m18:33:25.923628 [info ] [Thread-3 (]: 2 of 2 START sql table model dev__staging.player_games ......................... [RUN]
[0m18:33:25.924526 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.player_games'
[0m18:33:25.924971 [debug] [Thread-3 (]: Began compiling node model.pipeline.player_games
[0m18:33:25.929012 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.player_games"
[0m18:33:25.929685 [debug] [Thread-3 (]: Began executing node model.pipeline.player_games
[0m18:33:25.933565 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.player_games"
[0m18:33:25.934478 [debug] [Thread-3 (]: On model.pipeline.player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev__staging`.`player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316.chess_raw.games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev__universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      AND cal_date BETWEEN current_date-7 and current_date

)

SELECT * FROM cte_base
    );
  
[0m18:33:25.935136 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:33:27.320303 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:9034816d-e15d-486a-96db-e3c9ba553bfe&page=queryresults
[0m18:33:29.122677 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6a89e2a-78da-4457-8d26-26775bf976c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a3b95031d0>]}
[0m18:33:29.123389 [info ] [Thread-3 (]: 2 of 2 OK created sql table model dev__staging.player_games .................... [[32mCREATE TABLE (0.0 rows, 85.6 KiB processed)[0m in 3.20s]
[0m18:33:29.124023 [debug] [Thread-3 (]: Finished running node model.pipeline.player_games
[0m18:33:29.125322 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:33:29.125928 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:33:29.126260 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:33:29.126623 [debug] [MainThread]: Connection 'list_checkmate-453316_dev__staging' was properly closed.
[0m18:33:29.126937 [debug] [MainThread]: Connection 'model.pipeline.player_games' was properly closed.
[0m18:33:29.127298 [info ] [MainThread]: 
[0m18:33:29.127586 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 12.99 seconds (12.99s).
[0m18:33:29.128279 [debug] [MainThread]: Command end result
[0m18:33:29.148044 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:33:29.149259 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:33:29.153758 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:33:29.154041 [info ] [MainThread]: 
[0m18:33:29.154375 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:33:29.154645 [info ] [MainThread]: 
[0m18:33:29.155126 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:33:29.156116 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.145733, "process_in_blocks": "0", "process_kernel_time": 0.349356, "process_mem_max_rss": "239888", "process_out_blocks": "3120", "process_user_time": 4.207644}
[0m18:33:29.156771 [debug] [MainThread]: Command `dbt run` succeeded at 18:33:29.156626 after 15.15 seconds
[0m18:33:29.157297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40cf90290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40cf87ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70a40d1e5d10>]}
[0m18:33:29.157878 [debug] [MainThread]: Flushing usage events
[0m18:33:30.334554 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:39:49.256259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda179bd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda17f2b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda179b790>]}


============================== 18:39:49.258620 | 6d8e29ae-a568-4276-a68e-d567757c87b1 ==============================
[0m18:39:49.258620 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:39:49.259060 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt compile', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:39:49.875682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda261c790>]}
[0m18:39:49.984338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda41cde10>]}
[0m18:39:49.984901 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:39:50.105622 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:39:50.174527 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m18:39:50.174929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda25d0150>]}
[0m18:39:51.002186 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m18:39:51.135861 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m18:39:51.145089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bed69606490>]}
[0m18:39:51.201441 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:39:51.203527 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:39:51.212423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bed5327db10>]}
[0m18:39:51.212845 [info ] [MainThread]: Found 3 models, 1 source, 491 macros
[0m18:39:51.213116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bed5337f2d0>]}
[0m18:39:51.214428 [info ] [MainThread]: 
[0m18:39:51.214728 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:39:51.214966 [info ] [MainThread]: 
[0m18:39:51.215351 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:39:51.219229 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m18:39:51.220392 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m18:39:51.221046 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:51.221565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:52.431731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d8e29ae-a568-4276-a68e-d567757c87b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bed6806df90>]}
[0m18:39:52.432177 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:39:52.434753 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:39:52.435219 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m18:39:52.435522 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:39:52.441918 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:39:52.442449 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:39:52.442760 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:39:52.443474 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:39:52.443991 [debug] [Thread-3 (]: Began running node model.pipeline.player_games
[0m18:39:52.444598 [debug] [Thread-3 (]: Acquiring new bigquery connection 'model.pipeline.player_games'
[0m18:39:52.444877 [debug] [Thread-3 (]: Began compiling node model.pipeline.player_games
[0m18:39:52.448180 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.player_games"
[0m18:39:52.448638 [debug] [Thread-3 (]: Began executing node model.pipeline.player_games
[0m18:39:52.448934 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:39:52.449618 [debug] [Thread-3 (]: Finished running node model.pipeline.player_games
[0m18:39:52.450314 [debug] [Thread-2 (]: Began running node model.pipeline.monthly_player_games
[0m18:39:52.450638 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.monthly_player_games)
[0m18:39:52.450920 [debug] [Thread-2 (]: Began compiling node model.pipeline.monthly_player_games
[0m18:39:52.453493 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.monthly_player_games"
[0m18:39:52.453908 [debug] [Thread-2 (]: Began executing node model.pipeline.monthly_player_games
[0m18:39:52.454214 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:39:52.454889 [debug] [Thread-2 (]: Finished running node model.pipeline.monthly_player_games
[0m18:39:52.456071 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:39:52.456578 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:39:52.457002 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_games' was properly closed.
[0m18:39:52.457438 [debug] [MainThread]: Connection 'model.pipeline.player_games' was properly closed.
[0m18:39:52.458796 [debug] [MainThread]: Command end result
[0m18:39:52.477800 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:39:52.478933 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:39:52.483973 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:39:52.484516 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.2708485, "process_in_blocks": "0", "process_kernel_time": 0.345976, "process_mem_max_rss": "234420", "process_out_blocks": "3120", "process_user_time": 3.962065}
[0m18:39:52.484868 [debug] [MainThread]: Command `dbt compile` succeeded at 18:39:52.484790 after 3.27 seconds
[0m18:39:52.485159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda17d5090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda1afc410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7beda53fffd0>]}
[0m18:39:52.485457 [debug] [MainThread]: Flushing usage events
[0m18:39:52.854197 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:47:57.321528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7250376ab6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7250376a0590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725037702810>]}


============================== 18:47:57.324574 | f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b ==============================
[0m18:47:57.324574 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:47:57.325009 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:47:57.951597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724fff76fe50>]}
[0m18:47:58.060772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72503a04d990>]}
[0m18:47:58.061322 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:47:58.180681 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:47:58.264812 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 3 files added, 0 files changed.
[0m18:47:58.265258 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/stg__monthly_player_games.sql
[0m18:47:58.265547 [debug] [MainThread]: Partial parsing: added file: pipeline://models/aggregate/monthly_player_openings.sql
[0m18:47:58.265812 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/stg__player_games.sql
[0m18:47:58.266048 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/staging/player_games.sql
[0m18:47:58.266283 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/staging/monthly_player_games.sql
[0m18:47:58.487329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724fff7f0950>]}
[0m18:47:58.546637 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:47:58.548751 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:47:58.557061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724fff397f90>]}
[0m18:47:58.557537 [info ] [MainThread]: Found 4 models, 1 source, 491 macros
[0m18:47:58.557841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724fff7f1450>]}
[0m18:47:58.559336 [info ] [MainThread]: 
[0m18:47:58.559708 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:47:58.559978 [info ] [MainThread]: 
[0m18:47:58.560445 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:47:58.564049 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m18:47:58.565188 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m18:47:58.565782 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:58.566573 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m18:47:58.567207 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:58.568104 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:59.727950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f0c0de3a-a8a1-4b67-8c21-d8ba3ed1932b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725037c468d0>]}
[0m18:47:59.728382 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:47:59.730870 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:47:59.731628 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m18:47:59.732226 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:47:59.745115 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:47:59.746062 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:47:59.746688 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:47:59.748002 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:47:59.748966 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:47:59.749420 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m18:47:59.749819 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:47:59.752945 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:47:59.753357 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:47:59.753653 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:47:59.754287 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:47:59.754986 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:47:59.755605 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m18:47:59.756147 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:47:59.760776 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:47:59.761539 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:47:59.762095 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:47:59.765100 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:47:59.765960 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m18:47:59.766344 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.monthly_player_openings'
[0m18:47:59.766623 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m18:47:59.769128 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m18:47:59.769584 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m18:47:59.769870 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:47:59.770522 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:47:59.771590 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:47:59.772139 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m18:47:59.772594 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m18:47:59.773019 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:47:59.773492 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m18:47:59.775189 [debug] [MainThread]: Command end result
[0m18:47:59.809546 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:47:59.811547 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:47:59.821924 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:47:59.822638 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.5437477, "process_in_blocks": "0", "process_kernel_time": 0.339301, "process_mem_max_rss": "232284", "process_out_blocks": "3168", "process_user_time": 3.270118}
[0m18:47:59.823012 [debug] [MainThread]: Command `dbt compile` succeeded at 18:47:59.822930 after 2.54 seconds
[0m18:47:59.823328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x725037702810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7250376a0450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7250383bb1d0>]}
[0m18:47:59.823672 [debug] [MainThread]: Flushing usage events
[0m18:48:00.209081 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:51:22.360576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f6e94c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f6e94fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f6e97550>]}


============================== 18:51:22.363413 | 7a6e5a3c-3702-44dc-b69a-34230beb04ea ==============================
[0m18:51:22.363413 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:51:22.363848 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:51:23.003328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45bf6ce210>]}
[0m18:51:23.112763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f98cd610>]}
[0m18:51:23.113312 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:51:23.235987 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:51:23.330632 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:51:23.331190 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__monthly_player_games.sql
[0m18:51:23.549774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45bedfe9d0>]}
[0m18:51:23.611008 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:51:23.613122 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:51:23.621455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45beb52910>]}
[0m18:51:23.621880 [info ] [MainThread]: Found 4 models, 1 source, 491 macros
[0m18:51:23.622167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45beabd990>]}
[0m18:51:23.623627 [info ] [MainThread]: 
[0m18:51:23.623908 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:51:23.624175 [info ] [MainThread]: 
[0m18:51:23.624645 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:51:23.628534 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:51:23.629194 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:51:23.629672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:51:23.630261 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:51:23.630893 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:51:23.631629 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:51:24.771304 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_universal)
[0m18:51:24.772026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_aggregate)
[0m18:51:24.772727 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_universal"
"
[0m18:51:24.773322 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_staging)
[0m18:51:24.774109 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_aggregate"
"
[0m18:51:24.789182 [debug] [ThreadPool]: On create_checkmate-453316_dev_universal: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_universal"} */
create schema if not exists `checkmate-453316`.`dev_universal`
  
[0m18:51:24.789935 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_staging"
"
[0m18:51:24.793044 [debug] [ThreadPool]: On create_checkmate-453316_dev_aggregate: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_aggregate"} */
create schema if not exists `checkmate-453316`.`dev_aggregate`
  
[0m18:51:24.793680 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:51:24.796796 [debug] [ThreadPool]: On create_checkmate-453316_dev_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_staging"} */
create schema if not exists `checkmate-453316`.`dev_staging`
  
[0m18:51:24.797370 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:51:24.798546 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:51:25.222186 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:5874dfec-7e08-4349-862c-08c45b362202&page=queryresults
[0m18:51:25.228385 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:5a4d7c5e-5630-4c0d-b597-e604288c65f1&page=queryresults
[0m18:51:25.232485 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7b7da1d1-54b0-4f70-9396-fc6af4321146&page=queryresults
[0m18:51:28.015102 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_universal, now list_checkmate-453316_dev_universal)
[0m18:51:28.015895 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_aggregate, now list_checkmate-453316_dev_aggregate)
[0m18:51:28.016604 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_staging, now list_checkmate-453316_dev_staging)
[0m18:51:28.017231 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:51:28.017848 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:51:28.018378 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:51:28.386575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45beda9710>]}
[0m18:51:28.387387 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:51:28.392193 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:51:28.393057 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:51:28.393747 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.calendar)
[0m18:51:28.394303 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:51:28.402326 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:51:28.403061 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:51:28.431954 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:51:28.432676 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:51:28.433124 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:51:29.435536 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f35b27f9-11db-4e66-92d3-6a126a045e48&page=queryresults
[0m18:51:32.014509 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45bc0e8490>]}
[0m18:51:32.015245 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.62s]
[0m18:51:32.015901 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:51:32.016902 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:51:32.017757 [info ] [Thread-3 (]: 2 of 4 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m18:51:32.018458 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m18:51:32.019005 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:51:32.024962 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:51:32.026092 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:51:32.030716 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m18:51:32.032304 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      AND cal_date BETWEEN current_date-7 and current_date

)

SELECT * FROM cte_base
    );
  
[0m18:51:32.033425 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:51:33.547020 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6a0a88a6-9e6d-479d-b35d-79aff7dbea47&page=queryresults
[0m18:51:35.397429 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45bc0d9010>]}
[0m18:51:35.398371 [info ] [Thread-3 (]: 2 of 4 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (0.0 rows, 85.6 KiB processed)[0m in 3.38s]
[0m18:51:35.399282 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:51:35.400524 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:51:35.401442 [info ] [Thread-2 (]: 3 of 4 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m18:51:35.402137 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__monthly_player_games)
[0m18:51:35.402739 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:51:35.407566 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:51:35.408480 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:51:35.414654 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m18:51:35.415862 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        AND game_date BETWEEN current_date-7 AND current_date
      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg

);

END;
    );
  
[0m18:51:35.416748 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:51:35.559300 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2441eb9d-5569-4ade-bb06-89d5f0220b23&page=queryresults
[0m18:51:35.560159 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2441eb9d-5569-4ade-bb06-89d5f0220b23&page=queryresults
[0m18:51:35.567468 [debug] [Thread-2 (]: Database Error in model stg__monthly_player_games (models/staging/stg__monthly_player_games.sql)
  Syntax error: Unexpected keyword END at [70:1]
  compiled code at target/run/pipeline/models/staging/stg__monthly_player_games.sql
[0m18:51:35.567884 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7a6e5a3c-3702-44dc-b69a-34230beb04ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45b4b25ed0>]}
[0m18:51:35.568359 [error] [Thread-2 (]: 3 of 4 ERROR creating sql table model dev_staging.stg__monthly_player_games .... [[31mERROR[0m in 0.17s]
[0m18:51:35.568810 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:51:35.569563 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__monthly_player_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__monthly_player_games (models/staging/stg__monthly_player_games.sql)
  Syntax error: Unexpected keyword END at [70:1]
  compiled code at target/run/pipeline/models/staging/stg__monthly_player_games.sql.
[0m18:51:35.571542 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m18:51:35.572209 [info ] [Thread-4 (]: 4 of 4 SKIP relation dev_aggregate.monthly_player_openings ..................... [[33mSKIP[0m]
[0m18:51:35.572859 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:51:35.574421 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:51:35.575486 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:51:35.576003 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:51:35.576455 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m18:51:35.576880 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m18:51:35.577436 [info ] [MainThread]: 
[0m18:51:35.577944 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 11.95 seconds (11.95s).
[0m18:51:35.579506 [debug] [MainThread]: Command end result
[0m18:51:35.614022 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:51:35.616138 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:51:35.624834 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:51:35.625249 [info ] [MainThread]: 
[0m18:51:35.625697 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:51:35.626072 [info ] [MainThread]: 
[0m18:51:35.626487 [error] [MainThread]:   Database Error in model stg__monthly_player_games (models/staging/stg__monthly_player_games.sql)
  Syntax error: Unexpected keyword END at [70:1]
  compiled code at target/run/pipeline/models/staging/stg__monthly_player_games.sql
[0m18:51:35.626773 [info ] [MainThread]: 
[0m18:51:35.627050 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=1 TOTAL=4
[0m18:51:35.627614 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 13.309084, "process_in_blocks": "56", "process_kernel_time": 0.348047, "process_mem_max_rss": "238512", "process_out_blocks": "3208", "process_user_time": 3.534929}
[0m18:51:35.627955 [debug] [MainThread]: Command `dbt run` failed at 18:51:35.627880 after 13.31 seconds
[0m18:51:35.628251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f6ee62d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f6e8ca50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e45f6e83dd0>]}
[0m18:51:35.628550 [debug] [MainThread]: Flushing usage events
[0m18:51:36.002500 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:52:52.166758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053c1c7b250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053c1fdc590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053c1ec2610>]}


============================== 18:52:52.169734 | c518e2dd-9881-4d64-a8c1-fb187e2fc658 ==============================
[0m18:52:52.169734 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:52:52.170172 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:52:52.814203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70538a593590>]}
[0m18:52:52.925634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705389f7f490>]}
[0m18:52:52.926185 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:52:53.043763 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:52:53.130920 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m18:52:53.131464 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/monthly_player_openings.sql
[0m18:52:53.131815 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__monthly_player_games.sql
[0m18:52:53.353061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705389f7f150>]}
[0m18:52:53.409959 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:52:53.412078 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:52:53.420348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705389a36910>]}
[0m18:52:53.420785 [info ] [MainThread]: Found 4 models, 1 source, 491 macros
[0m18:52:53.421060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705389eb5810>]}
[0m18:52:53.422498 [info ] [MainThread]: 
[0m18:52:53.422775 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:52:53.423013 [info ] [MainThread]: 
[0m18:52:53.423390 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:52:53.427113 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:52:53.427745 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:52:53.428532 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:52:53.429533 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:52:53.430359 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:52:53.431074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:52:54.556579 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m18:52:54.557461 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:52:54.557908 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:52:54.558705 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m18:52:54.559259 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:52:54.561266 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:52:54.725208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705389a478d0>]}
[0m18:52:54.725923 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:52:54.729264 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:52:54.730158 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:52:54.730870 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m18:52:54.731434 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:52:54.742851 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:52:54.743758 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:52:54.759418 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:52:54.970812 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:52:54.972070 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:52:55.400021 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d1ad2342-dc18-473f-a5fe-8bede8539784&page=queryresults
[0m18:52:57.523613 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705383963f90>]}
[0m18:52:57.524655 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.79s]
[0m18:52:57.525578 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:52:57.526852 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:52:57.527745 [info ] [Thread-3 (]: 2 of 4 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m18:52:57.528432 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m18:52:57.528973 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:52:57.535034 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:52:57.535904 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:52:57.539242 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:52:57.677573 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m18:52:57.678918 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      AND cal_date BETWEEN current_date-7 and current_date

)

SELECT * FROM cte_base
    );
  
[0m18:52:58.060515 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:1ba36d5c-a647-4321-95b0-1ae3519d21b7&page=queryresults
[0m18:53:00.074316 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705383972450>]}
[0m18:53:00.075277 [info ] [Thread-3 (]: 2 of 4 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (0.0 rows, 85.6 KiB processed)[0m in 2.55s]
[0m18:53:00.076188 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:53:00.077188 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:53:00.077767 [info ] [Thread-2 (]: 3 of 4 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m18:53:00.078446 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__monthly_player_games)
[0m18:53:00.079009 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:53:00.082380 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:53:00.082988 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:53:00.085736 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m18:53:00.086519 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        AND game_date BETWEEN current_date-7 AND current_date
      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:53:00.087090 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:53:00.562557 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ff524558-b77e-4915-9c81-ed65d87d88b9&page=queryresults
[0m18:53:02.355987 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053814ff550>]}
[0m18:53:02.356944 [info ] [Thread-2 (]: 3 of 4 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (0.0 rows, 0 processed)[0m in 2.28s]
[0m18:53:02.357563 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:53:02.358622 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m18:53:02.359502 [info ] [Thread-4 (]: 4 of 4 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m18:53:02.360351 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.monthly_player_openings'
[0m18:53:02.360910 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m18:53:02.365838 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m18:53:02.366667 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m18:53:02.370798 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m18:53:02.371940 [debug] [Thread-4 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      

WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:53:02.372771 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:53:02.843976 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:47e24561-45a9-4b55-aa29-e4a080dc9f7e&page=queryresults
[0m18:53:04.759754 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c518e2dd-9881-4d64-a8c1-fb187e2fc658', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705381584650>]}
[0m18:53:04.760479 [info ] [Thread-4 (]: 4 of 4 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (0.0 rows, 142.5 KiB processed)[0m in 2.40s]
[0m18:53:04.761086 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:53:04.762780 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:53:04.763704 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:53:04.764152 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m18:53:04.764607 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:53:04.765028 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m18:53:04.765460 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m18:53:04.766002 [info ] [MainThread]: 
[0m18:53:04.766499 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 11.34 seconds (11.34s).
[0m18:53:04.768188 [debug] [MainThread]: Command end result
[0m18:53:04.805252 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:53:04.807280 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:53:04.815265 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:53:04.815573 [info ] [MainThread]: 
[0m18:53:04.815872 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:53:04.816106 [info ] [MainThread]: 
[0m18:53:04.816359 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m18:53:04.816876 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.693567, "process_in_blocks": "0", "process_kernel_time": 0.365002, "process_mem_max_rss": "237368", "process_out_blocks": "3232", "process_user_time": 3.528455}
[0m18:53:04.817210 [debug] [MainThread]: Command `dbt run` succeeded at 18:53:04.817136 after 12.69 seconds
[0m18:53:04.817501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053c1c703d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053c298ef90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7053c5677e90>]}
[0m18:53:04.817784 [debug] [MainThread]: Flushing usage events
[0m18:53:05.245432 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:12:08.073504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccda43c2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccda43c990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccda43cc50>]}


============================== 19:12:08.076501 | 28f8d987-f7ae-4c0f-86ef-f567e5b5761d ==============================
[0m19:12:08.076501 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:12:08.076965 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'True'}
[0m19:12:08.686758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '28f8d987-f7ae-4c0f-86ef-f567e5b5761d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccda568550>]}
[0m19:12:08.797885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '28f8d987-f7ae-4c0f-86ef-f567e5b5761d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccdce71b90>]}
[0m19:12:08.798458 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:12:08.920443 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:12:09.005047 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m19:12:09.005504 [debug] [MainThread]: Partial parsing: added file: pipeline://macros/testing_date_filter.sql
[0m19:12:09.005831 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m19:12:09.217396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '28f8d987-f7ae-4c0f-86ef-f567e5b5761d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cca2733f10>]}
[0m19:12:09.274091 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:12:09.276201 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:12:09.284089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '28f8d987-f7ae-4c0f-86ef-f567e5b5761d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cca21ef390>]}
[0m19:12:09.284515 [info ] [MainThread]: Found 4 models, 1 source, 492 macros
[0m19:12:09.284788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28f8d987-f7ae-4c0f-86ef-f567e5b5761d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cca20d5710>]}
[0m19:12:09.286123 [info ] [MainThread]: 
[0m19:12:09.286397 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:12:09.286640 [info ] [MainThread]: 
[0m19:12:09.287024 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:12:09.290729 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m19:12:09.291793 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:12:09.292293 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:12:09.293283 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m19:12:09.293820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:12:09.294539 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:12:10.167186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28f8d987-f7ae-4c0f-86ef-f567e5b5761d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cca2433f90>]}
[0m19:12:10.167566 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:12:10.170376 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:12:10.171225 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m19:12:10.171860 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:12:10.184620 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:12:10.185511 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:12:10.186106 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:12:10.187711 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:12:10.188716 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m19:12:10.189229 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m19:12:10.189631 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m19:12:10.198293 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m19:12:10.201535 [debug] [Thread-3 (]: Compilation Error in model stg__player_games (models/staging/stg__player_games.sql)
  'test_date_filter' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m19:12:10.202161 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m19:12:10.202910 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__player_games' to be skipped because of status 'error'.  Reason: Compilation Error in model stg__player_games (models/staging/stg__player_games.sql)
  'test_date_filter' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m19:12:10.204522 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:12:10.205082 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m19:12:10.205525 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m19:12:10.205941 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m19:12:10.206602 [error] [MainThread]: Encountered an error:
Runtime Error
  Compilation Error in model stg__player_games (models/staging/stg__player_games.sql)
    'test_date_filter' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m19:12:10.207601 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 2.1803288, "process_in_blocks": "16", "process_kernel_time": 0.338113, "process_mem_max_rss": "231664", "process_out_blocks": "2112", "process_user_time": 3.210733}
[0m19:12:10.208245 [debug] [MainThread]: Command `dbt compile` failed at 19:12:10.208098 after 2.18 seconds
[0m19:12:10.208788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccda4962d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71ccda497610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71cca1ff6010>]}
[0m19:12:10.209384 [debug] [MainThread]: Flushing usage events
[0m19:12:10.558576 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:13:04.291956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc50ec93510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc50ec93c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc50ec933d0>]}


============================== 19:13:04.295066 | 61cdee85-7d42-4be0-a1d1-215166f7999f ==============================
[0m19:13:04.295066 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:13:04.295513 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:13:04.909739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61cdee85-7d42-4be0-a1d1-215166f7999f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc4d6f6b410>]}
[0m19:13:05.018853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61cdee85-7d42-4be0-a1d1-215166f7999f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc511671810>]}
[0m19:13:05.019414 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:13:05.140564 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:13:05.225403 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:13:05.225899 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m19:13:05.438032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61cdee85-7d42-4be0-a1d1-215166f7999f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc4d6ba9b50>]}
[0m19:13:05.494882 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:13:05.496949 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:13:05.504850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61cdee85-7d42-4be0-a1d1-215166f7999f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc4d69f32d0>]}
[0m19:13:05.505270 [info ] [MainThread]: Found 4 models, 1 source, 492 macros
[0m19:13:05.505558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61cdee85-7d42-4be0-a1d1-215166f7999f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc4d6943010>]}
[0m19:13:05.506895 [info ] [MainThread]: 
[0m19:13:05.507172 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:13:05.507418 [info ] [MainThread]: 
[0m19:13:05.507797 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:13:05.511354 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m19:13:05.511999 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:13:05.512990 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m19:13:05.514003 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:13:05.514762 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:13:05.515530 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:13:06.964184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61cdee85-7d42-4be0-a1d1-215166f7999f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc4d6dd36d0>]}
[0m19:13:06.964883 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:13:06.968297 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:13:06.968897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m19:13:06.969318 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:13:06.976338 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:13:06.976886 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:13:06.977208 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:13:06.978090 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:13:06.978886 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m19:13:06.979232 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m19:13:06.979544 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m19:13:06.983715 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m19:13:06.984199 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m19:13:06.984508 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m19:13:06.985162 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m19:13:06.985923 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m19:13:06.986566 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__monthly_player_games)
[0m19:13:06.987119 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m19:13:06.992086 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m19:13:06.992919 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m19:13:06.993497 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:13:06.994710 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m19:13:06.995735 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m19:13:06.996448 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.monthly_player_openings'
[0m19:13:06.996993 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m19:13:07.003479 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m19:13:07.004348 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m19:13:07.004948 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:13:07.006187 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m19:13:07.007580 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:13:07.008080 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m19:13:07.008521 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m19:13:07.008939 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m19:13:07.009350 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m19:13:07.010920 [debug] [MainThread]: Command end result
[0m19:13:07.044021 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:13:07.045970 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:13:07.055859 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:13:07.056670 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.807645, "process_in_blocks": "0", "process_kernel_time": 0.32722, "process_mem_max_rss": "231604", "process_out_blocks": "3176", "process_user_time": 3.264556}
[0m19:13:07.057126 [debug] [MainThread]: Command `dbt compile` succeeded at 19:13:07.057023 after 2.81 seconds
[0m19:13:07.057512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc50ecf0810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc50edc6350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cc512677ed0>]}
[0m19:13:07.057916 [debug] [MainThread]: Flushing usage events
[0m19:13:07.427983 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:14:50.098336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de7899853d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de789987a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de789987450>]}


============================== 19:14:50.100954 | 40c68792-a5e7-4b17-b376-ea8d2d82c46d ==============================
[0m19:14:50.100954 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:14:50.101459 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:14:50.732188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40c68792-a5e7-4b17-b376-ea8d2d82c46d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de7581c0650>]}
[0m19:14:50.842134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40c68792-a5e7-4b17-b376-ea8d2d82c46d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de78c3cd710>]}
[0m19:14:50.842792 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:14:50.964677 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:14:51.050599 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m19:14:51.051038 [debug] [MainThread]: Partial parsing: added file: pipeline://macros/sql_macros.sql
[0m19:14:51.051442 [debug] [MainThread]: Partial parsing: deleted file: pipeline://macros/testing_date_filter.sql
[0m19:14:51.051721 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m19:14:51.274289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40c68792-a5e7-4b17-b376-ea8d2d82c46d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de751951850>]}
[0m19:14:51.331887 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:14:51.334143 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:14:51.343627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40c68792-a5e7-4b17-b376-ea8d2d82c46d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de7516e0950>]}
[0m19:14:51.344048 [info ] [MainThread]: Found 4 models, 1 source, 492 macros
[0m19:14:51.344325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40c68792-a5e7-4b17-b376-ea8d2d82c46d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de751bc5290>]}
[0m19:14:51.345665 [info ] [MainThread]: 
[0m19:14:51.345940 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:14:51.346174 [info ] [MainThread]: 
[0m19:14:51.346588 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:14:51.350230 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m19:14:51.351302 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:14:51.351794 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:14:51.352598 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m19:14:51.353038 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:14:51.353839 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:14:52.191603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40c68792-a5e7-4b17-b376-ea8d2d82c46d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de7518475d0>]}
[0m19:14:52.192333 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:14:52.195853 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:14:52.196632 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m19:14:52.197361 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:14:52.208944 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:14:52.209846 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:14:52.210443 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:14:52.212842 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:14:52.213629 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m19:14:52.214124 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m19:14:52.214429 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m19:14:52.218137 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m19:14:52.218612 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m19:14:52.218909 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m19:14:52.219581 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m19:14:52.220276 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m19:14:52.220937 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m19:14:52.221529 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m19:14:52.226225 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m19:14:52.227037 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m19:14:52.227616 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:14:52.228827 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m19:14:52.229557 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m19:14:52.229951 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.monthly_player_openings'
[0m19:14:52.230437 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m19:14:52.234181 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m19:14:52.234849 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m19:14:52.235278 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:14:52.236279 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m19:14:52.237490 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:14:52.238011 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m19:14:52.238457 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m19:14:52.238888 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m19:14:52.239331 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m19:14:52.240935 [debug] [MainThread]: Command end result
[0m19:14:52.276372 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:14:52.278349 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:14:52.288081 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:14:52.288814 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.236346, "process_in_blocks": "0", "process_kernel_time": 0.358031, "process_mem_max_rss": "233060", "process_out_blocks": "3184", "process_user_time": 3.314445}
[0m19:14:52.289277 [debug] [MainThread]: Command `dbt compile` succeeded at 19:14:52.289176 after 2.24 seconds
[0m19:14:52.289668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de7899df5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de78c3be7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7de78a69b5d0>]}
[0m19:14:52.290013 [debug] [MainThread]: Flushing usage events
[0m19:14:52.654086 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:16:18.788749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5fc79b650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5fc798910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5fc79a910>]}


============================== 19:16:18.791756 | 0b17a0ea-3943-40ca-9a9c-41523e8d6e02 ==============================
[0m19:16:18.791756 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:16:18.792235 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt compile', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:16:19.412641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b17a0ea-3943-40ca-9a9c-41523e8d6e02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5c892b050>]}
[0m19:16:19.522457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b17a0ea-3943-40ca-9a9c-41523e8d6e02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5ff1e1390>]}
[0m19:16:19.523012 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:16:19.640981 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:16:19.728889 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:16:19.729394 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m19:16:19.942905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b17a0ea-3943-40ca-9a9c-41523e8d6e02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5c89d1e10>]}
[0m19:16:20.000406 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:16:20.002530 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:16:20.010605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b17a0ea-3943-40ca-9a9c-41523e8d6e02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5c8491110>]}
[0m19:16:20.011020 [info ] [MainThread]: Found 4 models, 1 source, 492 macros
[0m19:16:20.011288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b17a0ea-3943-40ca-9a9c-41523e8d6e02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5c84b3110>]}
[0m19:16:20.012631 [info ] [MainThread]: 
[0m19:16:20.012914 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:16:20.013150 [info ] [MainThread]: 
[0m19:16:20.013543 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:16:20.017053 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:16:20.017698 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:16:20.018661 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m19:16:20.019672 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m19:16:20.020496 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:16:20.021206 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:16:20.832370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b17a0ea-3943-40ca-9a9c-41523e8d6e02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5c86a2d50>]}
[0m19:16:20.832755 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:16:20.837476 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:16:20.838230 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.calendar)
[0m19:16:20.838803 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:16:20.850479 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:16:20.851377 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:16:20.851971 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:16:20.853430 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:16:20.854429 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m19:16:20.855019 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m19:16:20.855451 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m19:16:20.859612 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m19:16:20.860053 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m19:16:20.860344 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m19:16:20.860986 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m19:16:20.861539 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m19:16:20.861927 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__monthly_player_games)
[0m19:16:20.862240 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m19:16:20.867149 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m19:16:20.868137 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m19:16:20.868740 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:16:20.869982 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m19:16:20.870926 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m19:16:20.871647 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.monthly_player_openings'
[0m19:16:20.872167 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m19:16:20.878485 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m19:16:20.879315 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m19:16:20.879895 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:16:20.881098 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m19:16:20.882459 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:16:20.883004 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m19:16:20.883449 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m19:16:20.883872 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m19:16:20.884291 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m19:16:20.885911 [debug] [MainThread]: Command end result
[0m19:16:20.920445 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:16:20.922466 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:16:20.932103 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:16:20.932815 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.1873689, "process_in_blocks": "0", "process_kernel_time": 0.312714, "process_mem_max_rss": "231968", "process_out_blocks": "3176", "process_user_time": 3.286761}
[0m19:16:20.933329 [debug] [MainThread]: Command `dbt compile` succeeded at 19:16:20.933218 after 2.19 seconds
[0m19:16:20.933782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5fc7f9010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5fc7f3490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70e5fc793f10>]}
[0m19:16:20.934220 [debug] [MainThread]: Flushing usage events
[0m19:16:21.306670 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:20:26.038682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe0186fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe04e4590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe0184c10>]}


============================== 19:20:26.041994 | 4084bffc-aef4-48b8-b4ec-90dc9b3e8370 ==============================
[0m19:20:26.041994 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:20:26.042443 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:20:26.661021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abae9c9f10>]}
[0m19:20:26.771006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe2b75910>]}
[0m19:20:26.771561 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:20:26.889727 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:20:26.974473 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m19:20:26.974979 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/monthly_player_openings.sql
[0m19:20:26.975304 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__monthly_player_games.sql
[0m19:20:27.191206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abac39a890>]}
[0m19:20:27.248341 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:20:27.250484 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:20:27.258948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba7ea0a10>]}
[0m19:20:27.259376 [info ] [MainThread]: Found 4 models, 1 source, 492 macros
[0m19:20:27.259659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba7e3a3d0>]}
[0m19:20:27.261091 [info ] [MainThread]: 
[0m19:20:27.261370 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:20:27.261614 [info ] [MainThread]: 
[0m19:20:27.261990 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:20:27.265802 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:20:27.266716 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:20:27.267279 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:20:27.268044 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:20:27.268528 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:20:27.269250 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:20:28.524504 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m19:20:28.525371 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m19:20:28.526157 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m19:20:28.526832 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:20:28.527457 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:20:28.528009 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:20:28.683139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba7e07810>]}
[0m19:20:28.683841 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:20:28.687162 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:20:28.688013 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m19:20:28.688690 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m19:20:28.689241 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:20:28.700668 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:20:28.701332 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:20:28.713754 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:20:28.917718 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m19:20:28.918977 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m19:20:29.319290 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6c8e24d7-6127-4069-863e-90ea0d509873&page=queryresults
[0m19:20:31.345357 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba7e2d810>]}
[0m19:20:31.346490 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.66s]
[0m19:20:31.348188 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:20:31.349315 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m19:20:31.350094 [info ] [Thread-3 (]: 2 of 4 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m19:20:31.350806 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m19:20:31.351395 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m19:20:31.358465 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m19:20:31.359440 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m19:20:31.364139 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m19:20:31.561189 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m19:20:31.562523 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m19:20:32.041873 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0848cd6c-5e51-48f4-9c2b-f75ae83b16fe&page=queryresults
[0m19:20:34.342791 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba7d23e10>]}
[0m19:20:34.343771 [info ] [Thread-3 (]: 2 of 4 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (131.6k rows, 9.2 MiB processed)[0m in 2.99s]
[0m19:20:34.344689 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m19:20:34.345781 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m19:20:34.346542 [info ] [Thread-2 (]: 3 of 4 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m19:20:34.347190 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__monthly_player_games)
[0m19:20:34.347764 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m19:20:34.353566 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m19:20:34.354533 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m19:20:34.358913 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:20:34.493782 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m19:20:34.494994 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m19:20:34.960567 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:fd60b4ee-1754-4649-aeee-0ab92fde9aa2&page=queryresults
[0m19:20:40.146692 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba7e3e650>]}
[0m19:20:40.147666 [info ] [Thread-2 (]: 3 of 4 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (29.9k rows, 13.6 MiB processed)[0m in 5.80s]
[0m19:20:40.148587 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m19:20:40.149725 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m19:20:40.150449 [info ] [Thread-4 (]: 4 of 4 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m19:20:40.151213 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.monthly_player_openings'
[0m19:20:40.151797 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m19:20:40.159134 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m19:20:40.160044 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m19:20:40.163518 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:20:40.317791 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m19:20:40.318998 [debug] [Thread-4 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m19:20:40.600096 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:8534673a-e9ee-41ce-9d34-78c18d6f451a&page=queryresults
[0m19:20:42.608938 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4084bffc-aef4-48b8-b4ec-90dc9b3e8370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74aba40bf650>]}
[0m19:20:42.609899 [info ] [Thread-4 (]: 4 of 4 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (699.0 rows, 6.1 MiB processed)[0m in 2.46s]
[0m19:20:42.610834 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m19:20:42.612690 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:20:42.613615 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:20:42.614067 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m19:20:42.614514 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m19:20:42.614936 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m19:20:42.615351 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m19:20:42.615903 [info ] [MainThread]: 
[0m19:20:42.616402 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 15.35 seconds (15.35s).
[0m19:20:42.618127 [debug] [MainThread]: Command end result
[0m19:20:42.653204 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:20:42.655387 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:20:42.663885 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:20:42.664259 [info ] [MainThread]: 
[0m19:20:42.664591 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:20:42.664834 [info ] [MainThread]: 
[0m19:20:42.665094 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m19:20:42.665613 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.67049, "process_in_blocks": "0", "process_kernel_time": 0.346829, "process_mem_max_rss": "237656", "process_out_blocks": "3240", "process_user_time": 3.525211}
[0m19:20:42.665946 [debug] [MainThread]: Command `dbt run` succeeded at 19:20:42.665871 after 16.67 seconds
[0m19:20:42.666225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe03b5d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe0173e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74abe03c9e90>]}
[0m19:20:42.666512 [debug] [MainThread]: Flushing usage events
[0m19:20:43.092746 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:11:51.192692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a7a98d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a7a984d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a7a98b10>]}


============================== 18:11:51.200548 | 8f9f92f9-037c-4783-8a2e-564020a83e8f ==============================
[0m18:11:51.200548 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:11:51.200998 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt build', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:11:53.177931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60751a9b10>]}
[0m18:11:53.299015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60aa474190>]}
[0m18:11:53.299594 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:11:53.448225 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:11:53.647840 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m18:11:53.648304 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/stg__weekly_games.sql
[0m18:11:53.877082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606fbec690>]}
[0m18:11:53.942158 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:11:53.945394 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:11:54.091557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606f1226d0>]}
[0m18:11:54.092030 [info ] [MainThread]: Found 5 models, 1 source, 492 macros
[0m18:11:54.093810 [info ] [MainThread]: 
[0m18:11:54.094157 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:11:54.094419 [info ] [MainThread]: 
[0m18:11:54.094847 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:11:54.099209 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:11:54.100141 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:11:54.100687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:11:54.101465 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:11:54.102034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:11:54.102828 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:11:55.283261 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m18:11:55.284053 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m18:11:55.284837 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:11:55.285306 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:11:55.285863 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:11:55.286334 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:11:55.481903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606f36db10>]}
[0m18:11:55.482593 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:11:55.489273 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:11:55.490149 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:11:55.490856 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m18:11:55.491413 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:11:55.504998 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:11:55.511310 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:11:55.529102 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:11:55.749664 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:11:55.754478 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:11:56.604237 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:719db5cb-f004-4e5b-a3a2-c379b53e1d04&page=queryresults
[0m18:11:58.857008 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606eb05710>]}
[0m18:11:58.858071 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.36s]
[0m18:11:58.859031 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:11:58.860202 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:11:58.860937 [info ] [Thread-3 (]: 2 of 5 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m18:11:58.861615 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m18:11:58.862173 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:11:58.869276 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:11:58.870383 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:11:58.873790 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:11:59.058033 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m18:11:59.060950 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m18:11:59.677190 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:b87d6a91-d9b2-4a3b-a048-0bff0878c0f2&page=queryresults
[0m18:12:02.037549 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606eb19190>]}
[0m18:12:02.038636 [info ] [Thread-3 (]: 2 of 5 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (35.3k rows, 2.5 MiB processed)[0m in 3.18s]
[0m18:12:02.039645 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:12:02.040857 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:12:02.041456 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m18:12:02.042339 [info ] [Thread-2 (]: 3 of 5 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m18:12:02.043124 [info ] [Thread-1 (]: 4 of 5 START sql table model dev_staging.stg__weekly_games ..................... [RUN]
[0m18:12:02.044052 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m18:12:02.044601 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m18:12:02.045147 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:12:02.045639 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m18:12:02.050241 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:12:02.056289 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m18:12:02.057290 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m18:12:02.057864 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:12:02.063114 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m18:12:02.066306 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:12:02.068765 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:12:02.069649 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:02.230094 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m18:12:02.231321 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:12:02.556365 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:dc865478-8740-4a91-814a-206177630430&page=queryresults
[0m18:12:02.649320 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:dc865478-8740-4a91-814a-206177630430&page=queryresults
[0m18:12:02.684231 [debug] [Thread-1 (]: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Unrecognized name: game_month at [47:15]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m18:12:02.685056 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606f4af810>]}
[0m18:12:02.685985 [error] [Thread-1 (]: 4 of 5 ERROR creating sql table model dev_staging.stg__weekly_games ............ [[31mERROR[0m in 0.64s]
[0m18:12:02.686870 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m18:12:02.687714 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__weekly_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Unrecognized name: game_month at [47:15]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql.
[0m18:12:03.117680 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3316655d-df78-40fe-b8ee-e5f4a55c4732&page=queryresults
[0m18:12:05.426107 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606eb30110>]}
[0m18:12:05.427109 [info ] [Thread-2 (]: 3 of 5 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (10.2k rows, 3.6 MiB processed)[0m in 3.38s]
[0m18:12:05.427883 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:12:05.428937 [debug] [Thread-3 (]: Began running node model.pipeline.monthly_player_openings
[0m18:12:05.429694 [info ] [Thread-3 (]: 5 of 5 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m18:12:05.430338 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m18:12:05.430905 [debug] [Thread-3 (]: Began compiling node model.pipeline.monthly_player_openings
[0m18:12:05.437029 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m18:12:05.439587 [debug] [Thread-3 (]: Began executing node model.pipeline.monthly_player_openings
[0m18:12:05.443438 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:12:05.583801 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m18:12:05.586615 [debug] [Thread-3 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:12:05.946794 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:44ef2cdb-4838-4edd-a655-83daf83198fb&page=queryresults
[0m18:12:07.742339 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8f9f92f9-037c-4783-8a2e-564020a83e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f606ea18dd0>]}
[0m18:12:07.743383 [info ] [Thread-3 (]: 5 of 5 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (107.0 rows, 1.8 MiB processed)[0m in 2.31s]
[0m18:12:07.744374 [debug] [Thread-3 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:12:07.746484 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:12:07.747415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:12:07.747885 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m18:12:07.748319 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m18:12:07.748764 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m18:12:07.749318 [info ] [MainThread]: 
[0m18:12:07.749821 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 13.65 seconds (13.65s).
[0m18:12:07.751786 [debug] [MainThread]: Command end result
[0m18:12:07.786805 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:12:07.788894 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:12:07.797452 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:12:07.797820 [info ] [MainThread]: 
[0m18:12:07.798145 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:12:07.798385 [info ] [MainThread]: 
[0m18:12:07.798683 [error] [MainThread]:   Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Unrecognized name: game_month at [47:15]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m18:12:07.798922 [info ] [MainThread]: 
[0m18:12:07.799167 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m18:12:07.799681 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": false, "command_wall_clock_time": 16.655071, "process_in_blocks": "196592", "process_kernel_time": 0.498869, "process_mem_max_rss": "270320", "process_out_blocks": "4592", "process_user_time": 3.944631}
[0m18:12:07.800013 [debug] [MainThread]: Command `dbt build` failed at 18:12:07.799939 after 16.66 seconds
[0m18:12:07.800305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a7a93dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a7a87d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a7a99450>]}
[0m18:12:07.800590 [debug] [MainThread]: Flushing usage events
[0m18:12:08.226310 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:18:33.275161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd219b97f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd219ddd9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd219b8f9d0>]}


============================== 18:18:33.278366 | 89abe407-5c1f-4623-b385-c4068ed355c3 ==============================
[0m18:18:33.278366 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:18:33.278818 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:18:33.933788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd21a2d0c50>]}
[0m18:18:34.056358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd21c585410>]}
[0m18:18:34.056967 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:18:34.178152 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:18:34.269977 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:18:34.270561 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__weekly_games.sql
[0m18:18:34.515221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e1d5ffd0>]}
[0m18:18:34.576096 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:18:34.578770 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:18:34.600113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e15cbc50>]}
[0m18:18:34.600676 [info ] [MainThread]: Found 5 models, 1 source, 492 macros
[0m18:18:34.602785 [info ] [MainThread]: 
[0m18:18:34.603164 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:18:34.603459 [info ] [MainThread]: 
[0m18:18:34.603943 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:18:34.608337 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:18:34.610594 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:18:34.611158 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:18:34.611763 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:18:34.612266 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:18:34.613135 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:18:35.794866 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m18:18:35.795619 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:18:35.796231 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:18:35.798751 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m18:18:35.799332 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:18:35.800002 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:18:35.992002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e1b78c50>]}
[0m18:18:35.992720 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:18:35.995987 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:18:35.996810 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:18:35.997263 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.calendar)
[0m18:18:35.997643 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:18:36.004704 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:18:36.005176 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:18:36.017188 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:18:36.230681 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:18:36.231383 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      

WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:18:36.714875 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c1cd8406-3bbf-41ff-8b84-e0166e4de096&page=queryresults
[0m18:18:38.944909 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e1616850>]}
[0m18:18:38.945638 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.95s]
[0m18:18:38.946271 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:18:38.947138 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:18:38.947734 [info ] [Thread-3 (]: 2 of 5 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m18:18:38.948170 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m18:18:38.948549 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:18:38.952369 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:18:38.952847 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:18:38.955186 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:18:39.129286 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m18:18:39.130612 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m18:18:39.463564 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7a636eed-213e-4c5c-9154-e4cdd1c63d97&page=queryresults
[0m18:18:41.275803 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e17f0dd0>]}
[0m18:18:41.276504 [info ] [Thread-3 (]: 2 of 5 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (35.3k rows, 2.5 MiB processed)[0m in 2.33s]
[0m18:18:41.277106 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:18:41.277957 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:18:41.278354 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m18:18:41.278858 [info ] [Thread-2 (]: 3 of 5 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m18:18:41.279445 [info ] [Thread-1 (]: 4 of 5 START sql table model dev_staging.stg__weekly_games ..................... [RUN]
[0m18:18:41.280032 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__monthly_player_games)
[0m18:18:41.280468 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m18:18:41.280963 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:18:41.281369 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m18:18:41.284644 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:18:41.290386 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m18:18:41.291354 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m18:18:41.291889 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:18:41.296260 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m18:18:41.299501 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:18:41.302090 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              week_start
            , week_number
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:18:41.302983 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:18:41.518212 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m18:18:41.519420 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:18:41.829122 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:fc824888-4cf5-4e16-82b8-64f1da8b79cb&page=queryresults
[0m18:18:42.005847 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:178a262e-4710-443f-b621-bac322e11862&page=queryresults
[0m18:18:44.195983 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e0096ad0>]}
[0m18:18:44.196978 [info ] [Thread-1 (]: 4 of 5 OK created sql table model dev_staging.stg__weekly_games ................ [[32mCREATE TABLE (11.6k rows, 3.2 MiB processed)[0m in 2.92s]
[0m18:18:44.197891 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m18:18:44.954695 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e00a67d0>]}
[0m18:18:44.955745 [info ] [Thread-2 (]: 3 of 5 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (10.2k rows, 3.6 MiB processed)[0m in 3.67s]
[0m18:18:44.956745 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:18:44.957776 [debug] [Thread-3 (]: Began running node model.pipeline.monthly_player_openings
[0m18:18:44.958617 [info ] [Thread-3 (]: 5 of 5 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m18:18:44.959247 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m18:18:44.959805 [debug] [Thread-3 (]: Began compiling node model.pipeline.monthly_player_openings
[0m18:18:44.963919 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m18:18:44.964520 [debug] [Thread-3 (]: Began executing node model.pipeline.monthly_player_openings
[0m18:18:44.966402 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:18:45.118954 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m18:18:45.120143 [debug] [Thread-3 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:18:45.610213 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f2165e52-4b43-4634-b5ce-84f326f997c6&page=queryresults
[0m18:18:47.325116 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89abe407-5c1f-4623-b385-c4068ed355c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1e00a67d0>]}
[0m18:18:47.326087 [info ] [Thread-3 (]: 5 of 5 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (107.0 rows, 1.8 MiB processed)[0m in 2.37s]
[0m18:18:47.326944 [debug] [Thread-3 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:18:47.328615 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:18:47.329533 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:18:47.329988 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m18:18:47.330424 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m18:18:47.330861 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m18:18:47.331407 [info ] [MainThread]: 
[0m18:18:47.331915 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 12.73 seconds (12.73s).
[0m18:18:47.333853 [debug] [MainThread]: Command end result
[0m18:18:47.370037 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:18:47.372135 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:18:47.380359 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:18:47.380648 [info ] [MainThread]: 
[0m18:18:47.380946 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:18:47.381190 [info ] [MainThread]: 
[0m18:18:47.381450 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m18:18:47.381978 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 14.149154, "process_in_blocks": "0", "process_kernel_time": 0.353436, "process_mem_max_rss": "245412", "process_out_blocks": "3312", "process_user_time": 3.670087}
[0m18:18:47.382335 [debug] [MainThread]: Command `dbt build` succeeded at 18:18:47.382252 after 14.15 seconds
[0m18:18:47.382644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd219bc9610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd219bebd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd21a8a6ed0>]}
[0m18:18:47.382930 [debug] [MainThread]: Flushing usage events
[0m18:18:47.827142 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:41:10.861743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfc9a950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfc90cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfede490>]}


============================== 18:41:10.864753 | ec0a518f-6022-49fb-bc64-009073b10217 ==============================
[0m18:41:10.864753 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:41:10.865195 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt build', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:41:11.514819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfd1a710>]}
[0m18:41:11.624665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea7e1ba10>]}
[0m18:41:11.625218 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:41:11.742806 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:41:11.838280 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m18:41:11.838788 [debug] [MainThread]: Partial parsing: added file: pipeline://models/aggregate/weekly_openings.sql
[0m18:41:11.839174 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m18:41:12.149447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea7b4ea50>]}
[0m18:41:12.216370 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:41:12.220030 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:41:12.238731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea779a0d0>]}
[0m18:41:12.239203 [info ] [MainThread]: Found 6 models, 1 source, 492 macros
[0m18:41:12.241040 [info ] [MainThread]: 
[0m18:41:12.241347 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:41:12.241623 [info ] [MainThread]: 
[0m18:41:12.242032 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:41:12.246209 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:41:12.247085 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:41:12.247919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:12.248589 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:41:12.249190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:12.250013 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:13.509669 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m18:41:13.510573 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m18:41:13.511419 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:41:13.512205 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:41:13.512869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:41:13.513409 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:41:13.745197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea7e0ef50>]}
[0m18:41:13.745884 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:41:13.749303 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:41:13.750156 [info ] [Thread-1 (]: 1 of 6 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:41:13.750840 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.calendar)
[0m18:41:13.751393 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:41:13.763640 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:41:13.764536 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:41:13.778084 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:41:13.973080 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:41:13.974413 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:41:14.320848 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d6402ae5-f262-4c74-aec2-a2bcc2535694&page=queryresults
[0m18:41:16.555223 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea7f9d350>]}
[0m18:41:16.556243 [info ] [Thread-1 (]: 1 of 6 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.80s]
[0m18:41:16.557164 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:41:16.558375 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:41:16.559167 [info ] [Thread-3 (]: 2 of 6 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m18:41:16.559923 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m18:41:16.560521 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:41:16.568300 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:41:16.569283 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:41:16.573159 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:41:16.752197 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m18:41:16.753652 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m18:41:17.221581 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:433fdeb2-8dc7-4ba8-9a72-dbff55fc1207&page=queryresults
[0m18:41:19.271074 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea428b890>]}
[0m18:41:19.272145 [info ] [Thread-3 (]: 2 of 6 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (35.3k rows, 2.5 MiB processed)[0m in 2.71s]
[0m18:41:19.273164 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:41:19.274340 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:41:19.275187 [info ] [Thread-2 (]: 3 of 6 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m18:41:19.275847 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m18:41:19.276500 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__monthly_player_games)
[0m18:41:19.277274 [info ] [Thread-1 (]: 4 of 6 START sql table model dev_staging.stg__weekly_games ..................... [RUN]
[0m18:41:19.277964 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:41:19.278634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m18:41:19.286546 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:41:19.287352 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m18:41:19.293983 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m18:41:19.294746 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:41:19.297986 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:41:19.300117 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m18:41:19.303356 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:41:19.449786 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m18:41:19.451104 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:41:19.482545 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m18:41:19.483865 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              week_start
            , week_number
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:41:19.771768 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:eedea0d2-9784-42a8-971c-31b9f60a48aa&page=queryresults
[0m18:41:19.886591 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2fc12ae4-7ca9-4dec-8a1a-dbb1ddfa22c1&page=queryresults
[0m18:41:21.995067 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea77fcb50>]}
[0m18:41:21.996329 [info ] [Thread-2 (]: 3 of 6 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (10.2k rows, 3.6 MiB processed)[0m in 2.72s]
[0m18:41:21.997338 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:41:21.998316 [debug] [Thread-3 (]: Began running node model.pipeline.monthly_player_openings
[0m18:41:21.999083 [info ] [Thread-3 (]: 5 of 6 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m18:41:21.999780 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m18:41:22.000365 [debug] [Thread-3 (]: Began compiling node model.pipeline.monthly_player_openings
[0m18:41:22.006932 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m18:41:22.007879 [debug] [Thread-3 (]: Began executing node model.pipeline.monthly_player_openings
[0m18:41:22.011579 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:41:22.186638 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m18:41:22.187940 [debug] [Thread-3 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:41:22.623251 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:805e5f8c-8609-4fa3-b72c-55536b5ba934&page=queryresults
[0m18:41:22.712911 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea4245810>]}
[0m18:41:22.714419 [info ] [Thread-1 (]: 4 of 6 OK created sql table model dev_staging.stg__weekly_games ................ [[32mCREATE TABLE (11.6k rows, 3.2 MiB processed)[0m in 3.43s]
[0m18:41:22.716883 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m18:41:22.717906 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_openings
[0m18:41:22.718701 [info ] [Thread-2 (]: 6 of 6 START sql table model dev_aggregate.weekly_openings ..................... [RUN]
[0m18:41:22.719462 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__monthly_player_games, now model.pipeline.weekly_openings)
[0m18:41:22.720074 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_openings
[0m18:41:22.726685 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m18:41:22.727548 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_openings
[0m18:41:22.732226 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m18:41:22.733378 [debug] [Thread-2 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.week_start
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.week_start BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    week_start DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:41:22.734275 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:41:23.217405 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:92d0272d-4286-47dd-9e89-27bf583be892&page=queryresults
[0m18:41:23.313458 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:92d0272d-4286-47dd-9e89-27bf583be892&page=queryresults
[0m18:41:23.320822 [debug] [Thread-2 (]: Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Unrecognized name: username at [71:5]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql
[0m18:41:23.321644 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea41a3f50>]}
[0m18:41:23.322679 [error] [Thread-2 (]: 6 of 6 ERROR creating sql table model dev_aggregate.weekly_openings ............ [[31mERROR[0m in 0.60s]
[0m18:41:23.323650 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_openings
[0m18:41:23.324600 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_openings' to be skipped because of status 'error'.  Reason: Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Unrecognized name: username at [71:5]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql.
[0m18:41:24.371355 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec0a518f-6022-49fb-bc64-009073b10217', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ea779bdd0>]}
[0m18:41:24.372419 [info ] [Thread-3 (]: 5 of 6 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (107.0 rows, 1.8 MiB processed)[0m in 2.37s]
[0m18:41:24.373404 [debug] [Thread-3 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:41:24.375276 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:41:24.376193 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:41:24.376656 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m18:41:24.377085 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m18:41:24.377518 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m18:41:24.378064 [info ] [MainThread]: 
[0m18:41:24.378563 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 12.14 seconds (12.14s).
[0m18:41:24.380774 [debug] [MainThread]: Command end result
[0m18:41:24.416978 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:41:24.418902 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:41:24.427182 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:41:24.427563 [info ] [MainThread]: 
[0m18:41:24.427872 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:41:24.428119 [info ] [MainThread]: 
[0m18:41:24.428416 [error] [MainThread]:   Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Unrecognized name: username at [71:5]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql
[0m18:41:24.428676 [info ] [MainThread]: 
[0m18:41:24.428932 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
[0m18:41:24.429436 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": false, "command_wall_clock_time": 13.614149, "process_in_blocks": "0", "process_kernel_time": 0.364326, "process_mem_max_rss": "246804", "process_out_blocks": "3376", "process_user_time": 3.810433}
[0m18:41:24.429790 [debug] [MainThread]: Command `dbt build` failed at 18:41:24.429714 after 13.61 seconds
[0m18:41:24.430081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfc93790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfc902d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3edfc91710>]}
[0m18:41:24.430367 [debug] [MainThread]: Flushing usage events
[0m18:41:24.880475 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:41:43.313942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bc3f9d6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bc3f9d090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bc3f9cf10>]}


============================== 18:41:43.316923 | 98c5d54e-ffa9-43e6-a4f8-b9395888b369 ==============================
[0m18:41:43.316923 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:41:43.317359 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:41:43.965188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b901eb3d0>]}
[0m18:41:44.088034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bc69dd3d0>]}
[0m18:41:44.088645 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:41:44.209797 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:41:44.303269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:41:44.303824 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m18:41:44.538233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b8bc47dd0>]}
[0m18:41:44.603247 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:41:44.605557 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:41:44.624142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b8ba4f3d0>]}
[0m18:41:44.624625 [info ] [MainThread]: Found 6 models, 1 source, 492 macros
[0m18:41:44.626450 [info ] [MainThread]: 
[0m18:41:44.626769 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:41:44.627032 [info ] [MainThread]: 
[0m18:41:44.627450 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:41:44.631746 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:41:44.632875 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:41:44.633423 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:44.634192 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:41:44.634756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:44.635388 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:45.758413 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:41:45.759306 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m18:41:45.759809 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:41:45.760479 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m18:41:45.761047 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:41:45.762660 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:41:45.916143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b8bf49dd0>]}
[0m18:41:45.916855 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:41:45.919907 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:41:45.920724 [info ] [Thread-1 (]: 1 of 6 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:41:45.921365 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.calendar)
[0m18:41:45.921920 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:41:45.934046 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:41:45.934967 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:41:45.948784 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:41:46.165143 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:41:46.166377 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:41:46.554141 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ee879a84-60b3-4273-b9d5-ed855bb1b0b2&page=queryresults
[0m18:41:48.459301 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b6bd50490>]}
[0m18:41:48.460354 [info ] [Thread-1 (]: 1 of 6 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.54s]
[0m18:41:48.461351 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:41:48.462649 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m18:41:48.463344 [info ] [Thread-3 (]: 2 of 6 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m18:41:48.464020 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m18:41:48.464587 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m18:41:48.471914 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m18:41:48.472800 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m18:41:48.476182 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:41:48.645693 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m18:41:48.647120 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m18:41:49.129105 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:23aecf1e-58d8-4b4f-a120-25ae7a025948&page=queryresults
[0m18:41:51.148811 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b8baccd50>]}
[0m18:41:51.149829 [info ] [Thread-3 (]: 2 of 6 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (35.3k rows, 2.5 MiB processed)[0m in 2.68s]
[0m18:41:51.150757 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m18:41:51.151872 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m18:41:51.152427 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m18:41:51.153139 [info ] [Thread-2 (]: 3 of 6 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m18:41:51.153899 [info ] [Thread-1 (]: 4 of 6 START sql table model dev_staging.stg__weekly_games ..................... [RUN]
[0m18:41:51.154580 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__monthly_player_games)
[0m18:41:51.155168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m18:41:51.155749 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m18:41:51.156295 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m18:41:51.162019 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m18:41:51.168152 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m18:41:51.169137 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m18:41:51.169659 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m18:41:51.172903 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:41:51.175958 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:41:51.348404 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m18:41:51.349656 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:41:51.353331 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m18:41:51.354476 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              week_start
            , week_number
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m18:41:51.741844 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:04913420-f019-4361-8afd-d468d3d8cdc1&page=queryresults
[0m18:41:51.761143 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e3d099f5-df42-439f-aafa-f77e2f8d7051&page=queryresults
[0m18:41:54.051144 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b6bdfdc90>]}
[0m18:41:54.052134 [info ] [Thread-2 (]: 3 of 6 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (10.2k rows, 3.6 MiB processed)[0m in 2.90s]
[0m18:41:54.052943 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m18:41:54.053716 [debug] [Thread-3 (]: Began running node model.pipeline.monthly_player_openings
[0m18:41:54.054431 [info ] [Thread-3 (]: 5 of 6 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m18:41:54.055111 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m18:41:54.055677 [debug] [Thread-3 (]: Began compiling node model.pipeline.monthly_player_openings
[0m18:41:54.061483 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m18:41:54.062299 [debug] [Thread-3 (]: Began executing node model.pipeline.monthly_player_openings
[0m18:41:54.065622 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m18:41:54.238576 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m18:41:54.239815 [debug] [Thread-3 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:41:54.355691 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b6bd53310>]}
[0m18:41:54.356657 [info ] [Thread-1 (]: 4 of 6 OK created sql table model dev_staging.stg__weekly_games ................ [[32mCREATE TABLE (11.6k rows, 3.2 MiB processed)[0m in 3.20s]
[0m18:41:54.357572 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m18:41:54.358543 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_openings
[0m18:41:54.359230 [info ] [Thread-2 (]: 6 of 6 START sql table model dev_aggregate.weekly_openings ..................... [RUN]
[0m18:41:54.359865 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__monthly_player_games, now model.pipeline.weekly_openings)
[0m18:41:54.360400 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_openings
[0m18:41:54.366317 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m18:41:54.367125 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_openings
[0m18:41:54.373130 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m18:41:54.374330 [debug] [Thread-2 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.week_start
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.week_start BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , ROUND(SAFE_DIVIDE(white_win_count, white_games)*100, 1) AS white_win_percentage
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
      , ROUND(SAFE_DIVIDE(black_win_count, black_games)*100, 1) AS black_win_percentage
  FROM cte_aggregate
  ORDER BY 
    week_start DESC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m18:41:54.375175 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m18:41:54.468573 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0c1930f3-6243-405e-be48-ec970f3a5cb1&page=queryresults
[0m18:41:55.241990 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:46ed99c7-c796-4e3c-b314-6ae6afb2ea64&page=queryresults
[0m18:41:56.486039 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b6bc4a9d0>]}
[0m18:41:56.487014 [info ] [Thread-3 (]: 5 of 6 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (107.0 rows, 1.8 MiB processed)[0m in 2.43s]
[0m18:41:56.487935 [debug] [Thread-3 (]: Finished running node model.pipeline.monthly_player_openings
[0m18:41:57.307693 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '98c5d54e-ffa9-43e6-a4f8-b9395888b369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b6bc0fd50>]}
[0m18:41:57.308635 [info ] [Thread-2 (]: 6 of 6 OK created sql table model dev_aggregate.weekly_openings ................ [[32mCREATE TABLE (746.0 rows, 1.9 MiB processed)[0m in 2.95s]
[0m18:41:57.309551 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_openings
[0m18:41:57.311356 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:41:57.312311 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:41:57.312773 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m18:41:57.313195 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m18:41:57.313624 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m18:41:57.314166 [info ] [MainThread]: 
[0m18:41:57.314680 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 12.69 seconds (12.69s).
[0m18:41:57.316870 [debug] [MainThread]: Command end result
[0m18:41:57.352231 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:41:57.354374 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:41:57.363041 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:41:57.363434 [info ] [MainThread]: 
[0m18:41:57.363850 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:41:57.364174 [info ] [MainThread]: 
[0m18:41:57.364464 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m18:41:57.365031 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 14.097905, "process_in_blocks": "0", "process_kernel_time": 0.351964, "process_mem_max_rss": "245768", "process_out_blocks": "3376", "process_user_time": 3.743655}
[0m18:41:57.365380 [debug] [MainThread]: Command `dbt build` succeeded at 18:41:57.365302 after 14.10 seconds
[0m18:41:57.365683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bc3fd1610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5bc3f94210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5b6bd5ea50>]}
[0m18:41:57.365971 [debug] [MainThread]: Flushing usage events
[0m18:41:57.808308 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:46:47.319876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0bc69ec50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0bc8e6110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0bc8d1bd0>]}


============================== 13:46:47.328034 | b12bccef-6956-44a4-8027-51bff1fcebd7 ==============================
[0m13:46:47.328034 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:46:47.328483 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt build', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:46:49.320516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe08b11b2d0>]}
[0m13:46:49.427950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0889c3dd0>]}
[0m13:46:49.428505 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:46:49.582181 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:46:49.788782 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:46:49.789686 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/monthly_player_openings.sql
[0m13:46:49.790324 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m13:46:50.035900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0888a7250>]}
[0m13:46:50.095031 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:46:50.097829 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:46:50.138427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0881f3710>]}
[0m13:46:50.138846 [info ] [MainThread]: Found 6 models, 1 source, 492 macros
[0m13:46:50.140574 [info ] [MainThread]: 
[0m13:46:50.140877 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:46:50.141151 [info ] [MainThread]: 
[0m13:46:50.141558 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:46:50.145823 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:46:50.146920 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:46:50.147439 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:46:50.148219 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:46:50.148813 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:46:50.149920 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:46:51.387346 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m13:46:51.388155 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m13:46:51.388924 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m13:46:51.389397 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:46:51.389925 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:46:51.390451 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:46:51.580446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0886e6e10>]}
[0m13:46:51.581210 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:46:51.586582 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m13:46:51.587409 [info ] [Thread-1 (]: 1 of 6 START sql table model dev_universal.calendar ............................ [RUN]
[0m13:46:51.588122 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m13:46:51.588710 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m13:46:51.598106 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m13:46:51.601358 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m13:46:51.614768 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:46:51.828944 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m13:46:51.834420 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m13:46:52.455041 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0516fd43-bfcc-4378-a75c-88f898bc9139&page=queryresults
[0m13:46:58.893063 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0bd459650>]}
[0m13:46:58.894150 [info ] [Thread-1 (]: 1 of 6 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 7.30s]
[0m13:46:58.895142 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m13:46:58.896222 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m13:46:58.896990 [info ] [Thread-3 (]: 2 of 6 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m13:46:58.897677 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m13:46:58.898240 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m13:46:58.905020 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m13:46:58.906059 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m13:46:58.909326 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:46:59.085329 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m13:46:59.088224 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m13:46:59.545190 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bc1bb240-c2f8-44d2-9685-b158d517578d&page=queryresults
[0m13:47:02.815854 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe080705510>]}
[0m13:47:02.816848 [info ] [Thread-3 (]: 2 of 6 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (34.0k rows, 2.4 MiB processed)[0m in 3.92s]
[0m13:47:02.817789 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m13:47:02.818856 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m13:47:02.819480 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m13:47:02.820236 [info ] [Thread-2 (]: 3 of 6 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m13:47:02.821001 [info ] [Thread-1 (]: 4 of 6 START sql table model dev_staging.stg__weekly_games ..................... [RUN]
[0m13:47:02.821664 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m13:47:02.822275 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m13:47:02.822849 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m13:47:02.823440 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m13:47:02.829244 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m13:47:02.835130 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m13:47:02.836028 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m13:47:02.836540 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m13:47:02.839987 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:47:02.843095 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:47:03.021137 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m13:47:03.023523 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m13:47:03.024677 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              week_start
            , week_number
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m13:47:03.025897 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m13:47:03.315499 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:de01da99-0610-467d-b6ae-895891775db1&page=queryresults
[0m13:47:03.473552 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3a47b67f-cb64-4717-b620-e33e81f015af&page=queryresults
[0m13:47:06.364519 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe08033fe10>]}
[0m13:47:06.365517 [info ] [Thread-1 (]: 4 of 6 OK created sql table model dev_staging.stg__weekly_games ................ [[32mCREATE TABLE (11.1k rows, 3.1 MiB processed)[0m in 3.54s]
[0m13:47:06.366199 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m13:47:06.366995 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m13:47:06.367723 [info ] [Thread-3 (]: 5 of 6 START sql table model dev_aggregate.weekly_openings ..................... [RUN]
[0m13:47:06.368371 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m13:47:06.368913 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_openings
[0m13:47:06.374697 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m13:47:06.377256 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_openings
[0m13:47:06.380950 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:47:06.528635 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe080361d50>]}
[0m13:47:06.529603 [info ] [Thread-2 (]: 3 of 6 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (9.9k rows, 3.5 MiB processed)[0m in 3.71s]
[0m13:47:06.530511 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m13:47:06.531367 [debug] [Thread-1 (]: Began running node model.pipeline.monthly_player_openings
[0m13:47:06.532081 [info ] [Thread-1 (]: 6 of 6 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m13:47:06.532743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__weekly_games, now model.pipeline.monthly_player_openings)
[0m13:47:06.533340 [debug] [Thread-1 (]: Began compiling node model.pipeline.monthly_player_openings
[0m13:47:06.539761 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m13:47:06.541944 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m13:47:06.542716 [debug] [Thread-1 (]: Began executing node model.pipeline.monthly_player_openings
[0m13:47:06.546237 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:47:06.548656 [debug] [Thread-3 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.week_start
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.week_start BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    week_start DESC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m13:47:06.718602 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m13:47:06.719799 [debug] [Thread-1 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(o.total)                                                              AS total_games

      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS white_games
      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.total      ELSE 0 END)   AS black_games
      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_games
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m13:47:06.898745 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:9da7a877-5718-417d-8cdc-0ceac0bfb97b&page=queryresults
[0m13:47:07.164690 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:09ca07ba-daa3-4a1a-9b39-f9e28190f11e&page=queryresults
[0m13:47:09.212398 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe080356110>]}
[0m13:47:09.213349 [info ] [Thread-3 (]: 5 of 6 OK created sql table model dev_aggregate.weekly_openings ................ [[32mCREATE TABLE (746.0 rows, 1.8 MiB processed)[0m in 2.84s]
[0m13:47:09.214258 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m13:47:09.478406 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b12bccef-6956-44a4-8027-51bff1fcebd7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0802feed0>]}
[0m13:47:09.479369 [info ] [Thread-1 (]: 6 of 6 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (107.0 rows, 1.8 MiB processed)[0m in 2.95s]
[0m13:47:09.480274 [debug] [Thread-1 (]: Finished running node model.pipeline.monthly_player_openings
[0m13:47:09.481955 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:47:09.482821 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:47:09.483322 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m13:47:09.483790 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m13:47:09.484280 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m13:47:09.484880 [info ] [MainThread]: 
[0m13:47:09.485448 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 19.34 seconds (19.34s).
[0m13:47:09.487836 [debug] [MainThread]: Command end result
[0m13:47:09.529374 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:47:09.531666 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:47:09.537411 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m13:47:09.537714 [info ] [MainThread]: 
[0m13:47:09.538047 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:47:09.538307 [info ] [MainThread]: 
[0m13:47:09.538584 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m13:47:09.539151 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 22.262558, "process_in_blocks": "196464", "process_kernel_time": 0.444806, "process_mem_max_rss": "244864", "process_out_blocks": "3376", "process_user_time": 3.808229}
[0m13:47:09.539520 [debug] [MainThread]: Command `dbt build` succeeded at 13:47:09.539439 after 22.26 seconds
[0m13:47:09.539830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0c03c9f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0bc694310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe0bc697e90>]}
[0m13:47:09.540149 [debug] [MainThread]: Flushing usage events
[0m13:47:09.971402 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:59:00.568316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4a99bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4a9eeb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4a998f10>]}


============================== 15:59:00.577592 | 823d221c-f0d1-4adc-bb2f-502ad358fb8f ==============================
[0m15:59:00.577592 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:59:00.578042 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m15:59:02.633851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa12b16f10>]}
[0m15:59:02.741262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4d3cd810>]}
[0m15:59:02.741806 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:59:02.891975 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:59:03.107748 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m15:59:03.108263 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__weekly_games.sql
[0m15:59:03.108577 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m15:59:03.108891 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/monthly_player_openings.sql
[0m15:59:03.328225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa12757c10>]}
[0m15:59:03.384872 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:59:03.388182 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:59:03.432261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa124339d0>]}
[0m15:59:03.432684 [info ] [MainThread]: Found 6 models, 1 source, 492 macros
[0m15:59:03.434351 [info ] [MainThread]: 
[0m15:59:03.434629 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:59:03.434875 [info ] [MainThread]: 
[0m15:59:03.435253 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:59:03.439152 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:59:03.440162 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:59:03.440541 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:03.441383 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:59:03.441893 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:03.442846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:04.681718 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m15:59:04.682581 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m15:59:04.683353 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m15:59:04.684006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:59:04.684591 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:59:04.685140 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:59:04.864409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa129735d0>]}
[0m15:59:04.865104 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:59:04.872294 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m15:59:04.873120 [info ] [Thread-1 (]: 1 of 6 START sql table model dev_universal.calendar ............................ [RUN]
[0m15:59:04.873772 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m15:59:04.874332 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m15:59:04.886479 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m15:59:04.892077 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m15:59:04.910289 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:59:05.091987 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m15:59:05.098166 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m15:59:05.562521 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:dd4e4377-ff33-4283-b68b-235b5dc0b807&page=queryresults
[0m15:59:07.783080 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa12641050>]}
[0m15:59:07.784106 [info ] [Thread-1 (]: 1 of 6 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.91s]
[0m15:59:07.785042 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m15:59:07.786180 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m15:59:07.786911 [info ] [Thread-3 (]: 2 of 6 START sql table model dev_staging.stg__player_games ..................... [RUN]
[0m15:59:07.787558 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m15:59:07.788151 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m15:59:07.795197 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m15:59:07.796384 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m15:59:07.800365 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:59:07.971910 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m15:59:07.974918 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
[0m15:59:08.300503 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3a0f7d16-f280-47de-9e3a-d310b22e3faa&page=queryresults
[0m15:59:10.934412 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa126b9210>]}
[0m15:59:10.935508 [info ] [Thread-3 (]: 2 of 6 OK created sql table model dev_staging.stg__player_games ................ [[32mCREATE TABLE (245.4k rows, 17.3 MiB processed)[0m in 3.15s]
[0m15:59:10.937230 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m15:59:10.938276 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m15:59:10.938853 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m15:59:10.939557 [info ] [Thread-2 (]: 3 of 6 START sql table model dev_staging.stg__monthly_player_games ............. [RUN]
[0m15:59:10.940377 [info ] [Thread-1 (]: 4 of 6 START sql table model dev_staging.stg__weekly_games ..................... [RUN]
[0m15:59:10.941037 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m15:59:10.941613 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m15:59:10.942186 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m15:59:10.942726 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m15:59:10.948680 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m15:59:10.955427 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m15:59:10.956441 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m15:59:10.956997 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m15:59:10.960336 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:59:10.963512 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:59:11.084329 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m15:59:11.085253 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m15:59:11.116358 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m15:59:11.117710 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND game_date BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              week_start
            , week_number
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m15:59:11.531196 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2a10fcd3-65bd-4727-a457-0001c27902b8&page=queryresults
[0m15:59:11.548711 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ced9365a-9e86-487d-a0ce-069c04f49cf4&page=queryresults
[0m15:59:16.329665 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa0273d750>]}
[0m15:59:16.330422 [info ] [Thread-2 (]: 3 of 6 OK created sql table model dev_staging.stg__monthly_player_games ........ [[32mCREATE TABLE (50.8k rows, 25.4 MiB processed)[0m in 5.39s]
[0m15:59:16.331115 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m15:59:16.331920 [debug] [Thread-3 (]: Began running node model.pipeline.monthly_player_openings
[0m15:59:16.332641 [info ] [Thread-3 (]: 5 of 6 START sql table model dev_aggregate.monthly_player_openings ............. [RUN]
[0m15:59:16.333303 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m15:59:16.333885 [debug] [Thread-3 (]: Began compiling node model.pipeline.monthly_player_openings
[0m15:59:16.339640 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m15:59:16.342092 [debug] [Thread-3 (]: Began executing node model.pipeline.monthly_player_openings
[0m15:59:16.347323 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:59:16.496049 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m15:59:16.498870 [debug] [Thread-3 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.game_month BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m15:59:16.949170 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:886cfd77-d2be-41a9-8c4c-35dad889bfaa&page=queryresults
[0m15:59:16.992999 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa100b5c50>]}
[0m15:59:16.993951 [info ] [Thread-1 (]: 4 of 6 OK created sql table model dev_staging.stg__weekly_games ................ [[32mCREATE TABLE (65.2k rows, 21.8 MiB processed)[0m in 6.05s]
[0m15:59:16.994864 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m15:59:16.995715 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_openings
[0m15:59:16.996400 [info ] [Thread-2 (]: 6 of 6 START sql table model dev_aggregate.weekly_openings ..................... [RUN]
[0m15:59:16.997047 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__monthly_player_games, now model.pipeline.weekly_openings)
[0m15:59:16.997595 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_openings
[0m15:59:17.003286 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m15:59:17.004170 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_openings
[0m15:59:17.007778 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:59:17.160882 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m15:59:17.162064 [debug] [Thread-2 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.week_start
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.week_start BETWEEN CURRENT_DATE()-60 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    week_start DESC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m15:59:17.584189 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:24388c2f-6366-4040-aeb2-34c3311c2609&page=queryresults
[0m15:59:19.230647 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa027ba550>]}
[0m15:59:19.231185 [info ] [Thread-3 (]: 5 of 6 OK created sql table model dev_aggregate.monthly_player_openings ........ [[32mCREATE TABLE (56.1k rows, 11.1 MiB processed)[0m in 2.90s]
[0m15:59:19.231650 [debug] [Thread-3 (]: Finished running node model.pipeline.monthly_player_openings
[0m15:59:19.647092 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '823d221c-f0d1-4adc-bb2f-502ad358fb8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa0260bf50>]}
[0m15:59:19.648122 [info ] [Thread-2 (]: 6 of 6 OK created sql table model dev_aggregate.weekly_openings ................ [[32mCREATE TABLE (1.7k rows, 12.6 MiB processed)[0m in 2.65s]
[0m15:59:19.649063 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_openings
[0m15:59:19.650732 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:59:19.651667 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:59:19.652141 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m15:59:19.652578 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m15:59:19.653021 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m15:59:19.653578 [info ] [MainThread]: 
[0m15:59:19.654082 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 16.22 seconds (16.22s).
[0m15:59:19.656561 [debug] [MainThread]: Command end result
[0m15:59:19.693703 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:59:19.695705 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:59:19.703882 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:59:19.704206 [info ] [MainThread]: 
[0m15:59:19.704532 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:59:19.704796 [info ] [MainThread]: 
[0m15:59:19.705088 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m15:59:19.705651 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 19.18057, "process_in_blocks": "195744", "process_kernel_time": 0.510845, "process_mem_max_rss": "240804", "process_out_blocks": "3376", "process_user_time": 3.762621}
[0m15:59:19.706042 [debug] [MainThread]: Command `dbt build` succeeded at 15:59:19.705959 after 19.18 seconds
[0m15:59:19.706356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4e5c9f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4abde210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa4e377d90>]}
[0m15:59:19.706670 [debug] [MainThread]: Flushing usage events
[0m15:59:20.136706 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:52:42.782342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ed67bed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ed67aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ed678590>]}


============================== 16:52:42.784938 | 5bf7adf7-41b1-4aca-b435-d43519eaaa71 ==============================
[0m16:52:42.784938 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:52:42.785383 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build --target prod', 'send_anonymous_usage_stats': 'True'}
[0m16:52:43.441760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b57d4c50>]}
[0m16:52:43.564886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b5a47890>]}
[0m16:52:43.565491 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:52:43.686276 [debug] [MainThread]: checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4, vars: {}, profile: , target: prod, version: 1.9.4
[0m16:52:43.763263 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m16:52:43.763694 [debug] [MainThread]: previous checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4, current checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c
[0m16:52:43.763965 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:52:43.764272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ee4b4150>]}
[0m16:52:44.707693 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m16:52:44.859294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b527ce90>]}
[0m16:52:44.923434 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:52:44.925690 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:52:44.943917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b5340910>]}
[0m16:52:44.944379 [info ] [MainThread]: Found 6 models, 1 source, 492 macros
[0m16:52:44.946342 [info ] [MainThread]: 
[0m16:52:44.946700 [info ] [MainThread]: Concurrency: 4 threads (target='prod')
[0m16:52:44.946977 [info ] [MainThread]: 
[0m16:52:44.947395 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:52:44.951765 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:52:44.952932 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:52:44.953474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:52:44.954369 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:52:44.954951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:52:44.955947 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:52:46.143955 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_prod_aggregate)
[0m16:52:46.144672 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_prod_staging)
[0m16:52:46.145207 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_prod_universal)
[0m16:52:46.145759 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "prod_aggregate"
"
[0m16:52:46.146434 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "prod_staging"
"
[0m16:52:46.146904 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "prod_universal"
"
[0m16:52:46.172110 [debug] [ThreadPool]: On create_checkmate-453316_prod_aggregate: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "connection_name": "create_checkmate-453316_prod_aggregate"} */
create schema if not exists `checkmate-453316`.`prod_aggregate`
  
[0m16:52:46.175485 [debug] [ThreadPool]: On create_checkmate-453316_prod_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "connection_name": "create_checkmate-453316_prod_staging"} */
create schema if not exists `checkmate-453316`.`prod_staging`
  
[0m16:52:46.178563 [debug] [ThreadPool]: On create_checkmate-453316_prod_universal: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "connection_name": "create_checkmate-453316_prod_universal"} */
create schema if not exists `checkmate-453316`.`prod_universal`
  
[0m16:52:46.179185 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:46.179772 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:46.180316 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:46.419052 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:36c53f58-b86e-4582-8b04-abc36f99eb47&page=queryresults
[0m16:52:46.432383 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:09ac79b6-69fd-44cd-b276-94c0002f2f90&page=queryresults
[0m16:52:46.546182 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:82f44654-8c4f-48a8-84be-784ac2a5f364&page=queryresults
[0m16:52:50.136492 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_prod_staging, now list_checkmate-453316_prod_aggregate)
[0m16:52:50.137397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_prod_aggregate, now list_checkmate-453316_prod_staging)
[0m16:52:50.137919 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:50.138705 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_prod_universal, now list_checkmate-453316_prod_universal)
[0m16:52:50.139317 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:50.140808 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:52:50.517165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b52f41d0>]}
[0m16:52:50.517925 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:52:50.521185 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:52:50.522081 [info ] [Thread-1 (]: 1 of 6 START sql table model prod_universal.calendar ........................... [RUN]
[0m16:52:50.522757 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_prod_universal, now model.pipeline.calendar)
[0m16:52:50.523325 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:52:50.537200 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:52:50.538216 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:52:50.571593 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m16:52:50.572367 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`prod_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit  
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit   
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit 
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS cal_month_start
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS cal_month_end
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1 
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m16:52:50.572868 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:52:51.576995 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:b4189378-32d1-4cea-8260-ffbe05f7508e&page=queryresults
[0m16:52:54.238806 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b432e1d0>]}
[0m16:52:54.239859 [info ] [Thread-1 (]: 1 of 6 OK created sql table model prod_universal.calendar ...................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.71s]
[0m16:52:54.240792 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:52:54.241977 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m16:52:54.242683 [info ] [Thread-3 (]: 2 of 6 START sql table model prod_staging.stg__player_games .................... [RUN]
[0m16:52:54.243353 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_prod_staging, now model.pipeline.stg__player_games)
[0m16:52:54.243907 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m16:52:54.250969 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:52:54.251854 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m16:52:54.255852 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m16:52:54.257078 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`prod_staging`.`stg__player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw   
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`prod_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  


)

SELECT * FROM cte_base
    );
  
[0m16:52:54.258037 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:52:55.399703 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:546f097f-cc39-4359-9b35-cf6f9fb3862e&page=queryresults
[0m16:52:59.642457 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b42bba50>]}
[0m16:52:59.643446 [info ] [Thread-3 (]: 2 of 6 OK created sql table model prod_staging.stg__player_games ............... [[32mCREATE TABLE (3.9m rows, 262.9 MiB processed)[0m in 5.40s]
[0m16:52:59.644385 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m16:52:59.645554 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m16:52:59.646166 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m16:52:59.646991 [info ] [Thread-2 (]: 3 of 6 START sql table model prod_staging.stg__monthly_player_games ............ [RUN]
[0m16:52:59.647830 [info ] [Thread-1 (]: 4 of 6 START sql table model prod_staging.stg__weekly_games .................... [RUN]
[0m16:52:59.648578 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_prod_aggregate, now model.pipeline.stg__monthly_player_games)
[0m16:52:59.649212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m16:52:59.649808 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m16:52:59.650398 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m16:52:59.657031 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m16:52:59.663190 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m16:52:59.664213 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m16:52:59.664784 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m16:52:59.669792 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m16:52:59.673726 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m16:52:59.675035 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`prod_staging`.`stg__monthly_player_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`prod_staging`.`stg__player_games` t
      WHERE 1=1
        
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT              
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate    
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m16:52:59.676240 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`prod_staging`.`stg__weekly_games`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_base_aggregate AS (
      SELECT
              cal.week_start
            , cal.week_number_type1                                     AS week_number
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`prod_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`prod_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              week_start
            , week_number
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m16:52:59.677147 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:52:59.677755 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:53:00.580372 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7dfe6664-9639-4e9a-b4de-9ad419886de8&page=queryresults
[0m16:53:00.689353 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c5dd73e0-fc74-4b28-a5ed-f1077536dad3&page=queryresults
[0m16:53:10.332214 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b422f9d0>]}
[0m16:53:10.333298 [info ] [Thread-1 (]: 4 of 6 OK created sql table model prod_staging.stg__weekly_games ............... [[32mCREATE TABLE (1.1m rows, 335.7 MiB processed)[0m in 10.68s]
[0m16:53:10.334398 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m16:53:10.335360 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m16:53:10.336165 [info ] [Thread-3 (]: 5 of 6 START sql table model prod_aggregate.weekly_openings .................... [RUN]
[0m16:53:10.336886 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m16:53:10.337480 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_openings
[0m16:53:10.343836 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m16:53:10.344711 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_openings
[0m16:53:10.349020 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m16:53:10.350110 [debug] [Thread-3 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`prod_aggregate`.`weekly_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.week_start
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`prod_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    week_start DESC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m16:53:10.350968 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:53:11.567289 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a29dadab-ecee-48f1-8fa6-7db26e22e444&page=queryresults
[0m16:53:13.535837 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b41e9550>]}
[0m16:53:13.536808 [info ] [Thread-2 (]: 3 of 6 OK created sql table model prod_staging.stg__monthly_player_games ....... [[32mCREATE TABLE (842.0k rows, 365.8 MiB processed)[0m in 13.89s]
[0m16:53:13.538695 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m16:53:13.539672 [debug] [Thread-1 (]: Began running node model.pipeline.monthly_player_openings
[0m16:53:13.540533 [info ] [Thread-1 (]: 6 of 6 START sql table model prod_aggregate.monthly_player_openings ............ [RUN]
[0m16:53:13.541224 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__weekly_games, now model.pipeline.monthly_player_openings)
[0m16:53:13.541897 [debug] [Thread-1 (]: Began compiling node model.pipeline.monthly_player_openings
[0m16:53:13.547674 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m16:53:13.548512 [debug] [Thread-1 (]: Began executing node model.pipeline.monthly_player_openings
[0m16:53:13.552462 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.monthly_player_openings"
[0m16:53:13.553195 [debug] [Thread-1 (]: On model.pipeline.monthly_player_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "model.pipeline.monthly_player_openings"} */

  
    

    create or replace table `checkmate-453316`.`prod_aggregate`.`monthly_player_openings`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      


WITH cte_aggregate AS (
    SELECT 
        games.game_month
      , games.game_month_str
      , games.username
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`prod_staging`.`stg__monthly_player_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316.universal.opening_map` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        game_month
      , game_month_str
      , username
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    game_month DESC
  , username ASC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m16:53:13.553748 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:53:14.031956 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b423c250>]}
[0m16:53:14.032944 [info ] [Thread-3 (]: 5 of 6 OK created sql table model prod_aggregate.weekly_openings ............... [[32mCREATE TABLE (23.1k rows, 181.3 MiB processed)[0m in 3.70s]
[0m16:53:14.033850 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m16:53:14.586895 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:b5cdea80-7bcc-4d15-9c1c-aa4670f85c5a&page=queryresults
[0m16:53:23.525000 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5bf7adf7-41b1-4aca-b435-d43519eaaa71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05b55be250>]}
[0m16:53:23.525983 [info ] [Thread-1 (]: 6 of 6 OK created sql table model prod_aggregate.monthly_player_openings ....... [[32mCREATE TABLE (1.0m rows, 160.6 MiB processed)[0m in 9.98s]
[0m16:53:23.526871 [debug] [Thread-1 (]: Finished running node model.pipeline.monthly_player_openings
[0m16:53:23.528643 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:53:23.529522 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:53:23.529990 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m16:53:23.530425 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m16:53:23.530904 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m16:53:23.531539 [info ] [MainThread]: 
[0m16:53:23.532114 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 38.58 seconds (38.58s).
[0m16:53:23.534587 [debug] [MainThread]: Command end result
[0m16:53:23.652846 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:53:23.654009 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:53:23.658663 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:53:23.658937 [info ] [MainThread]: 
[0m16:53:23.659257 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:53:23.659518 [info ] [MainThread]: 
[0m16:53:23.659790 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m16:53:23.660378 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 40.92529, "process_in_blocks": "0", "process_kernel_time": 0.320529, "process_mem_max_rss": "244464", "process_out_blocks": "3376", "process_user_time": 4.762279}
[0m16:53:23.660751 [debug] [MainThread]: Command `dbt build` succeeded at 16:53:23.660671 after 40.93 seconds
[0m16:53:23.661074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05ed6d4150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f1077f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f05f1713e50>]}
[0m16:53:23.661381 [debug] [MainThread]: Flushing usage events
[0m16:53:24.097429 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:33:10.790042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311c4eb1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311eec5d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311c48f9d0>]}


============================== 19:33:10.792406 | cfe4d790-e58c-45f4-8a67-bf986e5f061d ==============================
[0m19:33:10.792406 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:33:10.792863 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True'}
[0m19:33:11.442159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311c4a13d0>]}
[0m19:33:11.569178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311eedc950>]}
[0m19:33:11.569778 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:33:11.690335 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:33:11.759680 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:33:11.760075 [debug] [MainThread]: previous checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, current checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4
[0m19:33:11.760321 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:33:11.760598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311d2389d0>]}
[0m19:33:12.806968 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m19:33:12.964301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e823b7d0>]}
[0m19:33:13.035025 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:33:13.037325 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:33:13.047165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e3e77e90>]}
[0m19:33:13.047623 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m19:33:13.047936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e839ab90>]}
[0m19:33:13.049377 [info ] [MainThread]: 
[0m19:33:13.049678 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:33:13.049947 [info ] [MainThread]: 
[0m19:33:13.050354 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:33:13.051467 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:33:13.052201 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:33:14.117178 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev)
[0m19:33:14.117963 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev"
"
[0m19:33:14.132370 [debug] [ThreadPool]: On create_checkmate-453316_dev: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev"} */
create schema if not exists `checkmate-453316`.`dev`
  
[0m19:33:14.133034 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:33:14.530832 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:4e8d6841-f9f3-4c3c-b3ba-67fc74b4e25d&page=queryresults
[0m19:33:16.770338 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev, now list_checkmate-453316_dev_staging)
[0m19:33:16.771303 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev'
[0m19:33:16.771833 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:33:16.772963 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:33:16.773503 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:33:16.774007 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m19:33:16.776444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:33:16.777983 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:33:17.147018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e81fe710>]}
[0m19:33:17.147755 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:33:17.150996 [debug] [Thread-1 (]: Began running node seed.pipeline.opening_archetypes
[0m19:33:17.151876 [info ] [Thread-1 (]: 1 of 1 START seed file dev.opening_archetypes .................................. [RUN]
[0m19:33:17.152558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev, now seed.pipeline.opening_archetypes)
[0m19:33:17.153146 [debug] [Thread-1 (]: Began compiling node seed.pipeline.opening_archetypes
[0m19:33:17.153703 [debug] [Thread-1 (]: Began executing node seed.pipeline.opening_archetypes
[0m19:33:17.281539 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:33:20.462619 [debug] [Thread-1 (]: On seed.pipeline.opening_archetypes: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "seed.pipeline.opening_archetypes"} */

    alter table `checkmate-453316`.`dev`.`opening_archetypes` set OPTIONS()
  
[0m19:33:20.991232 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:1258dec2-d0c7-4bd5-bd44-6c9b65c665f6&page=queryresults
[0m19:33:21.593112 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.pipeline.opening_archetypes"
[0m19:33:21.623892 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cfe4d790-e58c-45f4-8a67-bf986e5f061d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e3f04250>]}
[0m19:33:21.624788 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file dev.opening_archetypes .............................. [[32mINSERT 2263[0m in 4.47s]
[0m19:33:21.625706 [debug] [Thread-1 (]: Finished running node seed.pipeline.opening_archetypes
[0m19:33:21.627486 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:33:21.628048 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:33:21.628295 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m19:33:21.628528 [debug] [MainThread]: Connection 'seed.pipeline.opening_archetypes' was properly closed.
[0m19:33:21.628750 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m19:33:21.628979 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m19:33:21.629225 [info ] [MainThread]: 
[0m19:33:21.629488 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 8.58 seconds (8.58s).
[0m19:33:21.630010 [debug] [MainThread]: Command end result
[0m19:33:21.648343 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:33:21.649579 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:33:21.653937 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:33:21.654197 [info ] [MainThread]: 
[0m19:33:21.654526 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:33:21.654770 [info ] [MainThread]: 
[0m19:33:21.655045 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:33:21.655618 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 10.90889, "process_in_blocks": "0", "process_kernel_time": 0.347851, "process_mem_max_rss": "254064", "process_out_blocks": "3184", "process_user_time": 4.566757}
[0m19:33:21.655992 [debug] [MainThread]: Command `dbt seed` succeeded at 19:33:21.655913 after 10.91 seconds
[0m19:33:21.656290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3120517d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3120517fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f311ff17cd0>]}
[0m19:33:21.656580 [debug] [MainThread]: Flushing usage events
[0m19:33:22.009802 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:38:16.601136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb76427bb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb76427b8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb764278850>]}


============================== 19:38:16.604132 | c880ef45-ec6f-4b7e-ba21-58937dcb3a30 ==============================
[0m19:38:16.604132 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:38:16.604626 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True'}
[0m19:38:17.282803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb730a17b10>]}
[0m19:38:17.395527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb766c71590>]}
[0m19:38:17.396089 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:38:17.516394 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:38:17.597299 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m19:38:17.597701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb76507ff10>]}
[0m19:38:18.660100 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m19:38:18.815905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb72bc50510>]}
[0m19:38:18.888036 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:38:18.890555 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:38:18.900130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb72bb92190>]}
[0m19:38:18.900597 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m19:38:18.900912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb72bc8a590>]}
[0m19:38:18.902350 [info ] [MainThread]: 
[0m19:38:18.902645 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:38:18.902917 [info ] [MainThread]: 
[0m19:38:18.903325 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:38:18.904376 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:38:18.905105 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:19.876211 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m19:38:19.877100 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m19:38:19.877765 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:38:19.878540 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:38:19.879018 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:19.881150 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:20.067530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb72bcfa6d0>]}
[0m19:38:20.068311 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:38:20.071846 [debug] [Thread-1 (]: Began running node seed.pipeline.opening_mapping
[0m19:38:20.072650 [info ] [Thread-1 (]: 1 of 1 START seed file dev_universal.opening_mapping ........................... [RUN]
[0m19:38:20.073166 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m19:38:20.073480 [debug] [Thread-1 (]: Began compiling node seed.pipeline.opening_mapping
[0m19:38:20.073770 [debug] [Thread-1 (]: Began executing node seed.pipeline.opening_mapping
[0m19:38:20.123951 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:38:23.484333 [debug] [Thread-1 (]: On seed.pipeline.opening_mapping: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "seed.pipeline.opening_mapping"} */

    alter table `checkmate-453316`.`dev_universal`.`opening_mapping` set OPTIONS()
  
[0m19:38:23.677879 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:60edaef1-9e00-419c-8699-22556afaa78f&page=queryresults
[0m19:38:24.021236 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.pipeline.opening_mapping"
[0m19:38:24.057586 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c880ef45-ec6f-4b7e-ba21-58937dcb3a30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb70be806d0>]}
[0m19:38:24.058713 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file dev_universal.opening_mapping ....................... [[32mINSERT 2263[0m in 3.98s]
[0m19:38:24.059698 [debug] [Thread-1 (]: Finished running node seed.pipeline.opening_mapping
[0m19:38:24.061452 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:38:24.062192 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:38:24.062530 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m19:38:24.062872 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m19:38:24.063189 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m19:38:24.063537 [info ] [MainThread]: 
[0m19:38:24.063912 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 5.16 seconds (5.16s).
[0m19:38:24.064645 [debug] [MainThread]: Command end result
[0m19:38:24.085645 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:38:24.086901 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:38:24.091760 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:38:24.092045 [info ] [MainThread]: 
[0m19:38:24.092404 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:38:24.092660 [info ] [MainThread]: 
[0m19:38:24.092945 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:38:24.093491 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 7.540371, "process_in_blocks": "0", "process_kernel_time": 0.338319, "process_mem_max_rss": "249064", "process_out_blocks": "3192", "process_user_time": 4.517129}
[0m19:38:24.093876 [debug] [MainThread]: Command `dbt seed` succeeded at 19:38:24.093784 after 7.54 seconds
[0m19:38:24.094187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7642d6410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb7644fdbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb764267e50>]}
[0m19:38:24.094496 [debug] [MainThread]: Flushing usage events
[0m19:38:24.458087 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:44:00.946802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04f9a7d210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04f9cc1e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04f9cc1d50>]}


============================== 19:44:00.949949 | 4934c30d-33e7-4479-ae18-27a1b622c4ad ==============================
[0m19:44:00.949949 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:44:00.950405 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt seed', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:44:01.651356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04c1bb3450>]}
[0m19:44:01.778393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04fc475790>]}
[0m19:44:01.778959 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:44:01.901164 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:44:02.006235 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:44:02.006644 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:44:02.049618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04cd7abb50>]}
[0m19:44:02.125740 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:44:02.128110 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:44:02.137667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04c19a0c10>]}
[0m19:44:02.138297 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m19:44:02.138636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04c1be4350>]}
[0m19:44:02.140359 [info ] [MainThread]: 
[0m19:44:02.140667 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:44:02.140940 [info ] [MainThread]: 
[0m19:44:02.141372 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:44:02.143055 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:44:02.143746 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:44:03.114539 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m19:44:03.115461 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m19:44:03.116082 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:44:03.116769 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m19:44:03.117238 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:44:03.119382 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:44:03.330213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04c2353f10>]}
[0m19:44:03.330757 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:44:03.333315 [debug] [Thread-1 (]: Began running node seed.pipeline.opening_mapping
[0m19:44:03.334228 [info ] [Thread-1 (]: 1 of 1 START seed file dev_universal.opening_mapping ........................... [RUN]
[0m19:44:03.334924 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m19:44:03.335490 [debug] [Thread-1 (]: Began compiling node seed.pipeline.opening_mapping
[0m19:44:03.336056 [debug] [Thread-1 (]: Began executing node seed.pipeline.opening_mapping
[0m19:44:03.398509 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:44:05.788943 [debug] [Thread-1 (]: On seed.pipeline.opening_mapping: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "seed.pipeline.opening_mapping"} */

    alter table `checkmate-453316`.`dev_universal`.`opening_mapping` set OPTIONS()
  
[0m19:44:06.011124 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:beab6651-ae5a-42b4-8a3b-57bcc09db592&page=queryresults
[0m19:44:06.336197 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.pipeline.opening_mapping"
[0m19:44:06.367399 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4934c30d-33e7-4479-ae18-27a1b622c4ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04c02fc3d0>]}
[0m19:44:06.368319 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file dev_universal.opening_mapping ....................... [[32mINSERT 2263[0m in 3.03s]
[0m19:44:06.369243 [debug] [Thread-1 (]: Finished running node seed.pipeline.opening_mapping
[0m19:44:06.371099 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:44:06.372106 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:44:06.372513 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m19:44:06.372836 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m19:44:06.373132 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m19:44:06.373462 [info ] [MainThread]: 
[0m19:44:06.373824 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 4.23 seconds (4.23s).
[0m19:44:06.374522 [debug] [MainThread]: Command end result
[0m19:44:06.395930 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:44:06.397244 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:44:06.402343 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:44:06.402658 [info ] [MainThread]: 
[0m19:44:06.403030 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:44:06.403290 [info ] [MainThread]: 
[0m19:44:06.403569 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:44:06.404137 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 5.5050845, "process_in_blocks": "0", "process_kernel_time": 0.331628, "process_mem_max_rss": "245072", "process_out_blocks": "2144", "process_user_time": 3.420564}
[0m19:44:06.404528 [debug] [MainThread]: Command `dbt seed` succeeded at 19:44:06.404443 after 5.51 seconds
[0m19:44:06.404894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04f9ad2790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04f9ad87d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04f9a6be10>]}
[0m19:44:06.405258 [debug] [MainThread]: Flushing usage events
[0m19:44:06.781362 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:45:20.501597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8b79b110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8b9de110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8b79bfd0>]}


============================== 19:45:20.504760 | 947cda48-2e32-4d5a-807f-902265c641a5 ==============================
[0m19:45:20.504760 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:45:20.505222 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt seed --target prod', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:45:21.177830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b53af3290>]}
[0m19:45:21.304610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8e1cd990>]}
[0m19:45:21.305348 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:45:21.437903 [debug] [MainThread]: checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4, vars: {}, profile: , target: prod, version: 1.9.4
[0m19:45:21.528510 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:45:21.528951 [debug] [MainThread]: previous checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4, current checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c
[0m19:45:21.529216 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:45:21.529514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8c5cc190>]}
[0m19:45:22.585368 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m19:45:22.742375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b53228910>]}
[0m19:45:22.814784 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:45:22.817091 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:45:22.826684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b5316a0d0>]}
[0m19:45:22.827156 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m19:45:22.827455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b53396cd0>]}
[0m19:45:22.828897 [info ] [MainThread]: 
[0m19:45:22.829191 [info ] [MainThread]: Concurrency: 4 threads (target='prod')
[0m19:45:22.829450 [info ] [MainThread]: 
[0m19:45:22.829865 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:45:22.830845 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:45:22.831345 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:23.754865 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_prod_universal)
[0m19:45:23.755793 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_prod_aggregate'
[0m19:45:23.756285 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:45:23.757109 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_prod_staging'
[0m19:45:23.757662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:23.759752 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:23.922619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b53789990>]}
[0m19:45:23.923346 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:45:23.926566 [debug] [Thread-1 (]: Began running node seed.pipeline.opening_mapping
[0m19:45:23.927147 [info ] [Thread-1 (]: 1 of 1 START seed file prod_universal.opening_mapping .......................... [RUN]
[0m19:45:23.927558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_prod_universal, now seed.pipeline.opening_mapping)
[0m19:45:23.927884 [debug] [Thread-1 (]: Began compiling node seed.pipeline.opening_mapping
[0m19:45:23.928192 [debug] [Thread-1 (]: Began executing node seed.pipeline.opening_mapping
[0m19:45:23.983164 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:45:27.048287 [debug] [Thread-1 (]: On seed.pipeline.opening_mapping: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "seed.pipeline.opening_mapping"} */

    alter table `checkmate-453316`.`prod_universal`.`opening_mapping` set OPTIONS()
  
[0m19:45:27.232570 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3a9aeb29-b908-44fe-b040-34d1478ef517&page=queryresults
[0m19:45:27.559071 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.pipeline.opening_mapping"
[0m19:45:27.590091 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '947cda48-2e32-4d5a-807f-902265c641a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b533e7e10>]}
[0m19:45:27.590771 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file prod_universal.opening_mapping ...................... [[32mINSERT 2263[0m in 3.66s]
[0m19:45:27.591397 [debug] [Thread-1 (]: Finished running node seed.pipeline.opening_mapping
[0m19:45:27.593141 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:45:27.594064 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:45:27.594508 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m19:45:27.594951 [debug] [MainThread]: Connection 'list_checkmate-453316_prod_aggregate' was properly closed.
[0m19:45:27.595370 [debug] [MainThread]: Connection 'list_checkmate-453316_prod_staging' was properly closed.
[0m19:45:27.595838 [info ] [MainThread]: 
[0m19:45:27.596350 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 4.77 seconds (4.77s).
[0m19:45:27.597323 [debug] [MainThread]: Command end result
[0m19:45:27.631808 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:45:27.633807 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:45:27.642031 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:45:27.642522 [info ] [MainThread]: 
[0m19:45:27.643157 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:45:27.643591 [info ] [MainThread]: 
[0m19:45:27.643951 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:45:27.644644 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 7.191696, "process_in_blocks": "0", "process_kernel_time": 0.366543, "process_mem_max_rss": "252668", "process_out_blocks": "3184", "process_user_time": 4.55352}
[0m19:45:27.645143 [debug] [MainThread]: Command `dbt seed` succeeded at 19:45:27.645034 after 7.19 seconds
[0m19:45:27.645547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8b790350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8b9de150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1b8f177f10>]}
[0m19:45:27.645947 [debug] [MainThread]: Flushing usage events
[0m19:45:27.995291 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:47:26.682442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7301095e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73010ef690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73010eef10>]}


============================== 19:47:26.685060 | 73333b19-8589-497d-aafd-d8ea891f6f80 ==============================
[0m19:47:26.685060 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:47:26.685543 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt compile', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:47:27.362951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '73333b19-8589-497d-aafd-d8ea891f6f80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f72cd350c10>]}
[0m19:47:27.478190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '73333b19-8589-497d-aafd-d8ea891f6f80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7303acd750>]}
[0m19:47:27.478788 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:47:27.601027 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:47:27.687740 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:47:27.688207 [debug] [MainThread]: previous checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, current checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4
[0m19:47:27.688478 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:47:27.688792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '73333b19-8589-497d-aafd-d8ea891f6f80', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7301e97fd0>]}
[0m19:47:28.754101 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m19:47:28.831291 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.pipeline.monthly_player_openings' (models/aggregate/monthly_player_openings.sql) depends on a node named 'opening_map' which was not found
[0m19:47:28.832018 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 2.1966367, "process_in_blocks": "0", "process_kernel_time": 0.325061, "process_mem_max_rss": "238872", "process_out_blocks": "16", "process_user_time": 4.128483}
[0m19:47:28.832418 [debug] [MainThread]: Command `dbt compile` failed at 19:47:28.832329 after 2.20 seconds
[0m19:47:28.832754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73010974d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7301094550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7304b17f10>]}
[0m19:47:28.833108 [debug] [MainThread]: Flushing usage events
[0m19:47:29.189975 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:51:28.605551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c920fc650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c91fddad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c91dee610>]}


============================== 19:51:28.608281 | f7d8d01b-9741-434f-ac3d-79df4621e44d ==============================
[0m19:51:28.608281 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:51:28.608725 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt seed', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:51:29.297200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7d8d01b-9741-434f-ac3d-79df4621e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c6052cd10>]}
[0m19:51:29.426507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7d8d01b-9741-434f-ac3d-79df4621e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c947cdb90>]}
[0m19:51:29.427126 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:51:29.550909 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:51:29.631074 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:51:29.631460 [debug] [MainThread]: previous checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, current checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4
[0m19:51:29.631702 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:51:29.631986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f7d8d01b-9741-434f-ac3d-79df4621e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c92bd0850>]}
[0m19:51:30.697837 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m19:51:30.774127 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.pipeline.monthly_player_openings' (models/aggregate/monthly_player_openings.sql) depends on a node named 'opening_map' which was not found
[0m19:51:30.774874 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 2.213196, "process_in_blocks": "0", "process_kernel_time": 0.306719, "process_mem_max_rss": "238684", "process_out_blocks": "16", "process_user_time": 4.152492}
[0m19:51:30.775266 [debug] [MainThread]: Command `dbt seed` failed at 19:51:30.775179 after 2.21 seconds
[0m19:51:30.775604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c91fde210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c91df4410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c59845090>]}
[0m19:51:30.775954 [debug] [MainThread]: Flushing usage events
[0m19:51:31.120266 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:58:53.099685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a8297a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a82ea090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a82eb390>]}


============================== 19:58:53.102290 | 8319c6ef-3999-4aa9-ac4e-77c7b82347f9 ==============================
[0m19:58:53.102290 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:58:53.102761 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt seed', 'send_anonymous_usage_stats': 'True'}
[0m19:58:53.748833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8319c6ef-3999-4aa9-ac4e-77c7b82347f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe276be0890>]}
[0m19:58:53.862260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8319c6ef-3999-4aa9-ac4e-77c7b82347f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2aaccd1d0>]}
[0m19:58:53.862828 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:58:53.983601 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:58:54.064012 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:58:54.064403 [debug] [MainThread]: previous checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, current checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4
[0m19:58:54.064645 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:58:54.064908 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m19:58:54.065180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8319c6ef-3999-4aa9-ac4e-77c7b82347f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a89be110>]}
[0m19:58:55.081625 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m19:58:55.157033 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.pipeline.monthly_player_openings' (models/aggregate/monthly_player_openings.sql) depends on a node named 'opening_map' which was not found
[0m19:58:55.157753 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": false, "command_wall_clock_time": 2.104765, "process_in_blocks": "0", "process_kernel_time": 0.319395, "process_mem_max_rss": "239124", "process_out_blocks": "16", "process_user_time": 4.057015}
[0m19:58:55.158155 [debug] [MainThread]: Command `dbt seed` failed at 19:58:55.158068 after 2.11 seconds
[0m19:58:55.158500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a82f0850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2a8283f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe26feed5d0>]}
[0m19:58:55.158850 [debug] [MainThread]: Flushing usage events
[0m19:58:55.521365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:00:16.205136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f2a9b850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f37a7450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f2a9add0>]}


============================== 20:00:16.208381 | cf790574-c075-4f13-8044-6eab65346b51 ==============================
[0m20:00:16.208381 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:00:16.208881 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt seed', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:00:16.891426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73c13deb10>]}
[0m20:00:17.018734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f54cd910>]}
[0m20:00:17.019348 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m20:00:17.153362 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m20:00:17.245012 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m20:00:17.245432 [debug] [MainThread]: previous checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, current checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4
[0m20:00:17.245698 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:00:17.245977 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m20:00:17.246268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f3893f90>]}
[0m20:00:18.311438 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m20:00:18.470215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73bada9310>]}
[0m20:00:18.542615 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:00:18.544990 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:00:18.554644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73ba460050>]}
[0m20:00:18.555205 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m20:00:18.555521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73baabcad0>]}
[0m20:00:18.557043 [info ] [MainThread]: 
[0m20:00:18.557338 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:00:18.557597 [info ] [MainThread]: 
[0m20:00:18.558015 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:00:18.558966 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m20:00:18.559580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:19.488859 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m20:00:19.489790 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m20:00:19.490320 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:00:19.490900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:19.491657 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m20:00:19.495115 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:19.687517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73ba6dd210>]}
[0m20:00:19.688395 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:19.692006 [debug] [Thread-1 (]: Began running node seed.pipeline.opening_mapping
[0m20:00:19.692717 [info ] [Thread-1 (]: 1 of 1 START seed file dev_universal.opening_mapping ........................... [RUN]
[0m20:00:19.693261 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now seed.pipeline.opening_mapping)
[0m20:00:19.693678 [debug] [Thread-1 (]: Began compiling node seed.pipeline.opening_mapping
[0m20:00:19.694098 [debug] [Thread-1 (]: Began executing node seed.pipeline.opening_mapping
[0m20:00:19.746236 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:00:22.204927 [debug] [Thread-1 (]: On seed.pipeline.opening_mapping: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "seed.pipeline.opening_mapping"} */

    alter table `checkmate-453316`.`dev_universal`.`opening_mapping` set OPTIONS()
  
[0m20:00:22.660289 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ae303a48-42cf-4fdb-a62f-10ddbf9b3aed&page=queryresults
[0m20:00:22.973960 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.pipeline.opening_mapping"
[0m20:00:23.002741 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cf790574-c075-4f13-8044-6eab65346b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73aa765610>]}
[0m20:00:23.003705 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file dev_universal.opening_mapping ....................... [[32mINSERT 2263[0m in 3.31s]
[0m20:00:23.004732 [debug] [Thread-1 (]: Finished running node seed.pipeline.opening_mapping
[0m20:00:23.006877 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:00:23.007891 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:00:23.008402 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m20:00:23.008873 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m20:00:23.009329 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m20:00:23.009837 [info ] [MainThread]: 
[0m20:00:23.010365 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 4.45 seconds (4.45s).
[0m20:00:23.011395 [debug] [MainThread]: Command end result
[0m20:00:23.046140 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:00:23.048155 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:00:23.056434 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m20:00:23.056796 [info ] [MainThread]: 
[0m20:00:23.057271 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:00:23.057626 [info ] [MainThread]: 
[0m20:00:23.058031 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:00:23.058715 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 6.901397, "process_in_blocks": "0", "process_kernel_time": 0.34355, "process_mem_max_rss": "252760", "process_out_blocks": "3192", "process_user_time": 4.601998}
[0m20:00:23.059107 [debug] [MainThread]: Command `dbt seed` succeeded at 20:00:23.059023 after 6.90 seconds
[0m20:00:23.059424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f2a906d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f2aef390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f73f2cddd10>]}
[0m20:00:23.059741 [debug] [MainThread]: Flushing usage events
[0m20:00:23.433978 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:00:40.933157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42dff81650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42e01c9c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42e1a91290>]}


============================== 20:00:40.936089 | 75cdf785-a758-4e91-9f21-9d8d70a32a7c ==============================
[0m20:00:40.936089 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:00:40.936523 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt seed --target prod', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:00:41.619547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42ad7e14d0>]}
[0m20:00:41.750546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42e2975690>]}
[0m20:00:41.751253 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m20:00:41.873936 [debug] [MainThread]: checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4, vars: {}, profile: , target: prod, version: 1.9.4
[0m20:00:41.957806 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m20:00:41.958294 [debug] [MainThread]: previous checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4, current checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c
[0m20:00:41.958558 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:00:41.958966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42e0db8250>]}
[0m20:00:42.978480 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m20:00:43.141859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42a7cdc510>]}
[0m20:00:43.216963 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:00:43.219414 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:00:43.229508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42a787dbd0>]}
[0m20:00:43.229982 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m20:00:43.230283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42a798e2d0>]}
[0m20:00:43.231709 [info ] [MainThread]: 
[0m20:00:43.232015 [info ] [MainThread]: Concurrency: 4 threads (target='prod')
[0m20:00:43.232278 [info ] [MainThread]: 
[0m20:00:43.232687 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:00:43.233755 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m20:00:43.234547 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:44.114988 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_prod_aggregate)
[0m20:00:44.115675 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_prod_universal'
[0m20:00:44.116153 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:00:44.116691 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_prod_staging'
[0m20:00:44.117120 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:44.119147 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:00:44.282071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42dffbaad0>]}
[0m20:00:44.282774 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:00:44.286276 [debug] [Thread-1 (]: Began running node seed.pipeline.opening_mapping
[0m20:00:44.287191 [info ] [Thread-1 (]: 1 of 1 START seed file prod_universal.opening_mapping .......................... [RUN]
[0m20:00:44.287995 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_prod_aggregate, now seed.pipeline.opening_mapping)
[0m20:00:44.288663 [debug] [Thread-1 (]: Began compiling node seed.pipeline.opening_mapping
[0m20:00:44.289327 [debug] [Thread-1 (]: Began executing node seed.pipeline.opening_mapping
[0m20:00:44.349279 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:00:46.681599 [debug] [Thread-1 (]: On seed.pipeline.opening_mapping: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "prod", "node_id": "seed.pipeline.opening_mapping"} */

    alter table `checkmate-453316`.`prod_universal`.`opening_mapping` set OPTIONS()
  
[0m20:00:47.024516 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:61020fd7-958f-482a-8425-9eb29c96eb87&page=queryresults
[0m20:00:47.322905 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.pipeline.opening_mapping"
[0m20:00:47.354776 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75cdf785-a758-4e91-9f21-9d8d70a32a7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42a798d510>]}
[0m20:00:47.355988 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file prod_universal.opening_mapping ...................... [[32mINSERT 2263[0m in 3.07s]
[0m20:00:47.357146 [debug] [Thread-1 (]: Finished running node seed.pipeline.opening_mapping
[0m20:00:47.359284 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m20:00:47.360314 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:00:47.360808 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m20:00:47.361297 [debug] [MainThread]: Connection 'list_checkmate-453316_prod_universal' was properly closed.
[0m20:00:47.361760 [debug] [MainThread]: Connection 'list_checkmate-453316_prod_staging' was properly closed.
[0m20:00:47.362295 [info ] [MainThread]: 
[0m20:00:47.362849 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 4.13 seconds (4.13s).
[0m20:00:47.363904 [debug] [MainThread]: Command end result
[0m20:00:47.400540 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:00:47.402794 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:00:47.411355 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m20:00:47.411737 [info ] [MainThread]: 
[0m20:00:47.412163 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:00:47.412426 [info ] [MainThread]: 
[0m20:00:47.412709 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:00:47.413275 [debug] [MainThread]: Resource report: {"command_name": "seed", "command_success": true, "command_wall_clock_time": 6.5284724, "process_in_blocks": "0", "process_kernel_time": 0.361144, "process_mem_max_rss": "252836", "process_out_blocks": "3184", "process_user_time": 4.493057}
[0m20:00:47.413643 [debug] [MainThread]: Command `dbt seed` succeeded at 20:00:47.413562 after 6.53 seconds
[0m20:00:47.413972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42e01ca290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42dff6fe10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f42e3bfbe50>]}
[0m20:00:47.414285 [debug] [MainThread]: Flushing usage events
[0m20:00:47.760741 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:02:02.346809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94e819b850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94e819b610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94e8199f90>]}


============================== 20:02:02.349758 | 37c03bbe-70fd-45ce-a99d-5e5741a95aa6 ==============================
[0m20:02:02.349758 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:02:02.350202 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:02:03.048581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94b454f1d0>]}
[0m20:02:03.179048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94eabcd990>]}
[0m20:02:03.179656 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m20:02:03.304628 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m20:02:03.397196 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m20:02:03.397595 [debug] [MainThread]: previous checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, current checksum: 6d8301ed5f8f0471e249cbcc8805b42bc5f78a9b258cb4e5c4b012c6ace116b4
[0m20:02:03.397848 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:02:03.398134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94e8f9bf50>]}
[0m20:02:04.399296 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m20:02:04.557525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94afe30750>]}
[0m20:02:04.631215 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:02:04.633537 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:02:04.642755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94afbc8ad0>]}
[0m20:02:04.643227 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m20:02:04.643533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94afcb7b90>]}
[0m20:02:04.645150 [info ] [MainThread]: 
[0m20:02:04.645448 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:02:04.645710 [info ] [MainThread]: 
[0m20:02:04.646149 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:02:04.650439 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m20:02:04.651424 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m20:02:04.652243 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m20:02:04.652704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:02:04.653310 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:02:04.653785 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:02:05.524589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37c03bbe-70fd-45ce-a99d-5e5741a95aa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94afeb6550>]}
[0m20:02:05.525314 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:02:05.528853 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m20:02:05.529543 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.calendar)
[0m20:02:05.530194 [debug] [Thread-2 (]: Began running node seed.pipeline.opening_mapping
[0m20:02:05.530732 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m20:02:05.531324 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now seed.pipeline.opening_mapping)
[0m20:02:05.543789 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m20:02:05.544400 [debug] [Thread-2 (]: Began compiling node seed.pipeline.opening_mapping
[0m20:02:05.547096 [debug] [Thread-2 (]: Began executing node seed.pipeline.opening_mapping
[0m20:02:05.547521 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:02:05.548238 [debug] [Thread-2 (]: Finished running node seed.pipeline.opening_mapping
[0m20:02:05.548628 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m20:02:05.549341 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:02:05.550557 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m20:02:05.551457 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m20:02:05.552066 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m20:02:05.552632 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m20:02:05.557967 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m20:02:05.558573 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m20:02:05.558921 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:02:05.559653 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m20:02:05.560288 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m20:02:05.560626 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.stg__monthly_player_games)
[0m20:02:05.561033 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m20:02:05.561607 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m20:02:05.562094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m20:02:05.565848 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m20:02:05.566333 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m20:02:05.572548 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m20:02:05.573207 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m20:02:05.573919 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:02:05.574911 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m20:02:05.575459 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m20:02:05.576212 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:02:05.576691 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m20:02:05.577861 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m20:02:05.578435 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m20:02:05.579144 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m20:02:05.579737 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_openings
[0m20:02:05.584944 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m20:02:05.585540 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__monthly_player_games, now model.pipeline.weekly_openings)
[0m20:02:05.586272 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_openings
[0m20:02:05.592409 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m20:02:05.593496 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m20:02:05.594269 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m20:02:05.595862 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m20:02:05.596587 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_openings
[0m20:02:05.597170 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:02:05.598330 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_openings
[0m20:02:05.599724 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:02:05.600255 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m20:02:05.600686 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m20:02:05.601123 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m20:02:05.601541 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m20:02:05.603805 [debug] [MainThread]: Command end result
[0m20:02:05.645393 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:02:05.647619 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:02:05.654157 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m20:02:05.654848 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.355691, "process_in_blocks": "0", "process_kernel_time": 0.355696, "process_mem_max_rss": "243904", "process_out_blocks": "3288", "process_user_time": 4.454957}
[0m20:02:05.655247 [debug] [MainThread]: Command `dbt compile` succeeded at 20:02:05.655162 after 3.36 seconds
[0m20:02:05.655588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94e81d5690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94e83ddcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f94ebb77dd0>]}
[0m20:02:05.655998 [debug] [MainThread]: Flushing usage events
[0m20:02:06.006794 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:59:21.954582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016d832d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016d82fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016d82290>]}


============================== 19:59:21.957575 | 578831fc-0202-4eaf-a122-d85ec6f85c27 ==============================
[0m19:59:21.957575 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:59:21.958068 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt docs serve', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:59:24.694309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '578831fc-0202-4eaf-a122-d85ec6f85c27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016dd7890>]}
[0m19:59:24.831869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '578831fc-0202-4eaf-a122-d85ec6f85c27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fafdf67b310>]}
[0m20:01:16.943085 [error] [MainThread]: Encountered an error:

[0m20:01:16.953141 [error] [MainThread]: Traceback (most recent call last):
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/cli/requires.py", line 235, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/cli/requires.py", line 311, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/cli/main.py", line 301, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "/home/filpill/projects/chess_analysis/dbt/.venv/lib/python3.11/site-packages/dbt/task/docs/serve.py", line 29, in run
    httpd.serve_forever()
  File "/home/filpill/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/socketserver.py", line 233, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/filpill/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m20:01:16.954200 [debug] [MainThread]: Resource report: {"command_name": "serve", "command_success": false, "command_wall_clock_time": 115.049805, "process_in_blocks": "171472", "process_kernel_time": 0.360269, "process_mem_max_rss": "215300", "process_out_blocks": "3384", "process_user_time": 3.100128}
[0m20:01:16.954928 [debug] [MainThread]: Command `dbt docs serve` failed at 20:01:16.954822 after 115.05 seconds
[0m20:01:16.955262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016d812d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016dd62d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb016d7bed0>]}
[0m20:01:16.955606 [debug] [MainThread]: Flushing usage events
[0m20:01:17.332786 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:01:22.710961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddfdea0990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddfdeab0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddfdeab650>]}


============================== 20:01:22.713907 | bd00274f-db78-49fd-941a-f941a0377cea ==============================
[0m20:01:22.713907 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:01:22.714357 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt docs generate', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:01:23.394855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd00274f-db78-49fd-941a-f941a0377cea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddc5f58150>]}
[0m20:01:23.507203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd00274f-db78-49fd-941a-f941a0377cea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddc5f0bfd0>]}
[0m20:01:23.507771 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m20:01:23.661572 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m20:01:23.910155 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:01:23.910599 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:01:23.961486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd00274f-db78-49fd-941a-f941a0377cea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddc5cbc450>]}
[0m20:01:23.986108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd00274f-db78-49fd-941a-f941a0377cea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddc5d82d10>]}
[0m20:01:23.986693 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m20:01:23.986992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd00274f-db78-49fd-941a-f941a0377cea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddc5d82e10>]}
[0m20:01:23.988609 [info ] [MainThread]: 
[0m20:01:23.988914 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:01:23.989172 [info ] [MainThread]: 
[0m20:01:23.989621 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:01:23.994018 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m20:01:23.994942 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m20:01:23.995434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:23.996220 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m20:01:23.996662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:23.997258 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:25.010495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd00274f-db78-49fd-941a-f941a0377cea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fddc60a1410>]}
[0m20:01:25.011229 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:01:25.019073 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m20:01:25.019737 [debug] [Thread-2 (]: Began running node seed.pipeline.opening_mapping
[0m20:01:25.020532 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m20:01:25.021208 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m20:01:25.021787 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m20:01:25.022312 [debug] [Thread-2 (]: Began compiling node seed.pipeline.opening_mapping
[0m20:01:25.036363 [debug] [Thread-2 (]: Began executing node seed.pipeline.opening_mapping
[0m20:01:25.042150 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:01:25.041700 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m20:01:25.043339 [debug] [Thread-2 (]: Finished running node seed.pipeline.opening_mapping
[0m20:01:25.051017 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m20:01:25.051616 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:01:25.052865 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m20:01:25.054103 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m20:01:25.054968 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m20:01:25.055535 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m20:01:25.064422 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m20:01:25.065642 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m20:01:25.065944 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:01:25.066632 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m20:01:25.067274 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m20:01:25.067598 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m20:01:25.067919 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.stg__monthly_player_games)
[0m20:01:25.068271 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m20:01:25.068595 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m20:01:25.068907 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m20:01:25.073041 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m20:01:25.076391 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m20:01:25.076986 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m20:01:25.077324 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m20:01:25.077711 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:01:25.078072 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:01:25.078864 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m20:01:25.079679 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m20:01:25.080501 [debug] [Thread-4 (]: Began running node model.pipeline.monthly_player_openings
[0m20:01:25.080930 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m20:01:25.081542 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.monthly_player_openings)
[0m20:01:25.081978 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.weekly_openings)
[0m20:01:25.082449 [debug] [Thread-4 (]: Began compiling node model.pipeline.monthly_player_openings
[0m20:01:25.082742 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_openings
[0m20:01:25.087485 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.monthly_player_openings"
[0m20:01:25.091521 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m20:01:25.092399 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_openings
[0m20:01:25.092763 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m20:01:25.093212 [debug] [Thread-4 (]: Began executing node model.pipeline.monthly_player_openings
[0m20:01:25.094163 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m20:01:25.094640 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m20:01:25.096069 [debug] [Thread-4 (]: Finished running node model.pipeline.monthly_player_openings
[0m20:01:25.097460 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:01:25.098013 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m20:01:25.098478 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m20:01:25.098917 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m20:01:25.099348 [debug] [MainThread]: Connection 'model.pipeline.monthly_player_openings' was properly closed.
[0m20:01:25.101645 [debug] [MainThread]: Command end result
[0m20:01:25.195991 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:01:25.199863 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:01:25.205867 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m20:01:25.208262 [debug] [MainThread]: Acquiring new bigquery connection 'generate_catalog'
[0m20:01:25.208566 [info ] [MainThread]: Building catalog
[0m20:01:25.213333 [debug] [ThreadPool]: Acquiring new bigquery connection 'checkmate-453316.information_schema'
[0m20:01:25.214149 [debug] [ThreadPool]: Acquiring new bigquery connection 'checkmate-453316.information_schema'
[0m20:01:25.214898 [debug] [ThreadPool]: Acquiring new bigquery connection 'checkmate-453316.information_schema'
[0m20:01:25.215660 [debug] [ThreadPool]: Acquiring new bigquery connection 'checkmate-453316.information_schema'
[0m20:01:25.261291 [debug] [ThreadPool]: On checkmate-453316.information_schema: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "checkmate-453316.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `checkmate-453316`.`dev_universal`.__TABLES__ tables
    left join `checkmate-453316`.`dev_universal`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `checkmate-453316`.`dev_universal`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('dev_universal')
                            and upper(table_name) = upper('calendar')
                            ) or (
                                upper(table_schema) = upper('dev_universal')
                            and upper(table_name) = upper('opening_mapping')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `checkmate-453316`.`dev_universal`.INFORMATION_SCHEMA.COLUMNS columns
    join `checkmate-453316`.`dev_universal`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m20:01:25.273566 [debug] [ThreadPool]: On checkmate-453316.information_schema: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "checkmate-453316.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `checkmate-453316`.`dev_aggregate`.__TABLES__ tables
    left join `checkmate-453316`.`dev_aggregate`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `checkmate-453316`.`dev_aggregate`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('dev_aggregate')
                            and upper(table_name) = upper('weekly_openings')
                            ) or (
                                upper(table_schema) = upper('dev_aggregate')
                            and upper(table_name) = upper('monthly_player_openings')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `checkmate-453316`.`dev_aggregate`.INFORMATION_SCHEMA.COLUMNS columns
    join `checkmate-453316`.`dev_aggregate`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m20:01:25.266437 [debug] [ThreadPool]: On checkmate-453316.information_schema: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "checkmate-453316.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `checkmate-453316`.`dev_staging`.__TABLES__ tables
    left join `checkmate-453316`.`dev_staging`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `checkmate-453316`.`dev_staging`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('dev_staging')
                            and upper(table_name) = upper('stg__weekly_games')
                            ) or (
                                upper(table_schema) = upper('dev_staging')
                            and upper(table_name) = upper('stg__monthly_player_games')
                            ) or (
                                upper(table_schema) = upper('dev_staging')
                            and upper(table_name) = upper('stg__player_games')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `checkmate-453316`.`dev_staging`.INFORMATION_SCHEMA.COLUMNS columns
    join `checkmate-453316`.`dev_staging`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m20:01:25.278873 [debug] [ThreadPool]: On checkmate-453316.information_schema: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "checkmate-453316.information_schema"} */

    with
                table_shards_stage as (
    select
        tables.project_id as table_catalog,
        tables.dataset_id as table_schema,
        coalesce(REGEXP_EXTRACT(tables.table_id, '^(.+)[0-9]{8}$'), tables.table_id) as table_name,
        tables.table_id as shard_name,
        REGEXP_EXTRACT(tables.table_id, '^.+([0-9]{8})$') as shard_index,
        REGEXP_CONTAINS(tables.table_id, '^.+[0-9]{8}$') and tables.type = 1 as is_date_shard,
        case
            when materialized_views.table_name is not null then 'materialized view'
            when tables.type = 1 then 'table'
            when tables.type = 2 then 'view'
            else 'external'
        end as table_type,
        tables.type = 1 as is_table,
        JSON_VALUE(table_description.option_value) as table_comment,
        tables.size_bytes,
        tables.row_count
    from `checkmate-453316`.`chess_raw`.__TABLES__ tables
    left join `checkmate-453316`.`chess_raw`.INFORMATION_SCHEMA.MATERIALIZED_VIEWS materialized_views
        on materialized_views.table_catalog = tables.project_id
        and materialized_views.table_schema = tables.dataset_id
        and materialized_views.table_name = tables.table_id
    left join `checkmate-453316`.`chess_raw`.INFORMATION_SCHEMA.TABLE_OPTIONS table_description
        on table_description.table_catalog = tables.project_id
        and table_description.table_schema = tables.dataset_id
        and table_description.table_name = tables.table_id
        and table_description.option_name = 'description'
),
                table_shards as (
                    select * from table_shards_stage
                    where ((
                                upper(table_schema) = upper('chess_raw')
                            and upper(table_name) = upper('games')
                            ))
                ),
                tables as (
    select distinct
        table_catalog,
        table_schema,
        table_name,
        is_date_shard,
        table_type,
        is_table,
        table_comment
    from table_shards
),
                table_stats as (
    select
        table_catalog,
        table_schema,
        table_name,
        max(shard_name) as latest_shard_name,
        min(shard_index) as shard_min,
        max(shard_index) as shard_max,
        count(shard_index) as shard_count,
        sum(size_bytes) as size_bytes,
        sum(row_count) as row_count
    from table_shards
    group by 1, 2, 3
),

                columns as (
    select
        columns.table_catalog,
        columns.table_schema,
        columns.table_name as shard_name,
        coalesce(paths.field_path, '<unknown>') as column_name,
        -- invent a row number to account for nested fields
        -- BQ does not treat these nested properties as independent fields
        row_number() over (
            partition by
                columns.table_catalog,
                columns.table_schema,
                columns.table_name
            order by
                columns.ordinal_position,
                paths.field_path
        ) as column_index,
        coalesce(paths.data_type, '<unknown>') as column_type,
        paths.description as column_comment,
        case when columns.is_partitioning_column = 'YES' then 1 else 0 end as is_partitioning_column,
        case when columns.is_partitioning_column = 'YES' then paths.field_path end as partition_column,
        case when columns.clustering_ordinal_position is not null then 1 else 0 end as is_clustering_column,
        case when columns.clustering_ordinal_position is not null then paths.field_path end as cluster_column,
        columns.clustering_ordinal_position
    from `checkmate-453316`.`chess_raw`.INFORMATION_SCHEMA.COLUMNS columns
    join `checkmate-453316`.`chess_raw`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS paths
        on paths.table_catalog = columns.table_catalog
        and paths.table_schema = columns.table_schema
        and paths.table_name = columns.table_name
        and paths.column_name = columns.column_name
    where columns.ordinal_position is not null
),
                column_stats as (
    select
        table_catalog,
        table_schema,
        shard_name,
        max(is_partitioning_column) = 1 as is_partitioned,
        max(partition_column) as partition_column,
        max(is_clustering_column) = 1 as is_clustered,
        array_to_string(
            array_agg(
                cluster_column ignore nulls
                order by clustering_ordinal_position
            ), ', '
        ) as clustering_columns
    from columns
    group by 1, 2, 3
)

            
    select
        tables.table_catalog as table_database,
        tables.table_schema,
        case
            when tables.is_date_shard then concat(tables.table_name, '*')
            else tables.table_name
        end as table_name,
        tables.table_type,
        tables.table_comment,
        -- coalesce column metadata fields to ensure they are non-null for catalog generation
        -- external table columns are not present in COLUMN_FIELD_PATHS
        coalesce(columns.column_name, '<unknown>') as column_name,
        coalesce(columns.column_index, 1) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        coalesce(columns.column_comment, '') as column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_stats.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        tables.is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_stats.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        tables.is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_stats.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        tables.is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        table_stats.row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        tables.is_table as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        table_stats.size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        tables.is_table as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        column_stats.partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        column_stats.is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        column_stats.clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        column_stats.is_clustered as `stats__clustering_fields__include`

    from tables
    join table_stats
        on table_stats.table_catalog = tables.table_catalog
        and table_stats.table_schema = tables.table_schema
        and table_stats.table_name = tables.table_name
    left join column_stats
        on column_stats.table_catalog = tables.table_catalog
        and column_stats.table_schema = tables.table_schema
        and column_stats.shard_name = table_stats.latest_shard_name
    left join columns
        on columns.table_catalog = tables.table_catalog
        and columns.table_schema = tables.table_schema
        and columns.shard_name = table_stats.latest_shard_name

  
[0m20:01:25.280327 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:25.280891 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:25.281484 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:25.282034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:25.776989 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:5697fbd6-2a4d-413f-a9e7-dc6cdcf14379&page=queryresults
[0m20:01:25.915426 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:550a0a47-0796-404f-baae-9903986beb36&page=queryresults
[0m20:01:25.919594 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c028593b-1749-47e0-be71-cfbd4b908575&page=queryresults
[0m20:01:25.934177 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3086067c-743a-4369-9c93-e02a89e55492&page=queryresults
[0m20:01:27.635883 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:01:27.651577 [debug] [MainThread]: Wrote artifact CatalogArtifact to /home/filpill/projects/chess_analysis/dbt/pipeline/target/catalog.json
[0m20:01:27.674165 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:01:27.675577 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:01:27.675869 [info ] [MainThread]: Catalog written to /home/filpill/projects/chess_analysis/dbt/pipeline/target/catalog.json
[0m20:01:27.676614 [debug] [MainThread]: Resource report: {"command_name": "generate", "command_success": true, "command_wall_clock_time": 5.0133204, "process_in_blocks": "6136", "process_kernel_time": 0.378779, "process_mem_max_rss": "229152", "process_out_blocks": "5768", "process_user_time": 3.5522}
[0m20:01:27.677037 [debug] [MainThread]: Command `dbt docs generate` succeeded at 20:01:27.676952 after 5.01 seconds
[0m20:01:27.677316 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m20:01:27.677592 [debug] [MainThread]: Connection 'checkmate-453316.information_schema' was properly closed.
[0m20:01:27.677844 [debug] [MainThread]: Connection 'checkmate-453316.information_schema' was properly closed.
[0m20:01:27.678094 [debug] [MainThread]: Connection 'checkmate-453316.information_schema' was properly closed.
[0m20:01:27.678340 [debug] [MainThread]: Connection 'checkmate-453316.information_schema' was properly closed.
[0m20:01:27.678652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde01eba090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde01884150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fde01918110>]}
[0m20:01:27.679000 [debug] [MainThread]: Flushing usage events
[0m20:01:28.024649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:01:38.852900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22fb8eb490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22fb8ea850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22fb88c1d0>]}


============================== 20:01:38.856335 | 91116e57-195a-4d00-a6f4-db724ac9a30b ==============================
[0m20:01:38.856335 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:01:38.856814 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt docs serve', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:01:39.527509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91116e57-195a-4d00-a6f4-db724ac9a30b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22f99fa650>]}
[0m20:01:39.656646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '91116e57-195a-4d00-a6f4-db724ac9a30b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22c3bb3d50>]}
[0m21:07:46.471511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98c5097190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98c5096a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98c5097fd0>]}


============================== 21:07:46.474111 | bc55a1f3-ce58-4356-8aa7-2215cfdaecaa ==============================
[0m21:07:46.474111 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:07:46.474598 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt docs serve', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:07:47.151118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc55a1f3-ce58-4356-8aa7-2215cfdaecaa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98c511e990>]}
[0m21:07:47.281298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc55a1f3-ce58-4356-8aa7-2215cfdaecaa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98c7add750>]}
[0m14:14:18.701646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8f7b83b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8f7b83890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8f7b80390>]}


============================== 14:14:18.713205 | 1629ef33-8b43-40ba-9f70-eaaca624c3ab ==============================
[0m14:14:18.713205 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:14:18.713652 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:14:21.518149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1629ef33-8b43-40ba-9f70-eaaca624c3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8bfc16590>]}
[0m14:14:21.640234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1629ef33-8b43-40ba-9f70-eaaca624c3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8c5625710>]}
[0m14:14:21.640845 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:14:21.796372 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:14:21.886547 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:14:21.887016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1629ef33-8b43-40ba-9f70-eaaca624c3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8f89bc050>]}
[0m14:14:22.051934 [error] [MainThread]: Encountered an error:
Compilation Error
  non-default argument follows default argument
    line 1
      {% macro last_n_days_filter(column='date_column', days) %}
[0m14:14:22.053096 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.3986397, "process_in_blocks": "272728", "process_kernel_time": 0.4832, "process_mem_max_rss": "217888", "process_out_blocks": "16", "process_user_time": 3.190943}
[0m14:14:22.054236 [debug] [MainThread]: Command `dbt run` failed at 14:14:22.054058 after 3.40 seconds
[0m14:14:22.054870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8f7b81090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8f7b780d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc8c63a1650>]}
[0m14:14:22.055502 [debug] [MainThread]: Flushing usage events
[0m14:14:22.430242 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:15:32.442372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccb88310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccb78cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccb815d0>]}


============================== 14:15:32.445397 | 0378f640-62b2-461b-b095-0919a31b4a90 ==============================
[0m14:15:32.445397 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:15:32.447213 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:15:33.107077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad9a2a4dd0>]}
[0m14:15:33.225968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcf5756d0>]}
[0m14:15:33.226520 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:15:33.348224 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:15:33.436533 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m14:15:33.436963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcd9bc0d0>]}
[0m14:15:34.478119 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m14:15:34.629952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad9872c5d0>]}
[0m14:15:34.697598 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:15:34.700874 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:15:34.730922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad98569490>]}
[0m14:15:34.731348 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m14:15:34.731641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad98841450>]}
[0m14:15:34.733163 [info ] [MainThread]: 
[0m14:15:34.733490 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:15:34.733727 [info ] [MainThread]: 
[0m14:15:34.734107 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:15:34.737903 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:15:34.738723 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:15:34.739399 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:15:34.739791 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:34.740293 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:34.740678 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:37.112940 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:15:37.113735 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:15:37.114495 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m14:15:37.114995 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:15:37.115684 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:15:37.116147 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:15:37.315162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad98b33990>]}
[0m14:15:37.315880 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:15:37.325624 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:15:37.326118 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:15:37.326527 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m14:15:37.326831 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:15:37.335439 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:15:37.341985 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:15:37.354747 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:37.605924 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:15:37.610310 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:15:38.048198 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f6e871e0-6c91-4464-b43f-f6eea0b041ac&page=queryresults
[0m14:15:40.274985 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad988a4490>]}
[0m14:15:40.275971 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.95s]
[0m14:15:40.276577 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:15:40.277381 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m14:15:40.277869 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:15:40.278283 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m14:15:40.278612 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m14:15:40.285701 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:15:40.286953 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m14:15:40.331276 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:40.561202 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:15:40.564761 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.cal_month_start                                    AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:15:40.717313 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e5704653-c991-4491-9afb-19c3a2f9059a&page=queryresults
[0m14:15:41.438752 [error] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e5704653-c991-4491-9afb-19c3a2f9059a&page=queryresults
[0m14:15:41.482541 [debug] [Thread-3 (]: Database Error in model stg__player_games (models/staging/stg__player_games.sql)
  Query error: Name cal_month_start not found inside cal at [71:13]
  compiled code at target/run/pipeline/models/staging/stg__player_games.sql
[0m14:15:41.483392 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0378f640-62b2-461b-b095-0919a31b4a90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fad98309a10>]}
[0m14:15:41.484346 [error] [Thread-3 (]: 2 of 5 ERROR creating sql incremental model dev_staging.stg__player_games ...... [[31mERROR[0m in 1.20s]
[0m14:15:41.485273 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m14:15:41.486162 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__player_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__player_games (models/staging/stg__player_games.sql)
  Query error: Name cal_month_start not found inside cal at [71:13]
  compiled code at target/run/pipeline/models/staging/stg__player_games.sql.
[0m14:15:41.488031 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m14:15:41.488638 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m14:15:41.489221 [info ] [Thread-2 (]: 3 of 5 SKIP relation dev_staging.stg__monthly_player_games ..................... [[33mSKIP[0m]
[0m14:15:41.489944 [info ] [Thread-1 (]: 4 of 5 SKIP relation dev_staging.stg__weekly_games ............................. [[33mSKIP[0m]
[0m14:15:41.490660 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m14:15:41.491287 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m14:15:41.492377 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m14:15:41.493032 [info ] [Thread-3 (]: 5 of 5 SKIP relation dev_aggregate.weekly_openings ............................. [[33mSKIP[0m]
[0m14:15:41.493725 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m14:15:41.495265 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:15:41.496118 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:15:41.496589 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:15:41.497020 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m14:15:41.497442 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:15:41.498000 [info ] [MainThread]: 
[0m14:15:41.498534 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 6.76 seconds (6.76s).
[0m14:15:41.499952 [debug] [MainThread]: Command end result
[0m14:15:41.536828 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:15:41.538857 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:15:41.546816 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:15:41.547194 [info ] [MainThread]: 
[0m14:15:41.547571 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:15:41.547833 [info ] [MainThread]: 
[0m14:15:41.548146 [error] [MainThread]:   Database Error in model stg__player_games (models/staging/stg__player_games.sql)
  Query error: Name cal_month_start not found inside cal at [71:13]
  compiled code at target/run/pipeline/models/staging/stg__player_games.sql
[0m14:15:41.548405 [info ] [MainThread]: 
[0m14:15:41.548678 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=3 TOTAL=5
[0m14:15:41.549246 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 9.15378, "process_in_blocks": "4488", "process_kernel_time": 0.332498, "process_mem_max_rss": "238704", "process_out_blocks": "3240", "process_user_time": 4.632748}
[0m14:15:41.549632 [debug] [MainThread]: Command `dbt run` failed at 14:15:41.549549 after 9.15 seconds
[0m14:15:41.549961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccb78150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccdca5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadd0617cd0>]}
[0m14:15:41.550276 [debug] [MainThread]: Flushing usage events
[0m14:15:41.906846 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:16:54.878382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c28583850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c287c9cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c287c9b50>]}


============================== 14:16:54.881335 | 614038a1-c184-453c-8bea-a4227d02503f ==============================
[0m14:16:54.881335 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:16:54.881785 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:16:55.553889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bf4f8de50>]}
[0m14:16:55.676747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c2af75750>]}
[0m14:16:55.677363 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:16:55.808546 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:16:55.916929 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:16:55.917490 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m14:16:56.165767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bf43c1290>]}
[0m14:16:56.239441 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:16:56.241811 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:16:56.251119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bf410bd10>]}
[0m14:16:56.251586 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m14:16:56.251890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bf40a1890>]}
[0m14:16:56.253521 [info ] [MainThread]: 
[0m14:16:56.253826 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:16:56.254087 [info ] [MainThread]: 
[0m14:16:56.254506 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:16:56.258617 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:16:56.259121 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:16:56.259591 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:16:56.259987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:16:56.260438 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:16:56.260785 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:16:57.215490 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:16:57.216329 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m14:16:57.217098 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:16:57.217616 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:16:57.218297 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:16:57.218793 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:16:57.405743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bf430e790>]}
[0m14:16:57.406461 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:16:57.409966 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:16:57.410638 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:16:57.411142 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m14:16:57.411560 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:16:57.419905 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:16:57.420522 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:16:57.434515 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:16:57.642432 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:16:57.643742 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:16:58.059261 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:36d3dd3e-896d-4b66-9b0f-599599ccedb3&page=queryresults
[0m14:17:00.181779 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bf4144cd0>]}
[0m14:17:00.182840 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.77s]
[0m14:17:00.183778 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:17:00.184945 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m14:17:00.185755 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:17:00.186478 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m14:17:00.187094 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m14:17:00.199391 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:17:00.200423 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m14:17:00.251556 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:17:00.436713 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:17:00.438317 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:17:00.687757 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c5af3bce-fbc6-4777-bdd4-6759b35dcdbe&page=queryresults
[0m14:17:10.786163 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c28c93490>]}
[0m14:17:10.787144 [info ] [Thread-3 (]: 2 of 5 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (94.7 MiB processed)[0m in 10.60s]
[0m14:17:10.788057 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m14:17:10.789312 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m14:17:10.789974 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m14:17:10.790797 [info ] [Thread-2 (]: 3 of 5 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m14:17:10.791641 [info ] [Thread-1 (]: 4 of 5 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m14:17:10.792376 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__monthly_player_games)
[0m14:17:10.793023 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m14:17:10.793640 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m14:17:10.794217 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m14:17:10.802331 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m14:17:10.809465 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m14:17:10.810298 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m14:17:10.813385 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:17:10.815292 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m14:17:10.819472 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:17:10.970837 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m14:17:10.972273 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
            week_start_date,
            week_number,
            username,
            piece_color,
            rules,
            time_class,
            CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
            ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
            SUM(win_count)                                            AS total_win_count,
            SUM(loss_count)                                           AS total_loss_count,
            SUM(draw_count)                                           AS total_draw_count,
            SUM(total)                                                AS total_games,
            ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m14:17:10.985627 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m14:17:10.987064 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m14:17:11.109310 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:be98b93c-937b-4636-9cdd-bcca2a99eb6d&page=queryresults
[0m14:17:11.120339 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:73a859ed-908b-46bb-83a7-f407252f3c84&page=queryresults
[0m14:17:17.081419 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:be98b93c-937b-4636-9cdd-bcca2a99eb6d&page=queryresults
[0m14:17:17.090788 [debug] [Thread-1 (]: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Query error: Name week_start_date not found inside DBT_INTERNAL_DEST at [106:37]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m14:17:17.091587 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bec3da890>]}
[0m14:17:17.092526 [error] [Thread-1 (]: 4 of 5 ERROR creating sql incremental model dev_staging.stg__weekly_games ...... [[31mERROR[0m in 6.30s]
[0m14:17:17.093467 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m14:17:17.094298 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__weekly_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Query error: Name week_start_date not found inside DBT_INTERNAL_DEST at [106:37]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql.
[0m14:17:17.096029 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m14:17:17.096636 [info ] [Thread-3 (]: 5 of 5 SKIP relation dev_aggregate.weekly_openings ............................. [[33mSKIP[0m]
[0m14:17:17.097264 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m14:17:20.816324 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '614038a1-c184-453c-8bea-a4227d02503f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1bec57fe50>]}
[0m14:17:20.817290 [info ] [Thread-2 (]: 3 of 5 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (48.0 MiB processed)[0m in 10.02s]
[0m14:17:20.818185 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m14:17:20.819989 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:17:20.820804 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:17:20.821256 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m14:17:20.821705 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m14:17:20.822134 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:17:20.822733 [info ] [MainThread]: 
[0m14:17:20.823219 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 24.57 seconds (24.57s).
[0m14:17:20.825004 [debug] [MainThread]: Command end result
[0m14:17:20.863800 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:17:20.866066 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:17:20.873719 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:17:20.874002 [info ] [MainThread]: 
[0m14:17:20.874311 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:17:20.874589 [info ] [MainThread]: 
[0m14:17:20.874916 [error] [MainThread]:   Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Query error: Name week_start_date not found inside DBT_INTERNAL_DEST at [106:37]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m14:17:20.875184 [info ] [MainThread]: 
[0m14:17:20.875467 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m14:17:20.876030 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 26.040274, "process_in_blocks": "0", "process_kernel_time": 0.372363, "process_mem_max_rss": "234312", "process_out_blocks": "3344", "process_user_time": 3.781761}
[0m14:17:20.876399 [debug] [MainThread]: Command `dbt run` failed at 14:17:20.876318 after 26.04 seconds
[0m14:17:20.876725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c28582590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c285d80d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c2c017f10>]}
[0m14:17:20.877032 [debug] [MainThread]: Flushing usage events
[0m14:17:21.319111 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:40.690184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ac828ba90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ac84ce490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ac84ce110>]}


============================== 14:31:40.692774 | d317ef37-5027-4f40-b642-6f7a0cd77ddd ==============================
[0m14:31:40.692774 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:31:40.693218 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:31:41.353623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a94f6f950>]}
[0m14:31:41.474275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8acacdd550>]}
[0m14:31:41.474833 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:31:41.593602 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:31:41.697596 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:31:41.698148 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__weekly_games.sql
[0m14:31:41.939951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a9421f510>]}
[0m14:31:42.011723 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:31:42.013947 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:31:42.023081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a8fe67d50>]}
[0m14:31:42.023546 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m14:31:42.023840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a8ff3cd50>]}
[0m14:31:42.025451 [info ] [MainThread]: 
[0m14:31:42.025756 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:31:42.026011 [info ] [MainThread]: 
[0m14:31:42.026412 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:31:42.030620 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:31:42.031500 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:31:42.032119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:42.032758 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:31:42.033376 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:42.034082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:43.034291 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:31:43.035157 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m14:31:43.035693 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:31:43.036462 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:31:43.037062 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:31:43.038578 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:31:43.212269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a9421c310>]}
[0m14:31:43.212944 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:31:43.216229 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:31:43.217091 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:31:43.217777 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m14:31:43.218337 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:31:43.231285 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:31:43.232182 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:31:43.246800 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:43.438685 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:31:43.439971 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:31:44.102758 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bacff95e-ba14-4d57-930e-85c15b2691a0&page=queryresults
[0m14:31:46.309813 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a8ff4ae50>]}
[0m14:31:46.310873 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.09s]
[0m14:31:46.311900 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:31:46.313122 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m14:31:46.314091 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:31:46.314785 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m14:31:46.315337 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m14:31:46.326182 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:31:46.327089 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m14:31:46.378262 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:31:46.608583 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:31:46.610003 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:31:46.773394 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:31dd737b-4f63-437f-8573-cde0206a9a98&page=queryresults
[0m14:31:55.966150 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a9443f210>]}
[0m14:31:55.967144 [info ] [Thread-3 (]: 2 of 5 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (94.7 MiB processed)[0m in 9.65s]
[0m14:31:55.968083 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m14:31:55.969212 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m14:31:55.970600 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m14:31:55.970011 [info ] [Thread-2 (]: 3 of 5 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m14:31:55.971619 [info ] [Thread-1 (]: 4 of 5 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m14:31:55.972345 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__monthly_player_games)
[0m14:31:55.973011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m14:31:55.973631 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m14:31:55.974211 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m14:31:55.982499 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m14:31:55.989812 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m14:31:55.990913 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m14:31:55.995236 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:31:55.995903 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m14:31:56.001460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:56.167611 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m14:31:56.170759 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m14:31:56.176660 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m14:31:56.178120 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m14:31:56.301824 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:597a445a-ad27-48e6-8493-778dea1f6b25&page=queryresults
[0m14:31:56.316936 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:35d2f41e-bb9a-48a2-83cb-979e3dff6b45&page=queryresults
[0m14:31:56.317839 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:35d2f41e-bb9a-48a2-83cb-979e3dff6b45&page=queryresults
[0m14:31:56.325412 [debug] [Thread-1 (]: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [56:1]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m14:31:56.326229 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a8c213d50>]}
[0m14:31:56.327380 [error] [Thread-1 (]: 4 of 5 ERROR creating sql incremental model dev_staging.stg__weekly_games ...... [[31mERROR[0m in 0.35s]
[0m14:31:56.328310 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m14:31:56.329245 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__weekly_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [56:1]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql.
[0m14:31:56.331318 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m14:31:56.331969 [info ] [Thread-3 (]: 5 of 5 SKIP relation dev_aggregate.weekly_openings ............................. [[33mSKIP[0m]
[0m14:31:56.332651 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m14:32:05.490760 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd317ef37-5027-4f40-b642-6f7a0cd77ddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a8ff83910>]}
[0m14:32:05.491730 [info ] [Thread-2 (]: 3 of 5 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (48.0 MiB processed)[0m in 9.52s]
[0m14:32:05.492643 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m14:32:05.494532 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:32:05.495493 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:32:05.495945 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m14:32:05.496377 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m14:32:05.496832 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:32:05.497400 [info ] [MainThread]: 
[0m14:32:05.497920 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 23.47 seconds (23.47s).
[0m14:32:05.499736 [debug] [MainThread]: Command end result
[0m14:32:05.537326 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:32:05.539361 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:32:05.547323 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:32:05.547596 [info ] [MainThread]: 
[0m14:32:05.547877 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:32:05.548127 [info ] [MainThread]: 
[0m14:32:05.548431 [error] [MainThread]:   Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [56:1]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m14:32:05.548686 [info ] [MainThread]: 
[0m14:32:05.548939 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m14:32:05.549438 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 24.90649, "process_in_blocks": "0", "process_kernel_time": 0.326642, "process_mem_max_rss": "233868", "process_out_blocks": "3328", "process_user_time": 3.847606}
[0m14:32:05.549778 [debug] [MainThread]: Command `dbt run` failed at 14:32:05.549704 after 24.91 seconds
[0m14:32:05.550065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8acbd17e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8acbd17c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8acbd17cd0>]}
[0m14:32:05.550345 [debug] [MainThread]: Flushing usage events
[0m14:32:06.015576 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:38.415536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f5149eed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f515d2610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f5149cd10>]}


============================== 14:32:38.418184 | a7026b9b-75cc-4945-b483-ca09339c5542 ==============================
[0m14:32:38.418184 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:32:38.418672 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:32:39.081527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f514e1c10>]}
[0m14:32:39.208722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f53dd5b90>]}
[0m14:32:39.209302 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:32:39.331550 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:32:39.429376 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:32:39.429894 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__weekly_games.sql
[0m14:32:39.653903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1d31e690>]}
[0m14:32:39.719183 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:32:39.721300 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:32:39.729888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1d1eb210>]}
[0m14:32:39.730313 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m14:32:39.730600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1d22d610>]}
[0m14:32:39.732191 [info ] [MainThread]: 
[0m14:32:39.732482 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:32:39.732721 [info ] [MainThread]: 
[0m14:32:39.733098 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:32:39.736901 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:32:39.737512 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:32:39.738029 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:32:39.738370 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:39.738968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:39.739314 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:40.665551 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m14:32:40.666206 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:32:40.666769 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:32:40.667597 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:32:40.668245 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:32:40.670434 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:32:40.832952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1d081d90>]}
[0m14:32:40.833681 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:40.836997 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:32:40.837769 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:32:40.838298 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m14:32:40.838684 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:32:40.845660 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:32:40.846162 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:32:40.858424 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:41.048052 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:32:41.049402 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:32:41.451154 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c739f1fc-d0f7-4b95-9dfe-b800b23b7207&page=queryresults
[0m14:32:43.879211 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1d5ef150>]}
[0m14:32:43.880263 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.04s]
[0m14:32:43.881187 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:32:43.882279 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m14:32:43.882778 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:32:43.883155 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m14:32:43.883455 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m14:32:43.889328 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:32:43.889817 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m14:32:43.923620 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:32:44.095555 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:32:44.096435 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:32:44.244525 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:50080eef-084d-4897-aebf-88833fae1661&page=queryresults
[0m14:32:54.504095 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1d22d3d0>]}
[0m14:32:54.505095 [info ] [Thread-3 (]: 2 of 5 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (94.7 MiB processed)[0m in 10.62s]
[0m14:32:54.506024 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m14:32:54.507293 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m14:32:54.508040 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m14:32:54.508805 [info ] [Thread-2 (]: 3 of 5 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m14:32:54.509710 [info ] [Thread-1 (]: 4 of 5 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m14:32:54.510467 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m14:32:54.511168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m14:32:54.511820 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m14:32:54.512412 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m14:32:54.521082 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m14:32:54.528315 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m14:32:54.529468 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m14:32:54.530183 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m14:32:54.534629 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:32:54.538712 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:54.674136 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m14:32:54.675555 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m14:32:54.712300 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m14:32:54.713744 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m14:32:54.822529 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e70c347c-925d-4f28-b632-eabfe0a2d06d&page=queryresults
[0m14:32:54.894990 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:71619ba5-ad83-4e1d-a695-a5a1b52eba22&page=queryresults
[0m14:33:00.423114 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e70c347c-925d-4f28-b632-eabfe0a2d06d&page=queryresults
[0m14:33:00.432502 [debug] [Thread-1 (]: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Query error: Name week_start_date not found inside DBT_INTERNAL_DEST at [102:37]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m14:33:00.433282 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1c5ad890>]}
[0m14:33:00.434222 [error] [Thread-1 (]: 4 of 5 ERROR creating sql incremental model dev_staging.stg__weekly_games ...... [[31mERROR[0m in 5.92s]
[0m14:33:00.435114 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m14:33:00.436095 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__weekly_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Query error: Name week_start_date not found inside DBT_INTERNAL_DEST at [102:37]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql.
[0m14:33:00.437907 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m14:33:00.438547 [info ] [Thread-3 (]: 5 of 5 SKIP relation dev_aggregate.weekly_openings ............................. [[33mSKIP[0m]
[0m14:33:00.439176 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m14:33:07.028434 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7026b9b-75cc-4945-b483-ca09339c5542', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f1c29c490>]}
[0m14:33:07.029406 [info ] [Thread-2 (]: 3 of 5 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (48.0 MiB processed)[0m in 12.52s]
[0m14:33:07.030300 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m14:33:07.032015 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:33:07.032819 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:07.033265 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m14:33:07.033708 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:33:07.034138 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m14:33:07.034696 [info ] [MainThread]: 
[0m14:33:07.035189 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 27.30 seconds (27.30s).
[0m14:33:07.036946 [debug] [MainThread]: Command end result
[0m14:33:07.066531 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:33:07.067665 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:33:07.072216 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:33:07.072483 [info ] [MainThread]: 
[0m14:33:07.072884 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:33:07.073138 [info ] [MainThread]: 
[0m14:33:07.073439 [error] [MainThread]:   Database Error in model stg__weekly_games (models/staging/stg__weekly_games.sql)
  Query error: Name week_start_date not found inside DBT_INTERNAL_DEST at [102:37]
  compiled code at target/run/pipeline/models/staging/stg__weekly_games.sql
[0m14:33:07.073695 [info ] [MainThread]: 
[0m14:33:07.073949 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m14:33:07.074462 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 28.70835, "process_in_blocks": "0", "process_kernel_time": 0.365955, "process_mem_max_rss": "233844", "process_out_blocks": "3320", "process_user_time": 3.738029}
[0m14:33:07.074808 [debug] [MainThread]: Command `dbt run` failed at 14:33:07.074733 after 28.71 seconds
[0m14:33:07.075103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f514ef390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f51497f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f5149d610>]}
[0m14:33:07.075388 [debug] [MainThread]: Flushing usage events
[0m14:33:07.510441 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:14:19.025594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9838bf2610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9838de6590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9838b98610>]}


============================== 15:14:19.028498 | 4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a ==============================
[0m15:14:19.028498 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:14:19.028936 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:14:19.678516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9804e12c50>]}
[0m15:14:19.789655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f983b675cd0>]}
[0m15:14:19.790273 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:14:19.909992 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:14:19.990882 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:14:19.991325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98399a3fd0>]}
[0m15:14:21.044591 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m15:14:21.199110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98048a99d0>]}
[0m15:14:21.269737 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:14:21.271869 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:14:21.280330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9804625090>]}
[0m15:14:21.280764 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m15:14:21.281043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98046e6e90>]}
[0m15:14:21.282536 [info ] [MainThread]: 
[0m15:14:21.282812 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:14:21.283050 [info ] [MainThread]: 
[0m15:14:21.283428 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:14:21.287116 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:14:21.287839 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:14:21.288318 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:21.289034 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:14:21.289540 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:21.290496 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:14:22.495532 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_aggregate)
[0m15:14:22.496260 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_staging)
[0m15:14:22.497169 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_aggregate"
"
[0m15:14:22.497957 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_staging"
"
[0m15:14:22.517327 [debug] [ThreadPool]: On create_checkmate-453316_dev_aggregate: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_aggregate"} */
create schema if not exists `checkmate-453316`.`dev_aggregate`
  
[0m15:14:22.525655 [debug] [ThreadPool]: On create_checkmate-453316_dev_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_staging"} */
create schema if not exists `checkmate-453316`.`dev_staging`
  
[0m15:14:22.526355 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:22.527028 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:22.900128 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:1f23fe86-bfbc-4a53-829c-ae2ee10afe9c&page=queryresults
[0m15:14:22.902813 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:69eb9af2-cf5e-4bc3-93f4-7558b74c388f&page=queryresults
[0m15:14:25.962122 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_staging, now list_checkmate-453316_dev_universal)
[0m15:14:25.962931 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_aggregate, now list_checkmate-453316_dev_aggregate)
[0m15:14:25.963459 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:25.964139 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m15:14:25.964821 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:25.966409 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:14:26.353709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9804a0dd90>]}
[0m15:14:26.354496 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:14:26.357586 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m15:14:26.358320 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m15:14:26.359081 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m15:14:26.359696 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m15:14:26.368130 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m15:14:26.368718 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m15:14:26.381803 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:14:26.564889 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m15:14:26.566091 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m15:14:27.031716 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:1b3f9ae4-7544-451a-a706-446a66f3872c&page=queryresults
[0m15:14:28.893529 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9804180490>]}
[0m15:14:28.894237 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.53s]
[0m15:14:28.894906 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m15:14:28.895984 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m15:14:28.896767 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m15:14:28.897461 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m15:14:28.898029 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m15:14:28.905920 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m15:14:28.906835 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m15:14:28.966610 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m15:14:28.967374 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    partition by game_date
    

    OPTIONS(
      description=""""""
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  


)

SELECT * FROM cte_base
    );
  
[0m15:14:28.967901 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:14:30.015866 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:09d1eae3-31ef-4eac-a554-a6ad221a351f&page=queryresults
[0m15:14:40.381283 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98040c25d0>]}
[0m15:14:40.381856 [info ] [Thread-3 (]: 2 of 5 OK created sql incremental model dev_staging.stg__player_games .......... [[32mCREATE TABLE (3.9m rows, 262.9 MiB processed)[0m in 11.48s]
[0m15:14:40.382322 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m15:14:40.383183 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m15:14:40.383576 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m15:14:40.384035 [info ] [Thread-2 (]: 3 of 5 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m15:14:40.384510 [info ] [Thread-1 (]: 4 of 5 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m15:14:40.384915 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__monthly_player_games)
[0m15:14:40.385294 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m15:14:40.385654 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m15:14:40.386008 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m15:14:40.389677 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m15:14:40.393269 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m15:14:40.393875 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m15:14:40.394260 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m15:14:40.397325 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m15:14:40.402260 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m15:14:40.403424 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      
    partition by week_start_date
    

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

      GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
[0m15:14:40.404349 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:14:40.406787 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
[0m15:14:40.407609 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:14:41.414530 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:b1d001de-fc73-44ef-b7d0-51d2c6911172&page=queryresults
[0m15:14:41.417188 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f6fd937c-0837-41d2-b0fc-f2cc72bb15cd&page=queryresults
[0m15:14:47.659532 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f97fe7a5f50>]}
[0m15:14:47.660506 [info ] [Thread-1 (]: 4 of 5 OK created sql incremental model dev_staging.stg__weekly_games .......... [[32mCREATE TABLE (1.1m rows, 335.7 MiB processed)[0m in 7.27s]
[0m15:14:47.661412 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m15:14:47.662374 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m15:14:47.663108 [info ] [Thread-3 (]: 5 of 5 START sql incremental model dev_aggregate.weekly_openings ............... [RUN]
[0m15:14:47.663612 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m15:14:47.663888 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_openings
[0m15:14:47.668083 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:14:47.688588 [debug] [Thread-3 (]: Compilation Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  'dev_date_filter' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:14:47.689398 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98047cbfd0>]}
[0m15:14:47.690324 [error] [Thread-3 (]: 5 of 5 ERROR creating sql incremental model dev_aggregate.weekly_openings ...... [[31mERROR[0m in 0.03s]
[0m15:14:47.691154 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m15:14:47.691924 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_openings' to be skipped because of status 'error'.  Reason: Compilation Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  'dev_date_filter' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:14:49.159433 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ce76ef4-6bb2-43b6-bf85-dec37e1ec16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f98047e6810>]}
[0m15:14:49.160199 [info ] [Thread-2 (]: 3 of 5 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mCREATE TABLE (842.0k rows, 365.8 MiB processed)[0m in 8.77s]
[0m15:14:49.160877 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m15:14:49.162543 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:14:49.163485 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:14:49.163967 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m15:14:49.164399 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m15:14:49.164843 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m15:14:49.165395 [info ] [MainThread]: 
[0m15:14:49.165906 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 27.88 seconds (27.88s).
[0m15:14:49.167784 [debug] [MainThread]: Command end result
[0m15:14:49.204735 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:14:49.207036 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:14:49.215153 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:14:49.215459 [info ] [MainThread]: 
[0m15:14:49.215773 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:14:49.216044 [info ] [MainThread]: 
[0m15:14:49.216372 [error] [MainThread]:   Compilation Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  'dev_date_filter' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:14:49.216651 [info ] [MainThread]: 
[0m15:14:49.216928 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m15:14:49.217495 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 30.24019, "process_in_blocks": "200", "process_kernel_time": 0.324258, "process_mem_max_rss": "237820", "process_out_blocks": "3304", "process_user_time": 4.833531}
[0m15:14:49.217862 [debug] [MainThread]: Command `dbt run` failed at 15:14:49.217781 after 30.24 seconds
[0m15:14:49.218181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9838b8fed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f983c71bd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f983c71bcd0>]}
[0m15:14:49.218498 [debug] [MainThread]: Flushing usage events
[0m15:14:49.656889 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:18:21.667327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd99b795e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd99b9c9bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd99b7e63d0>]}


============================== 15:18:21.670368 | c88732e3-7fd8-436a-b662-15dd2c7baf6e ==============================
[0m15:18:21.670368 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:18:21.670822 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:18:22.310476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd963e4d090>]}
[0m15:18:22.433639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9639bb450>]}
[0m15:18:22.434246 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:18:22.553501 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:18:22.649017 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:18:22.649534 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m15:18:22.882023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd963617410>]}
[0m15:18:22.954066 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:18:22.956349 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:18:22.965563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd96349ab10>]}
[0m15:18:22.966050 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m15:18:22.966348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd96320d950>]}
[0m15:18:22.967995 [info ] [MainThread]: 
[0m15:18:22.968298 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:18:22.968564 [info ] [MainThread]: 
[0m15:18:22.968969 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:18:22.973232 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:18:22.974118 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:18:22.974942 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:18:22.975560 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:22.976117 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:22.976644 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:18:23.913416 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m15:18:23.914206 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m15:18:23.914930 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m15:18:23.915562 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:23.916165 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:23.916727 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:24.100321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd963440ad0>]}
[0m15:18:24.101026 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:18:24.104384 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m15:18:24.104996 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m15:18:24.105467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m15:18:24.105842 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m15:18:24.112824 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m15:18:24.113331 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m15:18:24.126408 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:18:24.328676 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m15:18:24.329919 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m15:18:24.749928 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a5ace92c-ea39-4265-833d-4dd654015206&page=queryresults
[0m15:18:26.960325 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd9638cac50>]}
[0m15:18:26.961485 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.85s]
[0m15:18:26.962487 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m15:18:26.963753 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m15:18:26.964576 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m15:18:26.965274 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m15:18:26.965889 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m15:18:26.978097 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m15:18:26.979090 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m15:18:27.031727 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:18:27.273221 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m15:18:27.274273 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m15:18:27.492022 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:52abc615-dfcf-4a02-9d85-dfaad597e8b4&page=queryresults
[0m15:18:37.115575 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd96320e310>]}
[0m15:18:37.116557 [info ] [Thread-3 (]: 2 of 5 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (91.8 MiB processed)[0m in 10.15s]
[0m15:18:37.117484 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m15:18:37.118696 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m15:18:37.119339 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m15:18:37.120068 [info ] [Thread-2 (]: 3 of 5 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m15:18:37.120892 [info ] [Thread-1 (]: 4 of 5 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m15:18:37.121631 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__monthly_player_games)
[0m15:18:37.122249 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m15:18:37.122873 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m15:18:37.123434 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m15:18:37.131645 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m15:18:37.138787 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m15:18:37.139843 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m15:18:37.140404 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m15:18:37.144611 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:18:37.148367 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:18:37.301050 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m15:18:37.302731 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m15:18:37.335213 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m15:18:37.336607 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m15:18:37.448990 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:05a3fa5f-86df-4fda-9f42-fc805fa832c9&page=queryresults
[0m15:18:37.485607 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3fda5bc7-825f-4f39-8dd3-cb194878fe6c&page=queryresults
[0m15:18:43.901232 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd94b55dad0>]}
[0m15:18:43.902198 [info ] [Thread-1 (]: 4 of 5 OK created sql incremental model dev_staging.stg__weekly_games .......... [[32mSCRIPT (364.8 MiB processed)[0m in 6.78s]
[0m15:18:43.903104 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m15:18:43.904037 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m15:18:43.904937 [info ] [Thread-3 (]: 5 of 5 START sql incremental model dev_aggregate.weekly_openings ............... [RUN]
[0m15:18:43.905801 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m15:18:43.906411 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_openings
[0m15:18:43.914690 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m15:18:43.917275 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_openings
[0m15:18:43.922825 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m15:18:43.924178 [debug] [Thread-3 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      
    partition by week_start_date
    cluster by week_start_date, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_aggregate AS (
    SELECT 
        games.week_start_date
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start_date
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY 
    week_start DESC
  , total_games DESC

)

SELECT * FROM cte_percentage
    );
  
[0m15:18:43.925043 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:18:44.408886 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:be3a9c16-78b1-4d79-ab2c-f0667ac96718&page=queryresults
[0m15:18:44.507843 [error] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:be3a9c16-78b1-4d79-ab2c-f0667ac96718&page=queryresults
[0m15:18:44.515145 [debug] [Thread-3 (]: Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Unrecognized name: week_start at [63:5]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql
[0m15:18:44.515934 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd94b4461d0>]}
[0m15:18:44.516857 [error] [Thread-3 (]: 5 of 5 ERROR creating sql incremental model dev_aggregate.weekly_openings ...... [[31mERROR[0m in 0.61s]
[0m15:18:44.517772 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m15:18:44.518637 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_openings' to be skipped because of status 'error'.  Reason: Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Unrecognized name: week_start at [63:5]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql.
[0m15:18:44.649695 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c88732e3-7fd8-436a-b662-15dd2c7baf6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd94b5d6e90>]}
[0m15:18:44.650662 [info ] [Thread-2 (]: 3 of 5 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (388.5 MiB processed)[0m in 7.53s]
[0m15:18:44.651584 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m15:18:44.653479 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:18:44.654379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:18:44.654851 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m15:18:44.655285 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m15:18:44.655731 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m15:18:44.656290 [info ] [MainThread]: 
[0m15:18:44.656806 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 21.69 seconds (21.69s).
[0m15:18:44.658767 [debug] [MainThread]: Command end result
[0m15:18:44.697114 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:18:44.699154 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:18:44.706982 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:18:44.707245 [info ] [MainThread]: 
[0m15:18:44.707540 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:18:44.707790 [info ] [MainThread]: 
[0m15:18:44.708086 [error] [MainThread]:   Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Unrecognized name: week_start at [63:5]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql
[0m15:18:44.708329 [info ] [MainThread]: 
[0m15:18:44.708590 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m15:18:44.709100 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 23.08879, "process_in_blocks": "16", "process_kernel_time": 0.357957, "process_mem_max_rss": "233948", "process_out_blocks": "3360", "process_user_time": 3.815059}
[0m15:18:44.709435 [debug] [MainThread]: Command `dbt run` failed at 15:18:44.709360 after 23.09 seconds
[0m15:18:44.709734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd99b9dde10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd99b783f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd99f177e50>]}
[0m15:18:44.710023 [debug] [MainThread]: Flushing usage events
[0m15:18:45.180437 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:19:43.074078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aacd3bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aacd46e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aacd3b290>]}


============================== 15:19:43.077159 | 3ab7672b-0baf-4990-ad89-3e4f953b7e5a ==============================
[0m15:19:43.077159 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:19:43.077615 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:19:43.737505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aad49bf50>]}
[0m15:19:43.862217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aaf774e90>]}
[0m15:19:43.862828 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:19:43.986267 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:19:44.095832 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:19:44.096379 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m15:19:44.347225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a789a0250>]}
[0m15:19:44.420094 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:19:44.422368 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:19:44.431688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a78903dd0>]}
[0m15:19:44.432157 [info ] [MainThread]: Found 5 models, 1 seed, 1 source, 492 macros
[0m15:19:44.432472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a78a3ad90>]}
[0m15:19:44.434112 [info ] [MainThread]: 
[0m15:19:44.434419 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:19:44.434692 [info ] [MainThread]: 
[0m15:19:44.435103 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:19:44.439346 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:19:44.440174 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:19:44.440968 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:19:44.441472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:44.442055 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:44.442593 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:45.428256 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m15:19:45.429058 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m15:19:45.429810 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m15:19:45.430500 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:45.431122 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:45.431640 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:19:45.662077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a79621a90>]}
[0m15:19:45.662799 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:19:45.666130 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m15:19:45.667013 [info ] [Thread-1 (]: 1 of 5 START sql table model dev_universal.calendar ............................ [RUN]
[0m15:19:45.667715 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m15:19:45.668270 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m15:19:45.681558 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m15:19:45.682551 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m15:19:45.696535 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:46.134087 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m15:19:46.135442 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m15:19:46.937675 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:9176b004-d349-445a-833e-3bc331d2c0a8&page=queryresults
[0m15:19:49.148931 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a78b06cd0>]}
[0m15:19:49.150067 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.48s]
[0m15:19:49.151094 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m15:19:49.152357 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m15:19:49.153179 [info ] [Thread-3 (]: 2 of 5 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m15:19:49.153948 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.stg__player_games)
[0m15:19:49.154568 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m15:19:49.166855 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m15:19:49.167809 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m15:19:49.213785 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:19:49.445512 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m15:19:49.447025 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m15:19:49.601950 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ce5759e6-cc63-47a6-b878-ef0c359c1830&page=queryresults
[0m15:19:56.390614 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a70db7850>]}
[0m15:19:56.391605 [info ] [Thread-3 (]: 2 of 5 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (91.8 MiB processed)[0m in 7.24s]
[0m15:19:56.392576 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m15:19:56.393599 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m15:19:56.394102 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m15:19:56.394826 [info ] [Thread-2 (]: 3 of 5 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m15:19:56.395780 [info ] [Thread-1 (]: 4 of 5 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m15:19:56.396610 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__monthly_player_games)
[0m15:19:56.397247 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m15:19:56.397877 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m15:19:56.398439 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m15:19:56.407124 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m15:19:56.414350 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m15:19:56.415260 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m15:19:56.415834 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m15:19:56.419922 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:19:56.423658 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:56.572389 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m15:19:56.573862 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m15:19:56.598224 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m15:19:56.599638 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 60 AND CURRENT_DATE()
  

      GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m15:19:56.698023 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:09fde750-6d95-4db0-9748-906c52730c00&page=queryresults
[0m15:19:56.841154 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:90817787-c90b-48c3-9ce0-b60091c5c954&page=queryresults
[0m15:20:03.873470 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a70c6cb90>]}
[0m15:20:03.874548 [info ] [Thread-2 (]: 3 of 5 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (388.5 MiB processed)[0m in 7.48s]
[0m15:20:03.875553 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m15:20:04.012937 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a70be4250>]}
[0m15:20:04.013906 [info ] [Thread-1 (]: 4 of 5 OK created sql incremental model dev_staging.stg__weekly_games .......... [[32mSCRIPT (364.8 MiB processed)[0m in 7.62s]
[0m15:20:04.014822 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m15:20:04.015890 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_openings
[0m15:20:04.016391 [info ] [Thread-3 (]: 5 of 5 START sql incremental model dev_aggregate.weekly_openings ............... [RUN]
[0m15:20:04.017148 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m15:20:04.017796 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_openings
[0m15:20:04.023788 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m15:20:04.024467 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_openings
[0m15:20:04.028654 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m15:20:04.029529 [debug] [Thread-3 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      
    partition by week_start_date
    cluster by week_start_date, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_aggregate AS (
    SELECT 
        games.week_start_date
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start_date
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY
    week_start_date DESC
  , total_games     DESC

)

SELECT * FROM cte_percentage
    );
  
[0m15:20:04.030067 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:20:05.214699 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bda6af7d-902e-4908-a6f8-eda5a333e853&page=queryresults
[0m15:20:09.038490 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3ab7672b-0baf-4990-ad89-3e4f953b7e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a70d9f710>]}
[0m15:20:09.039535 [info ] [Thread-3 (]: 5 of 5 OK created sql incremental model dev_aggregate.weekly_openings .......... [[32mCREATE TABLE (23.1k rows, 181.3 MiB processed)[0m in 5.02s]
[0m15:20:09.040211 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_openings
[0m15:20:09.041933 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:20:09.042857 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:20:09.043319 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m15:20:09.043771 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m15:20:09.044202 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m15:20:09.044775 [info ] [MainThread]: 
[0m15:20:09.045282 [info ] [MainThread]: Finished running 4 incremental models, 1 table model in 0 hours 0 minutes and 24.61 seconds (24.61s).
[0m15:20:09.047251 [debug] [MainThread]: Command end result
[0m15:20:09.084381 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:20:09.086395 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:20:09.093381 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:20:09.093648 [info ] [MainThread]: 
[0m15:20:09.093948 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:20:09.094188 [info ] [MainThread]: 
[0m15:20:09.094452 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:20:09.094963 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 26.06786, "process_in_blocks": "0", "process_kernel_time": 0.339186, "process_mem_max_rss": "233008", "process_out_blocks": "3360", "process_user_time": 3.868431}
[0m15:20:09.095298 [debug] [MainThread]: Command `dbt run` succeeded at 15:20:09.095222 after 26.07 seconds
[0m15:20:09.095592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ab09c9f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aacf23fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aacd8b450>]}
[0m15:20:09.095875 [debug] [MainThread]: Flushing usage events
[0m15:20:09.536045 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:00.510374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3bafd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3baf690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3bace10>]}


============================== 15:31:00.513066 | 16d955e1-dbca-401a-a9eb-88ad7b48529f ==============================
[0m15:31:00.513066 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:31:00.513513 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run -s test_incremental', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:31:01.141369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3bb94d0>]}
[0m15:31:01.262801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb664d750>]}
[0m15:31:01.263398 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:31:01.380236 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:31:01.476402 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 5 files changed.
[0m15:31:01.476850 [debug] [MainThread]: Partial parsing: added file: pipeline://models/universal/test_incremental.sql
[0m15:31:01.477167 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m15:31:01.477484 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__weekly_games.sql
[0m15:31:01.477789 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m15:31:01.478139 [debug] [MainThread]: Partial parsing: updated file: pipeline://macros/sql_macros.sql
[0m15:31:01.478400 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__monthly_player_games.sql
[0m15:31:01.723156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae7b8ec8d0>]}
[0m15:31:01.789418 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:31:01.791521 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:31:01.800391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae7b9c5550>]}
[0m15:31:01.800823 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m15:31:01.801104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae7ba5c290>]}
[0m15:31:01.802237 [info ] [MainThread]: 
[0m15:31:01.802522 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:01.802770 [info ] [MainThread]: 
[0m15:31:01.803144 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:31:01.804097 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:31:01.804506 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:02.725102 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m15:31:02.726028 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m15:31:02.726544 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:31:02.727326 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m15:31:02.727687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:02.729668 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:02.926325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae7bacc890>]}
[0m15:31:02.927023 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:31:02.930144 [debug] [Thread-1 (]: Began running node model.pipeline.test_incremental
[0m15:31:02.930988 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_universal.test_incremental .............. [RUN]
[0m15:31:02.931653 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.test_incremental)
[0m15:31:02.932216 [debug] [Thread-1 (]: Began compiling node model.pipeline.test_incremental
[0m15:31:02.943303 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m15:31:02.944146 [debug] [Thread-1 (]: Began executing node model.pipeline.test_incremental
[0m15:31:03.003135 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.test_incremental"
[0m15:31:03.003711 [debug] [Thread-1 (]: On model.pipeline.test_incremental: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test_incremental"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`test_incremental`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      SELECT
  'False' AS is_incremental_run
    );
  
[0m15:31:03.004051 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:03.493819 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0dd175f5-7060-47c7-a5c5-13ea8932bd7c&page=queryresults
[0m15:31:05.115992 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '16d955e1-dbca-401a-a9eb-88ad7b48529f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae7baa5cd0>]}
[0m15:31:05.117082 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_universal.test_incremental ......... [[32mCREATE TABLE (1.0 rows, 0 processed)[0m in 2.18s]
[0m15:31:05.118098 [debug] [Thread-1 (]: Finished running node model.pipeline.test_incremental
[0m15:31:05.119981 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:31:05.120916 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:05.121369 [debug] [MainThread]: Connection 'model.pipeline.test_incremental' was properly closed.
[0m15:31:05.121817 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m15:31:05.122242 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m15:31:05.122720 [info ] [MainThread]: 
[0m15:31:05.123212 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.32 seconds (3.32s).
[0m15:31:05.124195 [debug] [MainThread]: Command end result
[0m15:31:05.158616 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:31:05.160560 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:31:05.169026 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:31:05.169528 [info ] [MainThread]: 
[0m15:31:05.170127 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:31:05.170603 [info ] [MainThread]: 
[0m15:31:05.171047 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:31:05.171706 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.703738, "process_in_blocks": "240", "process_kernel_time": 0.324246, "process_mem_max_rss": "231736", "process_out_blocks": "3200", "process_user_time": 3.477313}
[0m15:31:05.172101 [debug] [MainThread]: Command `dbt run` succeeded at 15:31:05.172025 after 4.70 seconds
[0m15:31:05.172386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3be2490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3ba4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faeb3ba7f90>]}
[0m15:31:05.172674 [debug] [MainThread]: Flushing usage events
[0m15:31:05.544763 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:47.568407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03541839d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0354183e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0354180b50>]}


============================== 15:31:47.571363 | 2c6166b4-f964-4456-ae00-57833f829470 ==============================
[0m15:31:47.571363 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:31:47.571815 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s test_incremental --full-refresh', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:31:48.182051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0320330910>]}
[0m15:31:48.302191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0356b74dd0>]}
[0m15:31:48.302743 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:31:48.422245 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:31:48.522993 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:48.523331 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:48.560514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f031bf70e10>]}
[0m15:31:48.626768 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:31:48.629318 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:31:48.638212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03201138d0>]}
[0m15:31:48.638661 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m15:31:48.638947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f031bf723d0>]}
[0m15:31:48.640124 [info ] [MainThread]: 
[0m15:31:48.640418 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:31:48.640664 [info ] [MainThread]: 
[0m15:31:48.641069 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:31:48.642180 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:31:48.642806 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:49.553438 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m15:31:49.554334 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m15:31:49.554943 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m15:31:49.555356 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:31:49.555881 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:49.556278 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:49.739857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0354f41390>]}
[0m15:31:49.740573 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:31:49.743897 [debug] [Thread-1 (]: Began running node model.pipeline.test_incremental
[0m15:31:49.744737 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_universal.test_incremental .............. [RUN]
[0m15:31:49.745393 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.test_incremental)
[0m15:31:49.745959 [debug] [Thread-1 (]: Began compiling node model.pipeline.test_incremental
[0m15:31:49.765309 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m15:31:49.765843 [debug] [Thread-1 (]: Began executing node model.pipeline.test_incremental
[0m15:31:49.795514 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:50.020994 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.test_incremental"
[0m15:31:50.021748 [debug] [Thread-1 (]: On model.pipeline.test_incremental: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test_incremental"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`test_incremental`
      
    
    

    OPTIONS(
      description=""""""
    )
    as (
      SELECT
  'False' AS is_incremental_run
    );
  
[0m15:31:50.415402 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:594d3247-57d2-44a9-aa49-6357cc853d70&page=queryresults
[0m15:31:52.025780 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c6166b4-f964-4456-ae00-57833f829470', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f031bd736d0>]}
[0m15:31:52.026826 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_universal.test_incremental ......... [[32mCREATE TABLE (1.0 rows, 0 processed)[0m in 2.28s]
[0m15:31:52.027825 [debug] [Thread-1 (]: Finished running node model.pipeline.test_incremental
[0m15:31:52.029738 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:31:52.030687 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:52.031142 [debug] [MainThread]: Connection 'model.pipeline.test_incremental' was properly closed.
[0m15:31:52.031588 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m15:31:52.032014 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m15:31:52.032500 [info ] [MainThread]: 
[0m15:31:52.033009 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.39 seconds (3.39s).
[0m15:31:52.033999 [debug] [MainThread]: Command end result
[0m15:31:52.069683 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:31:52.071794 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:31:52.079877 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:31:52.080214 [info ] [MainThread]: 
[0m15:31:52.080635 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:31:52.080947 [info ] [MainThread]: 
[0m15:31:52.081281 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:31:52.081867 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.555477, "process_in_blocks": "0", "process_kernel_time": 0.328842, "process_mem_max_rss": "228476", "process_out_blocks": "2152", "process_user_time": 3.261303}
[0m15:31:52.082209 [debug] [MainThread]: Command `dbt run` succeeded at 15:31:52.082131 after 4.56 seconds
[0m15:31:52.082514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0354e97650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0357dca090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0354401e10>]}
[0m15:31:52.082797 [debug] [MainThread]: Flushing usage events
[0m15:31:52.425598 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:33:17.247027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7602583350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76027c6490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76025b58d0>]}


============================== 15:33:17.250083 | 3aa6200f-c152-43ef-847a-17ee76d9063a ==============================
[0m15:33:17.250083 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:33:17.250579 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s test_incremental', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:33:17.928628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76026185d0>]}
[0m15:33:18.058418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7604f75750>]}
[0m15:33:18.059051 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:33:18.181411 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:33:18.280879 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:33:18.281251 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:33:18.320896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ca3bd610>]}
[0m15:33:18.396668 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:33:18.398987 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:33:18.408520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ca388a50>]}
[0m15:33:18.409005 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m15:33:18.409316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ca4976d0>]}
[0m15:33:18.410672 [info ] [MainThread]: 
[0m15:33:18.410980 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:33:18.411235 [info ] [MainThread]: 
[0m15:33:18.411676 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:33:18.412991 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:33:18.413671 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:19.290882 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m15:33:19.291586 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m15:33:19.291877 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:33:19.292362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:19.292871 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m15:33:19.295802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:19.457109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ca67d750>]}
[0m15:33:19.457792 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:33:19.461065 [debug] [Thread-1 (]: Began running node model.pipeline.test_incremental
[0m15:33:19.461833 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_universal.test_incremental .............. [RUN]
[0m15:33:19.462491 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.test_incremental)
[0m15:33:19.463038 [debug] [Thread-1 (]: Began compiling node model.pipeline.test_incremental
[0m15:33:19.482660 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m15:33:19.483192 [debug] [Thread-1 (]: Began executing node model.pipeline.test_incremental
[0m15:33:19.513067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:33:19.742651 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.test_incremental"
[0m15:33:19.743256 [debug] [Thread-1 (]: On model.pipeline.test_incremental: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test_incremental"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `checkmate-453316`.`dev_universal`.`test_incremental` as DBT_INTERNAL_DEST
        using (SELECT
  'True' AS is_incremental_run
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`is_incremental_run`)
    values
        (`is_incremental_run`)


    
[0m15:33:20.228530 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:74fc7a73-21e5-4f44-8336-767f4c51900b&page=queryresults
[0m15:33:21.966967 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3aa6200f-c152-43ef-847a-17ee76d9063a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f75ca6abcd0>]}
[0m15:33:21.967620 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_universal.test_incremental ......... [[32mMERGE (1.0 rows, 0 processed)[0m in 2.50s]
[0m15:33:21.968150 [debug] [Thread-1 (]: Finished running node model.pipeline.test_incremental
[0m15:33:21.969472 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:33:21.969944 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:33:21.970184 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m15:33:21.970419 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m15:33:21.970661 [debug] [MainThread]: Connection 'model.pipeline.test_incremental' was properly closed.
[0m15:33:21.970905 [info ] [MainThread]: 
[0m15:33:21.971160 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.56 seconds (3.56s).
[0m15:33:21.971677 [debug] [MainThread]: Command end result
[0m15:33:21.992286 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:33:21.993590 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:33:21.998568 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:33:21.998888 [info ] [MainThread]: 
[0m15:33:21.999236 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:33:21.999508 [info ] [MainThread]: 
[0m15:33:21.999791 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:33:22.000359 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.802613, "process_in_blocks": "0", "process_kernel_time": 0.340327, "process_mem_max_rss": "228488", "process_out_blocks": "2152", "process_user_time": 3.35327}
[0m15:33:22.000746 [debug] [MainThread]: Command `dbt run` succeeded at 15:33:22.000662 after 4.80 seconds
[0m15:33:22.001065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f760256ff90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7603297650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7602801c90>]}
[0m15:33:22.001389 [debug] [MainThread]: Flushing usage events
[0m15:33:22.350829 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:33:39.404550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6788be6b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6788b8f190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6788dda490>]}


============================== 15:33:39.407665 | ac98f04c-ff76-4056-9557-c3eea1b28920 ==============================
[0m15:33:39.407665 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:33:39.408106 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run -s test_incremental', 'send_anonymous_usage_stats': 'True'}
[0m15:33:40.050320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67568f80d0>]}
[0m15:33:40.160890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f678b5758d0>]}
[0m15:33:40.161478 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:33:40.283999 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:33:40.383607 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:33:40.383965 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:33:40.425695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6754a12910>]}
[0m15:33:40.502421 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:33:40.504734 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:33:40.514194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6754a12810>]}
[0m15:33:40.514677 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m15:33:40.514970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6754ccbe50>]}
[0m15:33:40.516256 [info ] [MainThread]: 
[0m15:33:40.516611 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:33:40.516853 [info ] [MainThread]: 
[0m15:33:40.517271 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:33:40.518619 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m15:33:40.519294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:41.374598 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m15:33:41.375227 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m15:33:41.375764 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:33:41.376333 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m15:33:41.376787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:41.378851 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:41.561665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6788ecb250>]}
[0m15:33:41.562690 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:33:41.566067 [debug] [Thread-1 (]: Began running node model.pipeline.test_incremental
[0m15:33:41.566536 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_universal.test_incremental .............. [RUN]
[0m15:33:41.566901 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.test_incremental)
[0m15:33:41.567189 [debug] [Thread-1 (]: Began compiling node model.pipeline.test_incremental
[0m15:33:41.578581 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m15:33:41.579102 [debug] [Thread-1 (]: Began executing node model.pipeline.test_incremental
[0m15:33:41.609831 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:33:41.823438 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.test_incremental"
[0m15:33:41.824054 [debug] [Thread-1 (]: On model.pipeline.test_incremental: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test_incremental"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `checkmate-453316`.`dev_universal`.`test_incremental` as DBT_INTERNAL_DEST
        using (SELECT
  'True' AS is_incremental_run
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`is_incremental_run`)
    values
        (`is_incremental_run`)


    
[0m15:33:42.326386 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:073cc398-d744-47ef-84ca-7b8d31e1bf37&page=queryresults
[0m15:33:44.306035 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac98f04c-ff76-4056-9557-c3eea1b28920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67547e4990>]}
[0m15:33:44.306627 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_universal.test_incremental ......... [[32mMERGE (1.0 rows, 0 processed)[0m in 2.74s]
[0m15:33:44.307107 [debug] [Thread-1 (]: Finished running node model.pipeline.test_incremental
[0m15:33:44.308471 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:33:44.308958 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:33:44.309202 [debug] [MainThread]: Connection 'model.pipeline.test_incremental' was properly closed.
[0m15:33:44.309438 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m15:33:44.309686 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m15:33:44.309948 [info ] [MainThread]: 
[0m15:33:44.310218 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.79 seconds (3.79s).
[0m15:33:44.310770 [debug] [MainThread]: Command end result
[0m15:33:44.330903 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:33:44.332074 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:33:44.336525 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:33:44.336780 [info ] [MainThread]: 
[0m15:33:44.337096 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:33:44.337335 [info ] [MainThread]: 
[0m15:33:44.337604 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:33:44.338127 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.9808497, "process_in_blocks": "0", "process_kernel_time": 0.304654, "process_mem_max_rss": "228196", "process_out_blocks": "2144", "process_user_time": 3.337576}
[0m15:33:44.338477 [debug] [MainThread]: Command `dbt run` succeeded at 15:33:44.338393 after 4.98 seconds
[0m15:33:44.338773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f678c577e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6788b8c090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6788b8f490>]}
[0m15:33:44.339068 [debug] [MainThread]: Flushing usage events
[0m15:33:44.692839 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:17:23.216236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f689d73ab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f689d97a390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f689d979c10>]}


============================== 19:17:23.220040 | 5b6781bd-0114-4ef0-9231-fcfdb4e5d096 ==============================
[0m19:17:23.220040 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:17:23.220509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:17:23.914874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6865afa910>]}
[0m19:17:24.042451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f68a0185690>]}
[0m19:17:24.043056 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:17:24.169496 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:17:24.281176 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:17:24.281950 [debug] [MainThread]: Partial parsing: updated file: pipeline://macros/sql_macros.sql
[0m19:17:24.563079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f68660fbd90>]}
[0m19:17:24.635748 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:17:24.638622 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:17:24.648432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6865542b10>]}
[0m19:17:24.648910 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m19:17:24.649213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f68657af4d0>]}
[0m19:17:24.650924 [info ] [MainThread]: 
[0m19:17:24.651233 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:17:24.651501 [info ] [MainThread]: 
[0m19:17:24.651933 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:17:24.656440 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:17:24.657267 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:17:24.657740 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:17:24.658188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:17:24.659064 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:17:24.660369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:17:25.954921 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m19:17:25.955715 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m19:17:25.956459 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m19:17:25.956960 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:17:25.957574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:17:25.958104 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:17:26.162652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f689d7fb7d0>]}
[0m19:17:26.163588 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:17:26.167225 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:17:26.167677 [debug] [Thread-2 (]: Began running node model.pipeline.test_incremental
[0m19:17:26.168505 [info ] [Thread-1 (]: 1 of 6 START sql table model dev_universal.calendar ............................ [RUN]
[0m19:17:26.169295 [info ] [Thread-2 (]: 2 of 6 START sql incremental model dev_universal.test_incremental .............. [RUN]
[0m19:17:26.170011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m19:17:26.170631 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.test_incremental)
[0m19:17:26.171209 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:17:26.171782 [debug] [Thread-2 (]: Began compiling node model.pipeline.test_incremental
[0m19:17:26.182117 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:17:26.190529 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m19:17:26.191344 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:17:26.204460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:17:26.204938 [debug] [Thread-2 (]: Began executing node model.pipeline.test_incremental
[0m19:17:26.250011 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:17:26.428912 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m19:17:26.429880 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m19:17:26.499922 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.test_incremental"
[0m19:17:26.500558 [debug] [Thread-2 (]: On model.pipeline.test_incremental: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test_incremental"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `checkmate-453316`.`dev_universal`.`test_incremental` as DBT_INTERNAL_DEST
        using (SELECT
  'True' AS is_incremental_run
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`is_incremental_run`)
    values
        (`is_incremental_run`)


    
[0m19:17:26.854134 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ff184592-0c03-4862-b997-75108278158a&page=queryresults
[0m19:17:27.005599 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:50e08f7a-180c-4b96-a210-c3a43889628e&page=queryresults
[0m19:17:28.686173 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6865994210>]}
[0m19:17:28.693025 [info ] [Thread-2 (]: 2 of 6 OK created sql incremental model dev_universal.test_incremental ......... [[32mMERGE (1.0 rows, 0 processed)[0m in 2.51s]
[0m19:17:28.694071 [debug] [Thread-2 (]: Finished running node model.pipeline.test_incremental
[0m19:17:29.020238 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f68657bda90>]}
[0m19:17:29.020830 [info ] [Thread-1 (]: 1 of 6 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.85s]
[0m19:17:29.021349 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:17:29.022502 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m19:17:29.023458 [info ] [Thread-4 (]: 3 of 6 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m19:17:29.024352 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m19:17:29.024973 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m19:17:29.034566 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m19:17:29.035866 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m19:17:29.054880 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:17:29.286005 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m19:17:29.287694 [debug] [Thread-4 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m19:17:29.459938 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3245eeeb-80d2-49c8-a42b-36a59d3a208c&page=queryresults
[0m19:17:35.895639 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f685d7f3b10>]}
[0m19:17:35.896725 [info ] [Thread-4 (]: 3 of 6 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (9.6 MiB processed)[0m in 6.87s]
[0m19:17:35.897733 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m19:17:35.899066 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m19:17:35.899681 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m19:17:35.900440 [info ] [Thread-2 (]: 4 of 6 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m19:17:35.901139 [info ] [Thread-1 (]: 5 of 6 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m19:17:35.901742 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.test_incremental, now model.pipeline.stg__monthly_player_games)
[0m19:17:35.902295 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m19:17:35.902805 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m19:17:35.903318 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m19:17:35.911040 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m19:17:35.917501 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m19:17:35.918375 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m19:17:35.918871 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m19:17:35.924572 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:17:35.928242 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:17:36.092332 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m19:17:36.093810 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m19:17:36.105724 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m19:17:36.106574 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

      GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m19:17:36.240296 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a393a14f-2f1e-4f2c-9e20-2eabad9cc0ab&page=queryresults
[0m19:17:36.261320 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e5b07753-8b76-4eb6-bdc6-8cafe0614b38&page=queryresults
[0m19:17:42.303864 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f685d521310>]}
[0m19:17:42.304724 [info ] [Thread-2 (]: 4 of 6 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (366.0 MiB processed)[0m in 6.40s]
[0m19:17:42.305503 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m19:17:42.683057 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f686554fc90>]}
[0m19:17:42.684057 [info ] [Thread-1 (]: 5 of 6 OK created sql incremental model dev_staging.stg__weekly_games .......... [[32mSCRIPT (337.9 MiB processed)[0m in 6.78s]
[0m19:17:42.684945 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m19:17:42.686289 [debug] [Thread-4 (]: Began running node model.pipeline.weekly_openings
[0m19:17:42.687248 [info ] [Thread-4 (]: 6 of 6 START sql incremental model dev_aggregate.weekly_openings ............... [RUN]
[0m19:17:42.688045 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m19:17:42.688684 [debug] [Thread-4 (]: Began compiling node model.pipeline.weekly_openings
[0m19:17:42.697114 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m19:17:42.698014 [debug] [Thread-4 (]: Began executing node model.pipeline.weekly_openings
[0m19:17:42.702510 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m19:17:42.853057 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m19:17:42.854599 [debug] [Thread-4 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`
      
    partition by week_start_date
    cluster by week_start_date, opening_archetype, time_class

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_aggregate AS (
    SELECT 
        games.week_start_date
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.week_start BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start_date
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY
    week_start_date DESC
  , total_games     DESC

)

SELECT * FROM cte_percentage
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_aggregate`.`weekly_openings` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `black_win_count`, `black_loss_count`, `black_draw_count`)
    values
        (`week_start_date`, `week_number`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `black_win_count`, `black_loss_count`, `black_draw_count`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`

  


  

    
[0m19:17:42.994542 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:86729055-65f6-4452-992e-ebabc1f0def1&page=queryresults
[0m19:17:43.707732 [error] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:86729055-65f6-4452-992e-ebabc1f0def1&page=queryresults
[0m19:17:43.717172 [debug] [Thread-4 (]: Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Query error: Name week_start not found inside games at [54:15]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql
[0m19:17:43.718044 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b6781bd-0114-4ef0-9231-fcfdb4e5d096', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f685d670d90>]}
[0m19:17:43.719054 [error] [Thread-4 (]: 6 of 6 ERROR creating sql incremental model dev_aggregate.weekly_openings ...... [[31mERROR[0m in 1.03s]
[0m19:17:43.720013 [debug] [Thread-4 (]: Finished running node model.pipeline.weekly_openings
[0m19:17:43.721026 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_openings' to be skipped because of status 'error'.  Reason: Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Query error: Name week_start not found inside games at [54:15]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql.
[0m19:17:43.723881 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:17:43.724829 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:17:43.725402 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m19:17:43.725971 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m19:17:43.726463 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m19:17:43.726938 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m19:17:43.727582 [info ] [MainThread]: 
[0m19:17:43.728132 [info ] [MainThread]: Finished running 5 incremental models, 1 table model in 0 hours 0 minutes and 19.08 seconds (19.08s).
[0m19:17:43.730626 [debug] [MainThread]: Command end result
[0m19:17:43.773398 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:17:43.774965 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:17:43.780553 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:17:43.780911 [info ] [MainThread]: 
[0m19:17:43.781283 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m19:17:43.781615 [info ] [MainThread]: 
[0m19:17:43.781951 [error] [MainThread]:   Database Error in model weekly_openings (models/aggregate/weekly_openings.sql)
  Query error: Name week_start not found inside games at [54:15]
  compiled code at target/run/pipeline/models/aggregate/weekly_openings.sql
[0m19:17:43.782227 [info ] [MainThread]: 
[0m19:17:43.782521 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
[0m19:17:43.783109 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 20.6163, "process_in_blocks": "0", "process_kernel_time": 0.343434, "process_mem_max_rss": "234988", "process_out_blocks": "3416", "process_user_time": 4.00494}
[0m19:17:43.783505 [debug] [MainThread]: Command `dbt run` failed at 19:17:43.783409 after 20.62 seconds
[0m19:17:43.783834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f689d923f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f685d481bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f689da90390>]}
[0m19:17:43.784156 [debug] [MainThread]: Flushing usage events
[0m19:17:44.209099 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:18:45.262151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd7699e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd78da610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd76eb790>]}


============================== 19:18:45.265181 | caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e ==============================
[0m19:18:45.265181 [info ] [MainThread]: Running with dbt=1.9.4
[0m19:18:45.265638 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:18:45.977470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fa5f78dd0>]}
[0m19:18:46.107141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fda0cd510>]}
[0m19:18:46.107752 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m19:18:46.232396 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m19:18:46.347808 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:18:46.348388 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/aggregate/weekly_openings.sql
[0m19:18:46.613831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f31dc50>]}
[0m19:18:46.690977 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:18:46.693355 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:18:46.703211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f360910>]}
[0m19:18:46.703799 [info ] [MainThread]: Found 6 models, 1 seed, 1 source, 492 macros
[0m19:18:46.704115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f34bb10>]}
[0m19:18:46.705812 [info ] [MainThread]: 
[0m19:18:46.706141 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m19:18:46.706399 [info ] [MainThread]: 
[0m19:18:46.706833 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m19:18:46.710915 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:18:46.711588 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:18:46.712365 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:18:46.713369 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m19:18:46.714169 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:18:46.714950 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:18:47.688567 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m19:18:47.689427 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m19:18:47.690149 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:18:47.690799 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m19:18:47.691413 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:18:47.693619 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:18:47.887790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f8f3f50>]}
[0m19:18:47.888857 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:18:47.893128 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m19:18:47.893854 [debug] [Thread-2 (]: Began running node model.pipeline.test_incremental
[0m19:18:47.894746 [info ] [Thread-1 (]: 1 of 6 START sql table model dev_universal.calendar ............................ [RUN]
[0m19:18:47.895607 [info ] [Thread-2 (]: 2 of 6 START sql incremental model dev_universal.test_incremental .............. [RUN]
[0m19:18:47.896408 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m19:18:47.897081 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.test_incremental)
[0m19:18:47.897725 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m19:18:47.898341 [debug] [Thread-2 (]: Began compiling node model.pipeline.test_incremental
[0m19:18:47.913323 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m19:18:47.920647 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m19:18:47.921678 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m19:18:47.922239 [debug] [Thread-2 (]: Began executing node model.pipeline.test_incremental
[0m19:18:47.949768 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:18:47.994809 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:18:48.222714 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.test_incremental"
[0m19:18:48.247627 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m19:18:48.248976 [debug] [Thread-2 (]: On model.pipeline.test_incremental: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.test_incremental"} */
-- back compat for old kwarg name
  
  
        
    

    

    merge into `checkmate-453316`.`dev_universal`.`test_incremental` as DBT_INTERNAL_DEST
        using (SELECT
  'True' AS is_incremental_run
        ) as DBT_INTERNAL_SOURCE
        on (FALSE)

    

    when not matched then insert
        (`is_incremental_run`)
    values
        (`is_incremental_run`)


    
[0m19:18:48.250237 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /*Weekly Formats - Mon to Sun*/
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /*Weekly Formats - Sun to Sat*/
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date
    FROM cte_date_array

)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m19:18:48.494927 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:50035324-42cc-46d3-80a2-68bb09caa3d3&page=queryresults
[0m19:18:48.631243 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0ac0adf8-397b-4614-ac4a-4aa907ba14aa&page=queryresults
[0m19:18:50.024077 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f2881d0>]}
[0m19:18:50.025219 [info ] [Thread-2 (]: 2 of 6 OK created sql incremental model dev_universal.test_incremental ......... [[32mMERGE (1.0 rows, 0 processed)[0m in 2.13s]
[0m19:18:50.026191 [debug] [Thread-2 (]: Finished running node model.pipeline.test_incremental
[0m19:18:50.316591 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f87699990>]}
[0m19:18:50.317754 [info ] [Thread-1 (]: 1 of 6 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.42s]
[0m19:18:50.318992 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m19:18:50.320219 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m19:18:50.321343 [info ] [Thread-4 (]: 3 of 6 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m19:18:50.322176 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m19:18:50.322735 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m19:18:50.332944 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m19:18:50.333868 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m19:18:50.350769 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m19:18:50.561655 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m19:18:50.562620 [debug] [Thread-4 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      




WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

cte_base AS (

    SELECT 
        t.game_id
      , t.game_date
      , cal.month_start_date                                   AS game_month
      , cal.month_year_type1                                   AS game_month_str
      , t.username
      , t.rating
      , t.piece_color
      , t.time_class
      , t.rules
      , t.result                                               AS raw_result
      , CASE
            WHEN t.result = "win"                 THEN "win"
            WHEN t.result = "timeout"             THEN "loss"
            WHEN t.result = "threecheck"          THEN "loss"
            WHEN t.result = "resigned"            THEN "loss"
            WHEN t.result = "kingofthehill"       THEN "loss"
            WHEN t.result = "checkmated"          THEN "loss"
            WHEN t.result = "bughousepartnerlose" THEN "loss"
            WHEN t.result = "abandoned"           THEN "loss"
            WHEN t.result = "timevsinsufficient"  THEN "draw"
            WHEN t.result = "stalemate"           THEN "draw"
            WHEN t.result = "repetition"          THEN "draw"
            WHEN t.result = "insufficient"        THEN "draw"
            WHEN t.result = "agreed"              THEN "draw"
            WHEN t.result = "50move"              THEN "draw"
        END                                                   AS win_loss_draw
      , t.opening_line                                        AS opening_line
      , TRIM(
            REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
        )                                                     AS opening
      , t.accuracy
    FROM cte_white_black_union t
    LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date

    WHERE 1=1 
      AND rated = TRUE
      
  
    AND t.game_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  


)

SELECT * FROM cte_base
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m19:18:50.724721 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:34da0b0d-6fa9-4102-93b2-169ba2ff4e9b&page=queryresults
[0m19:18:55.995991 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f8d3f50>]}
[0m19:18:55.996558 [info ] [Thread-4 (]: 3 of 6 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (9.6 MiB processed)[0m in 5.67s]
[0m19:18:55.997023 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m19:18:55.997902 [debug] [Thread-2 (]: Began running node model.pipeline.stg__monthly_player_games
[0m19:18:55.998366 [debug] [Thread-1 (]: Began running node model.pipeline.stg__weekly_games
[0m19:18:55.998949 [info ] [Thread-2 (]: 4 of 6 START sql incremental model dev_staging.stg__monthly_player_games ....... [RUN]
[0m19:18:55.999535 [info ] [Thread-1 (]: 5 of 6 START sql incremental model dev_staging.stg__weekly_games ............... [RUN]
[0m19:18:56.000097 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.test_incremental, now model.pipeline.stg__monthly_player_games)
[0m19:18:56.000736 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.stg__weekly_games)
[0m19:18:56.001186 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__monthly_player_games
[0m19:18:56.001655 [debug] [Thread-1 (]: Began compiling node model.pipeline.stg__weekly_games
[0m19:18:56.009051 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__monthly_player_games"
[0m19:18:56.013484 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.stg__weekly_games"
[0m19:18:56.014161 [debug] [Thread-1 (]: Began executing node model.pipeline.stg__weekly_games
[0m19:18:56.014552 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__monthly_player_games
[0m19:18:56.017144 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:18:56.021403 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m19:18:56.160898 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__monthly_player_games"
[0m19:18:56.162667 [debug] [Thread-2 (]: On model.pipeline.stg__monthly_player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__monthly_player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      
    partition by date_trunc(game_month, month)
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT  
              t.game_month
            , t.game_month_str
            , t.username
            , t.piece_color
            , t.rules
            , t.time_class
            , t.opening
            , AVG(t.rating)                                             AS avg_rating
            , AVG(t.accuracy)                                           AS avg_accuracy
            , SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count
            , SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count
            , SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count
            , COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      WHERE 1=1
        
  
    AND t.game_month BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

      GROUP BY ALL
),

cte_struct_agg AS (
      SELECT
              game_month
            , game_month_str
            , username
            , piece_color
            , rules
            , time_class
            , CAST(AVG(avg_rating)AS INT64)                             AS avg_rating
            , ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy
            , SUM(win_count)                                            AS total_win_count
            , SUM(loss_count)                                           AS total_loss_count
            , SUM(draw_count)                                           AS total_draw_count
            , SUM(total)                                                AS total_games
            , ARRAY_AGG(
                        STRUCT(
                               opening,
                               win_count,
                               loss_count,
                               draw_count,
                               total
                        )
                        ORDER BY total DESC
            )                                                           AS openings
      FROM cte_base_aggregate
      GROUP BY ALL
)

SELECT * FROM cte_struct_agg
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date_trunc(game_month, month) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__monthly_player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date_trunc(DBT_INTERNAL_DEST.game_month, month) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`game_month`, `game_month_str`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__monthly_player_games__dbt_tmp`

  


  

    
[0m19:18:56.173486 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.stg__weekly_games"
[0m19:18:56.174969 [debug] [Thread-1 (]: On model.pipeline.stg__weekly_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__weekly_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__weekly_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      
    partition by week_start_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_base_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND cal.week_start_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

      GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      piece_color,
      rules,
      time_class,
      CAST(AVG(avg_rating)AS INT64)                             AS avg_rating,
      ROUND(AVG(avg_accuracy),1)                                AS avg_accuracy,
      SUM(win_count)                                            AS total_win_count,
      SUM(loss_count)                                           AS total_loss_count,
      SUM(draw_count)                                           AS total_draw_count,
      SUM(total)                                                AS total_games,
      ARRAY_AGG(
                  STRUCT(
                         opening,
                         win_count,
                         loss_count,
                         draw_count,
                         total
                  )
                  ORDER BY total DESC
      )                                                         AS openings
FROM cte_base_aggregate
GROUP BY ALL
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__weekly_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)
    values
        (`week_start_date`, `week_number`, `username`, `piece_color`, `rules`, `time_class`, `avg_rating`, `avg_accuracy`, `total_win_count`, `total_loss_count`, `total_draw_count`, `total_games`, `openings`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__weekly_games__dbt_tmp`

  


  

    
[0m19:18:56.302871 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:4a4149b1-958b-4fed-90a1-facfa6d7488d&page=queryresults
[0m19:18:56.313883 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:017e36e5-ced2-4555-b95c-61359a77630f&page=queryresults
[0m19:19:02.330922 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9c0c0790>]}
[0m19:19:02.331798 [info ] [Thread-1 (]: 5 of 6 OK created sql incremental model dev_staging.stg__weekly_games .......... [[32mSCRIPT (337.9 MiB processed)[0m in 6.33s]
[0m19:19:02.332467 [debug] [Thread-1 (]: Finished running node model.pipeline.stg__weekly_games
[0m19:19:02.333120 [debug] [Thread-4 (]: Began running node model.pipeline.weekly_openings
[0m19:19:02.333601 [info ] [Thread-4 (]: 6 of 6 START sql incremental model dev_aggregate.weekly_openings ............... [RUN]
[0m19:19:02.334034 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.weekly_openings)
[0m19:19:02.334408 [debug] [Thread-4 (]: Began compiling node model.pipeline.weekly_openings
[0m19:19:02.339106 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.weekly_openings"
[0m19:19:02.339654 [debug] [Thread-4 (]: Began executing node model.pipeline.weekly_openings
[0m19:19:02.341833 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m19:19:02.346437 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f9f2e1350>]}
[0m19:19:02.347529 [info ] [Thread-2 (]: 4 of 6 OK created sql incremental model dev_staging.stg__monthly_player_games .. [[32mSCRIPT (366.0 MiB processed)[0m in 6.35s]
[0m19:19:02.348554 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__monthly_player_games
[0m19:19:02.496952 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.weekly_openings"
[0m19:19:02.498045 [debug] [Thread-4 (]: On model.pipeline.weekly_openings: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_openings"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_aggregate`.`weekly_openings`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`
      
    partition by week_start_date
    cluster by week_start_date, opening_archetype, time_class

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_aggregate AS (
    SELECT 
        games.week_start_date
      , games.week_number
      , games.time_class
      , map.opening_archetype
      , SUM(CASE WHEN games.piece_color = "white" THEN o.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN games.piece_color = "white" THEN o.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN games.piece_color = "white" THEN o.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN games.piece_color = "black" THEN o.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN games.piece_color = "black" THEN o.draw_count ELSE 0 END)   AS black_draw_count


    FROM `checkmate-453316`.`dev_staging`.`stg__weekly_games` games
    JOIN UNNEST(openings) o
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = o.opening
    WHERE 1=1
        AND rules = "chess"
        
  
    AND games.week_start_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  


    GROUP BY ALL
),

cte_percentage AS (
  SELECT
        week_start_date
      , week_number
      , time_class
      , opening_archetype
      , total_games
      , white_win_count
      , white_loss_count
      , white_draw_count
      , black_win_count
      , black_loss_count
      , black_draw_count
  FROM cte_aggregate
  ORDER BY
    week_start_date DESC
  , total_games     DESC

)

SELECT * FROM cte_percentage
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_aggregate`.`weekly_openings` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `black_win_count`, `black_loss_count`, `black_draw_count`)
    values
        (`week_start_date`, `week_number`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `black_win_count`, `black_loss_count`, `black_draw_count`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_aggregate`.`weekly_openings__dbt_tmp`

  


  

    
[0m19:19:02.617400 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:59ebc7a5-184d-43f9-b06f-4d0143e82182&page=queryresults
[0m19:19:08.704423 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'caf0174c-6e00-4bc2-9a7d-3f13dc4bfd4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6f874b2d50>]}
[0m19:19:08.705101 [info ] [Thread-4 (]: 6 of 6 OK created sql incremental model dev_aggregate.weekly_openings .......... [[32mSCRIPT (1015.5 KiB processed)[0m in 6.37s]
[0m19:19:08.705737 [debug] [Thread-4 (]: Finished running node model.pipeline.weekly_openings
[0m19:19:08.707398 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m19:19:08.708432 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:19:08.709014 [debug] [MainThread]: Connection 'model.pipeline.stg__monthly_player_games' was properly closed.
[0m19:19:08.709919 [debug] [MainThread]: Connection 'model.pipeline.stg__weekly_games' was properly closed.
[0m19:19:08.710412 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m19:19:08.710890 [debug] [MainThread]: Connection 'model.pipeline.weekly_openings' was properly closed.
[0m19:19:08.711516 [info ] [MainThread]: 
[0m19:19:08.712050 [info ] [MainThread]: Finished running 5 incremental models, 1 table model in 0 hours 0 minutes and 22.00 seconds (22.00s).
[0m19:19:08.714482 [debug] [MainThread]: Command end result
[0m19:19:08.754624 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m19:19:08.756714 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m19:19:08.763089 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m19:19:08.763488 [info ] [MainThread]: 
[0m19:19:08.763839 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:19:08.764117 [info ] [MainThread]: 
[0m19:19:08.764409 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m19:19:08.765064 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 23.553358, "process_in_blocks": "0", "process_kernel_time": 0.375505, "process_mem_max_rss": "233988", "process_out_blocks": "3408", "process_user_time": 4.07071}
[0m19:19:08.765490 [debug] [MainThread]: Command `dbt run` succeeded at 19:19:08.765383 after 23.55 seconds
[0m19:19:08.765825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fdb077dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fdb077d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd76901d0>]}
[0m19:19:08.766156 [debug] [MainThread]: Flushing usage events
[0m19:19:09.203041 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:50:13.760431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa694d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa695590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa695410>]}


============================== 20:50:13.771344 | d85fe6f1-f161-4aaf-8301-ed0c6b5e1c90 ==============================
[0m20:50:13.771344 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:50:13.771795 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'True'}
[0m20:50:16.524407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd85fe6f1-f161-4aaf-8301-ed0c6b5e1c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa6ded90>]}
[0m20:50:16.630276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd85fe6f1-f161-4aaf-8301-ed0c6b5e1c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafd0cd9d0>]}
[0m20:50:16.630819 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m20:50:16.784846 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m20:50:17.026575 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 1 files added, 2 files changed.
[0m20:50:17.027015 [debug] [MainThread]: Partial parsing: added file: pipeline://models/aggregate/weekly_rated_players.sql
[0m20:50:17.027443 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m20:50:17.027697 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/aggregate/weekly_openings.sql
[0m20:50:17.027927 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/staging/stg__weekly_games.sql
[0m20:50:17.028152 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/staging/stg__monthly_player_games.sql
[0m20:50:17.028449 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m20:50:17.310635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd85fe6f1-f161-4aaf-8301-ed0c6b5e1c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feac2202c10>]}
[0m20:50:17.372444 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:50:17.375764 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:50:17.399697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd85fe6f1-f161-4aaf-8301-ed0c6b5e1c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feac2252050>]}
[0m20:50:17.400113 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m20:50:17.400391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd85fe6f1-f161-4aaf-8301-ed0c6b5e1c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feac23d0350>]}
[0m20:50:17.401797 [info ] [MainThread]: 
[0m20:50:17.402072 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:50:17.402310 [info ] [MainThread]: 
[0m20:50:17.402686 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:50:17.406317 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m20:50:17.407356 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m20:50:17.407979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:17.408592 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m20:50:17.409163 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:17.409899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:19.237782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd85fe6f1-f161-4aaf-8301-ed0c6b5e1c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feac28990d0>]}
[0m20:50:19.238449 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:50:19.247832 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m20:50:19.248450 [debug] [Thread-2 (]: Began running node model.pipeline.test_incremental
[0m20:50:19.249111 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m20:50:19.249718 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m20:50:19.250324 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.test_incremental)
[0m20:50:19.250931 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m20:50:19.251507 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m20:50:19.252022 [debug] [Thread-2 (]: Began compiling node model.pipeline.test_incremental
[0m20:50:19.252586 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m20:50:19.259844 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m20:50:19.266722 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m20:50:19.269497 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m20:50:19.270340 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m20:50:19.271646 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m20:50:19.275992 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m20:50:19.276458 [debug] [Thread-2 (]: Began executing node model.pipeline.test_incremental
[0m20:50:19.277080 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:50:19.277684 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:50:19.278925 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m20:50:19.280166 [debug] [Thread-2 (]: Finished running node model.pipeline.test_incremental
[0m20:50:19.281334 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m20:50:19.282072 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m20:50:19.282627 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m20:50:19.290489 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m20:50:19.291875 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m20:50:19.292446 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:50:19.293706 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m20:50:19.294548 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_rated_players
[0m20:50:19.295131 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.weekly_rated_players)
[0m20:50:19.295841 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_rated_players
[0m20:50:19.303494 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_rated_players"
[0m20:50:19.304468 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_rated_players
[0m20:50:19.304918 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:50:19.305739 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_rated_players
[0m20:50:19.306784 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:50:19.307317 [debug] [MainThread]: Connection 'model.pipeline.test_incremental' was properly closed.
[0m20:50:19.307816 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m20:50:19.308252 [debug] [MainThread]: Connection 'model.pipeline.weekly_rated_players' was properly closed.
[0m20:50:19.308698 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m20:50:19.310615 [debug] [MainThread]: Command end result
[0m20:50:19.347804 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:50:19.349801 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:50:19.358822 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m20:50:19.359365 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.6414957, "process_in_blocks": "274616", "process_kernel_time": 0.541775, "process_mem_max_rss": "227180", "process_out_blocks": "3184", "process_user_time": 3.541755}
[0m20:50:19.359728 [debug] [MainThread]: Command `dbt compile` succeeded at 20:50:19.359648 after 5.64 seconds
[0m20:50:19.360022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa6ef490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa6ef690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feafa683f10>]}
[0m20:50:19.360332 [debug] [MainThread]: Flushing usage events
[0m20:50:19.712946 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:50:39.186471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1607197650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16071979d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16071ef450>]}


============================== 20:50:39.189058 | 14668f1e-3088-4b5c-998f-e075cfe70e5a ==============================
[0m20:50:39.189058 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:50:39.189484 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt clean', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:50:39.271334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '14668f1e-3088-4b5c-998f-e075cfe70e5a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1607090450>]}
[0m20:50:39.290438 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.1460409, "process_in_blocks": "88", "process_kernel_time": 0.157746, "process_mem_max_rss": "102448", "process_out_blocks": "16", "process_user_time": 1.095389}
[0m20:50:39.290802 [debug] [MainThread]: Command `dbt clean` succeeded at 20:50:39.290721 after 0.15 seconds
[0m20:50:39.291097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f160719e050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f160719dcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f160ab77e50>]}
[0m20:50:39.291397 [debug] [MainThread]: Flushing usage events
[0m20:50:39.629313 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:50:43.900460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc81f59b790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc81f59b610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc81f59a0d0>]}


============================== 20:50:43.903187 | 81d3681b-f1dd-44d7-a4de-bf015e759161 ==============================
[0m20:50:43.903187 [info ] [MainThread]: Running with dbt=1.9.4
[0m20:50:43.903628 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:50:44.501502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7eddd5b10>]}
[0m20:50:44.608629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc821fcd610>]}
[0m20:50:44.609174 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m20:50:44.727343 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m20:50:44.727904 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:50:44.728206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e78f7d90>]}
[0m20:50:45.789609 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m20:50:45.944140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e54b4450>]}
[0m20:50:46.020162 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:50:46.022369 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:50:46.030833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e51480d0>]}
[0m20:50:46.031288 [info ] [MainThread]: Found 4 models, 1 seed, 1 source, 492 macros
[0m20:50:46.031601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e54457d0>]}
[0m20:50:46.033133 [info ] [MainThread]: 
[0m20:50:46.033429 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:50:46.033739 [info ] [MainThread]: 
[0m20:50:46.034144 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m20:50:46.038074 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m20:50:46.038737 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m20:50:46.039049 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:46.039677 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m20:50:46.040071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:46.040672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:46.904438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81d3681b-f1dd-44d7-a4de-bf015e759161', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7e75747d0>]}
[0m20:50:46.904971 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:50:46.907826 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m20:50:46.908358 [debug] [Thread-2 (]: Began running node model.pipeline.test_incremental
[0m20:50:46.908808 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m20:50:46.909466 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m20:50:46.909963 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_aggregate, now model.pipeline.test_incremental)
[0m20:50:46.910412 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m20:50:46.911014 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m20:50:46.911455 [debug] [Thread-2 (]: Began compiling node model.pipeline.test_incremental
[0m20:50:46.911883 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m20:50:46.929348 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m20:50:46.929827 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.test_incremental"
[0m20:50:46.932617 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m20:50:46.933615 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m20:50:46.934923 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m20:50:46.935764 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m20:50:46.936331 [debug] [Thread-2 (]: Began executing node model.pipeline.test_incremental
[0m20:50:46.936885 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:50:46.937457 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m20:50:46.938659 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m20:50:46.939833 [debug] [Thread-2 (]: Finished running node model.pipeline.test_incremental
[0m20:50:46.940904 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m20:50:46.941744 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m20:50:46.942283 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m20:50:46.950010 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m20:50:46.950855 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m20:50:46.951428 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m20:50:46.952641 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m20:50:46.953548 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_rated_players
[0m20:50:46.954224 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now model.pipeline.weekly_rated_players)
[0m20:50:46.954782 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_rated_players
[0m20:50:46.961159 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_rated_players"
[0m20:50:46.961650 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_rated_players
[0m20:50:46.961943 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:50:46.962573 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_rated_players
[0m20:50:46.963814 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:50:46.964300 [debug] [MainThread]: Connection 'model.pipeline.test_incremental' was properly closed.
[0m20:50:46.964753 [debug] [MainThread]: Connection 'seed.pipeline.opening_mapping' was properly closed.
[0m20:50:46.965185 [debug] [MainThread]: Connection 'model.pipeline.weekly_rated_players' was properly closed.
[0m20:50:46.965619 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m20:50:46.967399 [debug] [MainThread]: Command end result
[0m20:50:47.001600 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m20:50:47.003418 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m20:50:47.013017 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m20:50:47.013945 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.158992, "process_in_blocks": "240", "process_kernel_time": 0.334037, "process_mem_max_rss": "230856", "process_out_blocks": "3168", "process_user_time": 4.229302}
[0m20:50:47.014403 [debug] [MainThread]: Command `dbt compile` succeeded at 20:50:47.014300 after 3.16 seconds
[0m20:50:47.014805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc81f7cdd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc81f593590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc81f593f90>]}
[0m20:50:47.015152 [debug] [MainThread]: Flushing usage events
[0m20:50:47.355905 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:01:59.463799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd7e97890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd81f8490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd7e95d90>]}


============================== 21:01:59.467027 | 6d9be730-1146-4e39-ac1d-a7dc263faaee ==============================
[0m21:01:59.467027 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:01:59.468697 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run -s calendar', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:02:00.127048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2da406aa90>]}
[0m21:02:00.250785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dda8cd9d0>]}
[0m21:02:00.251402 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:02:00.375794 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:02:00.484003 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:02:00.484609 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m21:02:00.795888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd8dd6050>]}
[0m21:02:00.870027 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:02:00.872458 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:02:00.883212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d9fa5d2d0>]}
[0m21:02:00.883694 [info ] [MainThread]: Found 4 models, 1 seed, 1 source, 492 macros
[0m21:02:00.883995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd9adcc50>]}
[0m21:02:00.885327 [info ] [MainThread]: 
[0m21:02:00.885683 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:02:00.885943 [info ] [MainThread]: 
[0m21:02:00.886360 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:02:00.887307 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:02:00.887922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:02:01.896090 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_aggregate)
[0m21:02:01.896717 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m21:02:01.897188 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:02:01.897737 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m21:02:01.898149 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:02:01.900146 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:02:02.100236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd8dd4350>]}
[0m21:02:02.100970 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:02:02.104430 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m21:02:02.105345 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m21:02:02.106142 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m21:02:02.106779 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m21:02:02.120377 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m21:02:02.121308 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m21:02:02.136142 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:02:02.389751 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m21:02:02.390971 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Monthly Formats*/
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH) 
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) 
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH) 
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY) 
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_previous_year
    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m21:02:02.800022 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:695466c9-5fe5-4880-bbaf-01cf1e5b1aef&page=queryresults
[0m21:02:05.871988 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6d9be730-1146-4e39-ac1d-a7dc263faaee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2d7fe8b650>]}
[0m21:02:05.873028 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.76s]
[0m21:02:05.873989 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m21:02:05.875883 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:02:05.876923 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:02:05.877422 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m21:02:05.877922 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m21:02:05.878390 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m21:02:05.878919 [info ] [MainThread]: 
[0m21:02:05.879456 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.99 seconds (4.99s).
[0m21:02:05.880544 [debug] [MainThread]: Command end result
[0m21:02:05.917300 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:02:05.919403 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:02:05.927496 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:02:05.927911 [info ] [MainThread]: 
[0m21:02:05.928367 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:02:05.928681 [info ] [MainThread]: 
[0m21:02:05.928933 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:02:05.929437 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.5120773, "process_in_blocks": "2096", "process_kernel_time": 0.335221, "process_mem_max_rss": "232452", "process_out_blocks": "3168", "process_user_time": 3.627519}
[0m21:02:05.929785 [debug] [MainThread]: Command `dbt run` succeeded at 21:02:05.929710 after 6.51 seconds
[0m21:02:05.930070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ddbee2090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ddbf13e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd8ba3350>]}
[0m21:02:05.930350 [debug] [MainThread]: Flushing usage events
[0m21:02:06.298158 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:14:42.724568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd047f9bd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd047feae50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd047fead50>]}


============================== 21:14:42.727873 | c40bec3e-d5e6-44ee-96fa-deb73021de6e ==============================
[0m21:14:42.727873 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:14:42.728348 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s calendar', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:14:43.392215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0167ca050>]}
[0m21:14:43.513648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd04a9cd890>]}
[0m21:14:43.514232 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:14:43.642910 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:14:43.749833 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:14:43.750404 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m21:14:44.054131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd00ff6ed50>]}
[0m21:14:44.125669 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:14:44.127970 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:14:44.137195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0141c1350>]}
[0m21:14:44.137665 [info ] [MainThread]: Found 4 models, 1 seed, 1 source, 492 macros
[0m21:14:44.137968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd00fcf1150>]}
[0m21:14:44.139282 [info ] [MainThread]: 
[0m21:14:44.139594 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:14:44.139855 [info ] [MainThread]: 
[0m21:14:44.140281 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:14:44.141384 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:14:44.142050 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:14:45.083941 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m21:14:45.084820 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m21:14:45.085319 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:14:45.085903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:14:45.086519 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m21:14:45.090200 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:14:45.291814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd00fcb2fd0>]}
[0m21:14:45.292506 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:14:45.295899 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m21:14:45.296756 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m21:14:45.297455 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m21:14:45.298018 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m21:14:45.310647 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m21:14:45.311567 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m21:14:45.325520 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:14:45.553610 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m21:14:45.555199 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_1st_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_2nd_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_1st_previous_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_2nd_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)) AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE() AS flag_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AS flag_prev_12_months

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m21:14:46.009914 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:4434e003-d35e-440c-9678-18f507fc86e5&page=queryresults
[0m21:14:47.885615 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c40bec3e-d5e6-44ee-96fa-deb73021de6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd00fddc8d0>]}
[0m21:14:47.886655 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.59s]
[0m21:14:47.887647 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m21:14:47.889469 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:14:47.890385 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:14:47.890845 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m21:14:47.891270 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m21:14:47.891703 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m21:14:47.892160 [info ] [MainThread]: 
[0m21:14:47.892649 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.75 seconds (3.75s).
[0m21:14:47.893613 [debug] [MainThread]: Command end result
[0m21:14:47.926165 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:14:47.927625 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:14:47.933236 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:14:47.933575 [info ] [MainThread]: 
[0m21:14:47.934014 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:14:47.934329 [info ] [MainThread]: 
[0m21:14:47.934673 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:14:47.935581 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.2574954, "process_in_blocks": "0", "process_kernel_time": 0.325314, "process_mem_max_rss": "232572", "process_out_blocks": "3184", "process_user_time": 3.659502}
[0m21:14:47.936224 [debug] [MainThread]: Command `dbt run` succeeded at 21:14:47.936081 after 5.26 seconds
[0m21:14:47.936764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd04bbca090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd04bbfbe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd04b977f50>]}
[0m21:14:47.937324 [debug] [MainThread]: Flushing usage events
[0m21:14:48.303554 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:27:55.914563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05c97410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05c97190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05c94510>]}


============================== 21:27:55.917729 | 9dde9190-8bde-4517-8b24-94aff2187fbb ==============================
[0m21:27:55.917729 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:27:55.918457 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt clean', 'send_anonymous_usage_stats': 'True'}
[0m21:27:56.020659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9dde9190-8bde-4517-8b24-94aff2187fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05b95b50>]}
[0m21:27:56.029152 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.16213919, "process_in_blocks": "0", "process_kernel_time": 0.183035, "process_mem_max_rss": "102404", "process_out_blocks": "8", "process_user_time": 1.228984}
[0m21:27:56.029552 [debug] [MainThread]: Command `dbt clean` succeeded at 21:27:56.029459 after 0.16 seconds
[0m21:27:56.029867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05c8c210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05ce6710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a05c83ed0>]}
[0m21:27:56.030200 [debug] [MainThread]: Flushing usage events
[0m21:27:56.378068 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:28:00.109357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1bc2db10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1bc2d290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1bc2d3d0>]}


============================== 21:28:00.112543 | 9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762 ==============================
[0m21:28:00.112543 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:28:00.113016 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:28:00.802430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbce3d37010>]}
[0m21:28:00.935490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1e670fd0>]}
[0m21:28:00.936159 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:28:01.076156 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:28:01.076765 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:28:01.077084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbce3eb7950>]}
[0m21:28:02.190239 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m21:28:02.355963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbce3bd52d0>]}
[0m21:28:02.438390 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:28:02.440624 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:28:02.449911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbce3561f50>]}
[0m21:28:02.450494 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:28:02.450810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbce38da2d0>]}
[0m21:28:02.452462 [info ] [MainThread]: 
[0m21:28:02.452775 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:28:02.453034 [info ] [MainThread]: 
[0m21:28:02.453446 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:28:02.457579 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_aggregate'
[0m21:28:02.458418 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m21:28:02.458820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:28:02.459572 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m21:28:02.460010 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:28:02.460709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:28:03.467062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9cf4f462-f5e1-4e77-8e3c-e1dcdb2f3762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbce38f6ad0>]}
[0m21:28:03.467826 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:28:03.471357 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m21:28:03.472021 [debug] [Thread-2 (]: Began running node seed.pipeline.opening_mapping
[0m21:28:03.472719 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m21:28:03.473306 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now seed.pipeline.opening_mapping)
[0m21:28:03.473888 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m21:28:03.474468 [debug] [Thread-2 (]: Began compiling node seed.pipeline.opening_mapping
[0m21:28:03.489407 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m21:28:03.491417 [debug] [Thread-2 (]: Began executing node seed.pipeline.opening_mapping
[0m21:28:03.491922 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:28:03.492687 [debug] [Thread-2 (]: Finished running node seed.pipeline.opening_mapping
[0m21:28:03.493432 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m21:28:03.494087 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:28:03.495514 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m21:28:03.496742 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m21:28:03.497562 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m21:28:03.497999 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m21:28:03.506899 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m21:28:03.507639 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m21:28:03.508077 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m21:28:03.509022 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m21:28:03.509831 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_rated_players
[0m21:28:03.510255 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_rated_players)
[0m21:28:03.510551 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_rated_players
[0m21:28:03.519878 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_rated_players"
[0m21:28:03.520990 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_rated_players
[0m21:28:03.521650 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:28:03.523100 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_rated_players
[0m21:28:03.524750 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:28:03.525338 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_aggregate' was properly closed.
[0m21:28:03.525793 [debug] [MainThread]: Connection 'model.pipeline.weekly_rated_players' was properly closed.
[0m21:28:03.526224 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m21:28:03.526657 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m21:28:03.528233 [debug] [MainThread]: Command end result
[0m21:28:03.565648 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:28:03.567851 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:28:03.575823 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:28:03.576458 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.5208757, "process_in_blocks": "0", "process_kernel_time": 0.348504, "process_mem_max_rss": "230988", "process_out_blocks": "3176", "process_user_time": 4.426903}
[0m21:28:03.576864 [debug] [MainThread]: Command `dbt compile` succeeded at 21:28:03.576777 after 3.52 seconds
[0m21:28:03.577188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1bc2cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1bc2c4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1bc2ff50>]}
[0m21:28:03.577509 [debug] [MainThread]: Flushing usage events
[0m21:28:03.916135 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:45:06.037541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fb919bdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fb919bbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fb91984d0>]}


============================== 21:45:06.040816 | df9b2588-4502-4c21-93cf-0b59152022d3 ==============================
[0m21:45:06.040816 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:45:06.041298 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:45:06.695751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df9b2588-4502-4c21-93cf-0b59152022d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f859271d0>]}
[0m21:45:06.818827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df9b2588-4502-4c21-93cf-0b59152022d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fbbbcda10>]}
[0m21:45:06.819440 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:45:06.939948 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:45:07.047214 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m21:45:07.047707 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/chess/weekly_players.sql
[0m21:45:07.047985 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/aggregate/weekly_rated_players.sql
[0m21:45:07.048340 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m21:45:07.291707 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m21:45:07.303546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df9b2588-4502-4c21-93cf-0b59152022d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f853a6c10>]}
[0m21:45:07.374537 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:45:07.376644 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:45:07.384586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df9b2588-4502-4c21-93cf-0b59152022d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f84d16350>]}
[0m21:45:07.384999 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:45:07.385272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df9b2588-4502-4c21-93cf-0b59152022d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f85118250>]}
[0m21:45:07.386643 [info ] [MainThread]: 
[0m21:45:07.386924 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:45:07.387161 [info ] [MainThread]: 
[0m21:45:07.387554 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:45:07.391287 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m21:45:07.392368 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m21:45:07.392871 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:45:07.393671 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev'
[0m21:45:07.394113 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:45:07.394935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:45:08.675685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df9b2588-4502-4c21-93cf-0b59152022d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1f84e36610>]}
[0m21:45:08.676305 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:45:08.680260 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m21:45:08.680857 [debug] [Thread-2 (]: Began running node seed.pipeline.opening_mapping
[0m21:45:08.681422 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev, now model.pipeline.calendar)
[0m21:45:08.681965 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now seed.pipeline.opening_mapping)
[0m21:45:08.682449 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m21:45:08.682952 [debug] [Thread-2 (]: Began compiling node seed.pipeline.opening_mapping
[0m21:45:08.697004 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m21:45:08.699476 [debug] [Thread-2 (]: Began executing node seed.pipeline.opening_mapping
[0m21:45:08.700263 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:45:08.701609 [debug] [Thread-2 (]: Finished running node seed.pipeline.opening_mapping
[0m21:45:08.702153 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m21:45:08.702868 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:45:08.703934 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m21:45:08.704859 [debug] [Thread-4 (]: Began running node model.pipeline.stg__player_games
[0m21:45:08.705608 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.stg__player_games'
[0m21:45:08.706199 [debug] [Thread-4 (]: Began compiling node model.pipeline.stg__player_games
[0m21:45:08.714725 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m21:45:08.715648 [debug] [Thread-4 (]: Began executing node model.pipeline.stg__player_games
[0m21:45:08.716274 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m21:45:08.717700 [debug] [Thread-4 (]: Finished running node model.pipeline.stg__player_games
[0m21:45:08.718518 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_players
[0m21:45:08.719094 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_players)
[0m21:45:08.719482 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_players
[0m21:45:08.726860 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_players"
[0m21:45:08.727818 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_players
[0m21:45:08.728383 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:45:08.729796 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_players
[0m21:45:08.731193 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:45:08.731746 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m21:45:08.732175 [debug] [MainThread]: Connection 'model.pipeline.weekly_players' was properly closed.
[0m21:45:08.732474 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m21:45:08.732790 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m21:45:08.733878 [debug] [MainThread]: Command end result
[0m21:45:08.752695 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:45:08.753790 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:45:08.759129 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:45:08.759728 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.7648425, "process_in_blocks": "0", "process_kernel_time": 0.28396, "process_mem_max_rss": "226964", "process_out_blocks": "3192", "process_user_time": 3.45002}
[0m21:45:08.760112 [debug] [MainThread]: Command `dbt compile` succeeded at 21:45:08.760028 after 2.77 seconds
[0m21:45:08.760438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fb9193fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fb9190350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1fb9187f90>]}
[0m21:45:08.760786 [debug] [MainThread]: Flushing usage events
[0m21:45:09.337607 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:46:19.611455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3db38810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3db42bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3db8a290>]}


============================== 21:46:19.614162 | 1bba0bb0-9dfc-49ea-a9ab-dee33fabf629 ==============================
[0m21:46:19.614162 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:46:19.614617 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:46:20.252062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c05dc0a50>]}
[0m21:46:20.373262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c40575650>]}
[0m21:46:20.373824 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:46:20.494180 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:46:20.589952 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:46:20.590463 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/chess/weekly_players.sql
[0m21:46:20.801164 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m21:46:20.811536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c059eb510>]}
[0m21:46:20.874986 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:46:20.877074 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:46:20.885341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c059118d0>]}
[0m21:46:20.885774 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:46:20.886047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c0580f1d0>]}
[0m21:46:20.887459 [info ] [MainThread]: 
[0m21:46:20.887746 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:46:20.887980 [info ] [MainThread]: 
[0m21:46:20.888352 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:46:20.892091 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:46:20.892965 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:46:20.893457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:46:20.894247 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:46:20.894744 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:46:20.895481 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:46:22.226393 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_staging)
[0m21:46:22.227123 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev)
[0m21:46:22.227769 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_staging"
"
[0m21:46:22.228416 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev"
"
[0m21:46:22.242914 [debug] [ThreadPool]: On create_checkmate-453316_dev_staging: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_staging"} */
create schema if not exists `checkmate-453316`.`dev_staging`
  
[0m21:46:22.247092 [debug] [ThreadPool]: On create_checkmate-453316_dev: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev"} */
create schema if not exists `checkmate-453316`.`dev`
  
[0m21:46:22.247763 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:46:22.248317 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:46:22.475576 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:490c1de0-a2b9-44c6-b391-e5399c8b47ad&page=queryresults
[0m21:46:22.478522 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6b531285-3e41-4ed7-a4bf-58a27567a4dd&page=queryresults
[0m21:46:24.989113 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev, now list_checkmate-453316_dev_universal)
[0m21:46:24.989920 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_staging, now list_checkmate-453316_dev_staging)
[0m21:46:24.990740 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev)
[0m21:46:24.991345 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:46:24.991929 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:46:24.992438 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:46:25.342214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c05aac410>]}
[0m21:46:25.342955 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:46:25.346340 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m21:46:25.347259 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m21:46:25.347990 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev, now model.pipeline.calendar)
[0m21:46:25.348665 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m21:46:25.361123 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m21:46:25.362033 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m21:46:25.376520 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:46:25.540366 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m21:46:25.542062 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_1st_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_2nd_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_1st_previous_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_2nd_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)) AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE() AS flag_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AS flag_prev_12_months

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m21:46:25.938652 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7cbb1f23-551d-421c-bcb4-699de18f245e&page=queryresults
[0m21:46:27.807731 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c043a4b90>]}
[0m21:46:27.808775 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.46s]
[0m21:46:27.809725 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m21:46:27.810931 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m21:46:27.811749 [info ] [Thread-3 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m21:46:27.812494 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m21:46:27.813104 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m21:46:27.821668 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m21:46:27.822596 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m21:46:27.885022 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m21:46:27.885851 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    partition by game_date
    

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
),

SELECT 
    t.game_id
  , t.game_date
  , cal.month_start_date                                   AS game_month
  , cal.month_year_type1                                   AS game_month_str
  , t.username
  , t.rating
  , t.piece_color
  , t.time_class
  , t.rules
  , t.result                                               AS raw_result
  , t.rated
  , CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw
  , t.opening_line                                        AS opening_line
  , TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening
  , t.accuracy
FROM cte_white_black_union t
LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
    ON t.game_date = cal.cal_date

WHERE 1=1 
  
  

    );
  
[0m21:46:27.886380 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m21:46:28.047479 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a7150fb4-3927-4bb6-85c4-8c8931153ca4&page=queryresults
[0m21:46:28.048396 [error] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a7150fb4-3927-4bb6-85c4-8c8931153ca4&page=queryresults
[0m21:46:28.080578 [debug] [Thread-3 (]: Database Error in model stg__player_games (models/staging/stg__player_games.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [55:1]
  compiled code at target/run/pipeline/models/staging/stg__player_games.sql
[0m21:46:28.081435 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1bba0bb0-9dfc-49ea-a9ab-dee33fabf629', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c043b3f90>]}
[0m21:46:28.082383 [error] [Thread-3 (]: 2 of 3 ERROR creating sql incremental model dev_staging.stg__player_games ...... [[31mERROR[0m in 0.27s]
[0m21:46:28.083286 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m21:46:28.084146 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.stg__player_games' to be skipped because of status 'error'.  Reason: Database Error in model stg__player_games (models/staging/stg__player_games.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [55:1]
  compiled code at target/run/pipeline/models/staging/stg__player_games.sql.
[0m21:46:28.086097 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_players
[0m21:46:28.086732 [info ] [Thread-2 (]: 3 of 3 SKIP relation dev.weekly_players ........................................ [[33mSKIP[0m]
[0m21:46:28.087363 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_players
[0m21:46:28.089040 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:46:28.089947 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:46:28.090400 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m21:46:28.090850 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m21:46:28.091280 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m21:46:28.091849 [info ] [MainThread]: 
[0m21:46:28.092371 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 7.20 seconds (7.20s).
[0m21:46:28.093681 [debug] [MainThread]: Command end result
[0m21:46:28.128030 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:46:28.130057 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:46:28.138640 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:46:28.139102 [info ] [MainThread]: 
[0m21:46:28.139479 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:46:28.139820 [info ] [MainThread]: 
[0m21:46:28.140214 [error] [MainThread]:   Database Error in model stg__player_games (models/staging/stg__player_games.sql)
  Syntax error: Trailing comma after the WITH clause before the main query is not allowed at [55:1]
  compiled code at target/run/pipeline/models/staging/stg__player_games.sql
[0m21:46:28.140547 [info ] [MainThread]: 
[0m21:46:28.140882 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m21:46:28.141505 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.577203, "process_in_blocks": "592", "process_kernel_time": 0.346629, "process_mem_max_rss": "233872", "process_out_blocks": "3216", "process_user_time": 3.69024}
[0m21:46:28.141848 [debug] [MainThread]: Command `dbt run` failed at 21:46:28.141772 after 8.58 seconds
[0m21:46:28.142147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3db30410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c41617d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c41617e50>]}
[0m21:46:28.142430 [debug] [MainThread]: Flushing usage events
[0m21:46:28.518997 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:47:32.512134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd13cf90410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd13cfea290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd13cfca390>]}


============================== 21:47:32.515346 | d241738f-4355-4b78-bbdc-45846dc8f253 ==============================
[0m21:47:32.515346 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:47:32.515802 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m21:47:33.118335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1091b6ed0>]}
[0m21:47:33.226858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd13f9cd5d0>]}
[0m21:47:33.227405 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:47:33.343094 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:47:33.447468 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:47:33.448024 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m21:47:33.681791 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.pipeline.aggregate
[0m21:47:33.693463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108d20ed0>]}
[0m21:47:33.760777 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:47:33.762855 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:47:33.771177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108d87a90>]}
[0m21:47:33.771614 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:47:33.771893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108c94110>]}
[0m21:47:33.773327 [info ] [MainThread]: 
[0m21:47:33.773617 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:47:33.773853 [info ] [MainThread]: 
[0m21:47:33.774236 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:47:33.777970 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:47:33.778878 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:47:33.779379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:47:33.780172 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:47:33.780638 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:47:33.781310 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:47:34.972117 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m21:47:34.972973 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev)
[0m21:47:34.973778 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m21:47:34.974377 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:47:34.974951 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:47:34.975451 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:47:35.134398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108c59a90>]}
[0m21:47:35.135102 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:47:35.138328 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m21:47:35.139228 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m21:47:35.139935 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m21:47:35.140492 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m21:47:35.153888 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m21:47:35.154786 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m21:47:35.167852 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:47:35.355627 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m21:47:35.356565 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_1st_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_2nd_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_1st_previous_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_2nd_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)) AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE() AS flag_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AS flag_prev_12_months

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m21:47:35.743654 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:60efab62-1ed6-4e4f-8099-0dc984b101d2&page=queryresults
[0m21:47:37.850448 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108bd4910>]}
[0m21:47:37.851623 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.71s]
[0m21:47:37.852693 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m21:47:37.853865 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m21:47:37.854676 [info ] [Thread-3 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m21:47:37.855362 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev, now model.pipeline.stg__player_games)
[0m21:47:37.855926 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m21:47:37.863788 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m21:47:37.864757 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m21:47:37.924232 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m21:47:37.925020 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    partition by game_date
    

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id  
      , game_date
      , "white"                                                AS piece_color
      , white.username                                         AS username
      , white.rating                                           AS rating
      , white.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.white                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT  
        game_id  
      , game_date
      , "black"                                                AS piece_color
      , black.username                                         AS username
      , black.rating                                           AS rating
      , black.result                                           AS result
      , rated
      , time_class
      , time_control
      , rules
      , accuracies.black                                       AS accuracy
      , opening                                                AS opening_line
    FROM `checkmate-453316`.`chess_raw`.`games`
  
)

SELECT 
    t.game_id
  , t.game_date
  , cal.month_start_date                                   AS game_month
  , cal.month_year_type1                                   AS game_month_str
  , t.username
  , t.rating
  , t.piece_color
  , t.time_class
  , t.rules
  , t.result                                               AS raw_result
  , t.rated
  , CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw
  , t.opening_line                                        AS opening_line
  , TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening
  , t.accuracy
FROM cte_white_black_union t
LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
    ON t.game_date = cal.cal_date

WHERE 1=1 
  
  

    );
  
[0m21:47:37.925531 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m21:47:38.451508 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7169353f-280a-4931-a9d4-160c41a886e7&page=queryresults
[0m21:47:48.183331 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108b81b90>]}
[0m21:47:48.184246 [info ] [Thread-3 (]: 2 of 3 OK created sql incremental model dev_staging.stg__player_games .......... [[32mCREATE TABLE (4.1m rows, 262.9 MiB processed)[0m in 10.33s]
[0m21:47:48.184889 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m21:47:48.185895 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_players
[0m21:47:48.186830 [info ] [Thread-2 (]: 3 of 3 START sql incremental model dev.weekly_players .......................... [RUN]
[0m21:47:48.187517 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_players)
[0m21:47:48.188077 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_players
[0m21:47:48.197280 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_players"
[0m21:47:48.198303 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_players
[0m21:47:48.203174 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.weekly_players"
[0m21:47:48.204467 [debug] [Thread-2 (]: On model.pipeline.weekly_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_players"} */

  
    

    create or replace table `checkmate-453316`.`dev`.`weekly_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date
      , wagg.week_number
      , wagg.time_class
      , map.opening_archetype
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count


    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date
    , week_number
    , time_class
    , opening_archetype
    , total_games
    , white_win_count
    , white_loss_count
    , white_draw_count
    , black_win_count
    , black_loss_count
    , black_draw_count
FROM cte_pivot_piece_color
ORDER BY
    week_start_date DESC
  , total_games     DESC
    );
  
[0m21:47:48.205372 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m21:47:48.675169 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:b788f9af-a58e-4ef0-9095-4c03fccc6dee&page=queryresults
[0m21:47:48.762007 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:b788f9af-a58e-4ef0-9095-4c03fccc6dee&page=queryresults
[0m21:47:48.769342 [debug] [Thread-2 (]: Database Error in model weekly_players (models/marts/chess/weekly_players.sql)
  Unrecognized name: username at [9:16]
  compiled code at target/run/pipeline/models/marts/chess/weekly_players.sql
[0m21:47:48.770091 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd241738f-4355-4b78-bbdc-45846dc8f253', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd100e6d4d0>]}
[0m21:47:48.771009 [error] [Thread-2 (]: 3 of 3 ERROR creating sql incremental model dev.weekly_players ................. [[31mERROR[0m in 0.58s]
[0m21:47:48.771912 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_players
[0m21:47:48.772761 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_players (models/marts/chess/weekly_players.sql)
  Unrecognized name: username at [9:16]
  compiled code at target/run/pipeline/models/marts/chess/weekly_players.sql.
[0m21:47:48.775387 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:47:48.776293 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:47:48.776765 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m21:47:48.777227 [debug] [MainThread]: Connection 'model.pipeline.weekly_players' was properly closed.
[0m21:47:48.777670 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m21:47:48.778222 [info ] [MainThread]: 
[0m21:47:48.778739 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 15.00 seconds (15.00s).
[0m21:47:48.780369 [debug] [MainThread]: Command end result
[0m21:47:48.819677 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:47:48.821678 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:47:48.828858 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:47:48.829120 [info ] [MainThread]: 
[0m21:47:48.829415 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:47:48.829701 [info ] [MainThread]: 
[0m21:47:48.830040 [error] [MainThread]:   Database Error in model weekly_players (models/marts/chess/weekly_players.sql)
  Unrecognized name: username at [9:16]
  compiled code at target/run/pipeline/models/marts/chess/weekly_players.sql
[0m21:47:48.830311 [info ] [MainThread]: 
[0m21:47:48.830595 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m21:47:48.831155 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 16.36367, "process_in_blocks": "0", "process_kernel_time": 0.33097, "process_mem_max_rss": "234412", "process_out_blocks": "3256", "process_user_time": 3.643762}
[0m21:47:48.831530 [debug] [MainThread]: Command `dbt run` failed at 21:47:48.831443 after 16.36 seconds
[0m21:47:48.831854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd13cfc9610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd100dca450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd108dc8290>]}
[0m21:47:48.832168 [debug] [MainThread]: Flushing usage events
[0m21:47:49.188935 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:54:18.939620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012e697dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012e8d8090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012e694310>]}


============================== 21:54:18.942444 | f06caa44-5233-411d-8630-e464e5e3e9f5 ==============================
[0m21:54:18.942444 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:54:18.942910 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run -s weekly_chess_players', 'send_anonymous_usage_stats': 'True'}
[0m21:54:19.566868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00f6e44450>]}
[0m21:54:19.688354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01310dd6d0>]}
[0m21:54:19.688955 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:54:19.809190 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:54:19.887862 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m21:54:19.888290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012edbe210>]}
[0m21:54:20.815811 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m21:54:20.952851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00f68e4dd0>]}
[0m21:54:21.015254 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:54:21.017305 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:54:21.025607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00f605e610>]}
[0m21:54:21.026018 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:54:21.026314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00f638fa90>]}
[0m21:54:21.027503 [info ] [MainThread]: 
[0m21:54:21.027817 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:54:21.028064 [info ] [MainThread]: 
[0m21:54:21.028448 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:54:21.029465 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:54:21.030114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:54:22.036397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now create_checkmate-453316_dev_marts)
[0m21:54:22.037024 [debug] [ThreadPool]: Creating schema "database: "checkmate-453316"
schema: "dev_marts"
"
[0m21:54:22.049577 [debug] [ThreadPool]: On create_checkmate-453316_dev_marts: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "connection_name": "create_checkmate-453316_dev_marts"} */
create schema if not exists `checkmate-453316`.`dev_marts`
  
[0m21:54:22.050207 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:54:22.436473 [debug] [ThreadPool]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:4da083a6-9741-4b45-bb47-79ebf82a6752&page=queryresults
[0m21:54:25.236759 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_checkmate-453316_dev_marts, now list_checkmate-453316_dev_marts)
[0m21:54:25.237415 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m21:54:25.237792 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:54:25.238495 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m21:54:25.238977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:54:25.240427 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:54:25.640813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00f63038d0>]}
[0m21:54:25.641575 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:54:25.645003 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m21:54:25.645897 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m21:54:25.646608 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_players)
[0m21:54:25.647184 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m21:54:25.661845 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m21:54:25.662684 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m21:54:25.726199 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m21:54:25.726971 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date
      , wagg.week_number
      , wagg.time_class
      , map.opening_archetype
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count


    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date
    , week_number
    , time_class
    , opening_archetype
    , total_games
    , white_win_count
    , white_loss_count
    , white_draw_count
    , black_win_count
    , black_loss_count
    , black_draw_count
FROM cte_pivot_piece_color
ORDER BY
    week_start_date DESC
  , total_games     DESC
    );
  
[0m21:54:25.727495 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:54:26.779821 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d856a853-60c5-4154-830b-777b54eed0aa&page=queryresults
[0m21:54:26.880012 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d856a853-60c5-4154-830b-777b54eed0aa&page=queryresults
[0m21:54:26.887234 [debug] [Thread-1 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Unrecognized name: username at [9:16]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m21:54:26.889925 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f06caa44-5233-411d-8630-e464e5e3e9f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f00f645e590>]}
[0m21:54:26.891005 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 1.24s]
[0m21:54:26.892010 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m21:54:26.892952 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Unrecognized name: username at [9:16]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m21:54:26.895681 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:54:26.896654 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:54:26.897114 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m21:54:26.897589 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m21:54:26.898014 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m21:54:26.898483 [info ] [MainThread]: 
[0m21:54:26.899020 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 5.87 seconds (5.87s).
[0m21:54:26.900022 [debug] [MainThread]: Command end result
[0m21:54:26.934172 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:54:26.936118 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:54:26.945048 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:54:26.945513 [info ] [MainThread]: 
[0m21:54:26.945971 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:54:26.946338 [info ] [MainThread]: 
[0m21:54:26.946795 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Unrecognized name: username at [9:16]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m21:54:26.947152 [info ] [MainThread]: 
[0m21:54:26.947537 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:54:26.948148 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 8.054936, "process_in_blocks": "0", "process_kernel_time": 0.311097, "process_mem_max_rss": "237704", "process_out_blocks": "3144", "process_user_time": 4.353249}
[0m21:54:26.948506 [debug] [MainThread]: Command `dbt run` failed at 21:54:26.948427 after 8.06 seconds
[0m21:54:26.948829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012e68c290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012e697610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f012e696f50>]}
[0m21:54:26.949133 [debug] [MainThread]: Flushing usage events
[0m21:54:27.309077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:56:12.031121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfce58add0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfce7d2090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfce8e84d0>]}


============================== 21:56:12.034272 | 5a4093f7-bf72-42b0-b7c4-935c8d132c04 ==============================
[0m21:56:12.034272 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:56:12.034714 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run -s weekly_chess_players', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:56:12.659016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf9671c710>]}
[0m21:56:12.767256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfd0fdd710>]}
[0m21:56:12.767842 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:56:12.888688 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:56:12.984589 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:56:12.985088 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m21:56:13.207917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf96201850>]}
[0m21:56:13.271848 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:56:13.273896 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:56:13.282158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf962465d0>]}
[0m21:56:13.282594 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:56:13.282870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf962aed10>]}
[0m21:56:13.283959 [info ] [MainThread]: 
[0m21:56:13.284231 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:56:13.284465 [info ] [MainThread]: 
[0m21:56:13.284840 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:56:13.285985 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:56:13.286669 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:14.252975 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m21:56:14.253867 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m21:56:14.254382 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:56:14.255229 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m21:56:14.255691 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:14.257738 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:14.452724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf96881210>]}
[0m21:56:14.453416 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:56:14.456798 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m21:56:14.457475 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m21:56:14.457955 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_players)
[0m21:56:14.458273 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m21:56:14.466832 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m21:56:14.467380 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m21:56:14.530426 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m21:56:14.531197 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date
      , wagg.week_number
      , wagg.username
      , wagg.time_class
      , map.opening_archetype
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games /*Black and White Data Unioned -- only counting one piece color*/

      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count
      , SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count

      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count
      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count
      , SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date
    , week_number
    , username
    , time_class
    , opening_archetype
    , total_games
    , white_win_count
    , white_loss_count
    , white_draw_count
    , black_win_count
    , black_loss_count
    , black_draw_count
FROM cte_pivot_piece_color
ORDER BY
    week_start_date DESC
  , username        DESC
    );
  
[0m21:56:14.531688 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:56:15.043694 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:53282a9e-7113-4772-a995-9a5a5c5d8ff0&page=queryresults
[0m21:56:15.162156 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:53282a9e-7113-4772-a995-9a5a5c5d8ff0&page=queryresults
[0m21:56:15.169249 [debug] [Thread-1 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m21:56:15.171681 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5a4093f7-bf72-42b0-b7c4-935c8d132c04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf96184c10>]}
[0m21:56:15.172830 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 0.71s]
[0m21:56:15.173763 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m21:56:15.174624 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m21:56:15.177779 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:56:15.178719 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:56:15.179175 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m21:56:15.179626 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m21:56:15.180053 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m21:56:15.180519 [info ] [MainThread]: 
[0m21:56:15.181022 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.90 seconds (1.90s).
[0m21:56:15.182005 [debug] [MainThread]: Command end result
[0m21:56:15.218940 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:56:15.220876 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:56:15.228908 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:56:15.229263 [info ] [MainThread]: 
[0m21:56:15.229679 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:56:15.229997 [info ] [MainThread]: 
[0m21:56:15.230302 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m21:56:15.230550 [info ] [MainThread]: 
[0m21:56:15.230802 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:56:15.231307 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.2427955, "process_in_blocks": "0", "process_kernel_time": 0.32385, "process_mem_max_rss": "228476", "process_out_blocks": "3152", "process_user_time": 3.431597}
[0m21:56:15.231663 [debug] [MainThread]: Command `dbt run` failed at 21:56:15.231589 after 3.24 seconds
[0m21:56:15.231961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfce577e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfce577c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf867be4d0>]}
[0m21:56:15.232245 [debug] [MainThread]: Flushing usage events
[0m21:56:15.608974 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:58:48.217378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ebbf30d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ebe953610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ebc263290>]}


============================== 21:58:48.220540 | 451ea111-da53-4da5-b77c-a1e38359d3b7 ==============================
[0m21:58:48.220540 [info ] [MainThread]: Running with dbt=1.9.4
[0m21:58:48.221000 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run -s weekly_chess_players', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m21:58:48.874760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e88110d50>]}
[0m21:58:48.995386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ebe9757d0>]}
[0m21:58:48.995950 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m21:58:49.125598 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m21:58:49.234237 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:58:49.234795 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m21:58:49.481163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e83e1b150>]}
[0m21:58:49.548885 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:58:49.551002 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:58:49.559306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e83ba8e90>]}
[0m21:58:49.559736 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m21:58:49.560009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e83b694d0>]}
[0m21:58:49.561082 [info ] [MainThread]: 
[0m21:58:49.561354 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:58:49.561611 [info ] [MainThread]: 
[0m21:58:49.561981 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m21:58:49.563074 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m21:58:49.563717 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:58:50.541044 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m21:58:50.541942 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m21:58:50.542592 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:58:50.543328 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m21:58:50.543919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:58:50.546091 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:58:50.710597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e83c74c10>]}
[0m21:58:50.711348 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:58:50.714780 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m21:58:50.715292 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m21:58:50.715801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_players)
[0m21:58:50.716184 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m21:58:50.725456 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m21:58:50.725972 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m21:58:50.789110 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m21:58:50.789856 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        map.opening_archetype,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games, /*Black and White Data Unioned -- only counting one piece color*/

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
FROM cte_pivot_piece_color
ORDER BY
    username DESC
    );
  
[0m21:58:50.790364 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:51.261617 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:975377df-e19c-4a15-aa81-575e69df8c59&page=queryresults
[0m21:58:51.375746 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:975377df-e19c-4a15-aa81-575e69df8c59&page=queryresults
[0m21:58:51.381420 [debug] [Thread-1 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m21:58:51.383023 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '451ea111-da53-4da5-b77c-a1e38359d3b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e83b5cd10>]}
[0m21:58:51.383559 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 0.67s]
[0m21:58:51.384041 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m21:58:51.384686 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m21:58:51.387584 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:58:51.388520 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:58:51.388979 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m21:58:51.389407 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m21:58:51.389839 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m21:58:51.390299 [info ] [MainThread]: 
[0m21:58:51.390812 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.83 seconds (1.83s).
[0m21:58:51.391858 [debug] [MainThread]: Command end result
[0m21:58:51.430279 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m21:58:51.432479 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m21:58:51.440313 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m21:58:51.440602 [info ] [MainThread]: 
[0m21:58:51.440934 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:58:51.441218 [info ] [MainThread]: 
[0m21:58:51.441572 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m21:58:51.441841 [info ] [MainThread]: 
[0m21:58:51.442122 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:58:51.442696 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.2678323, "process_in_blocks": "0", "process_kernel_time": 0.324903, "process_mem_max_rss": "228728", "process_out_blocks": "3152", "process_user_time": 3.472172}
[0m21:58:51.443066 [debug] [MainThread]: Command `dbt run` failed at 21:58:51.442984 after 3.27 seconds
[0m21:58:51.443397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ebcc57650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ebbf912d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e83c96610>]}
[0m21:58:51.443721 [debug] [MainThread]: Flushing usage events
[0m21:58:51.818131 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:01:54.184145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec3983910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec3bc9a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec39d2290>]}


============================== 22:01:54.186729 | 5ea8fd18-828a-4ded-9064-cea7416976fc ==============================
[0m22:01:54.186729 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:01:54.187208 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run -s weekly_chess_players', 'send_anonymous_usage_stats': 'True'}
[0m22:01:54.828183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e8bb56e10>]}
[0m22:01:54.949877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec6375790>]}
[0m22:01:54.950477 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:01:55.079002 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:01:55.184384 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:01:55.184939 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m22:01:55.435020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e8b7b1210>]}
[0m22:01:55.506324 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:01:55.508646 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:01:55.517720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e8b67c410>]}
[0m22:01:55.518176 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:01:55.518483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e8b69acd0>]}
[0m22:01:55.519665 [info ] [MainThread]: 
[0m22:01:55.519967 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:01:55.520232 [info ] [MainThread]: 
[0m22:01:55.520643 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:01:55.521831 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:01:55.522493 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:01:56.438052 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m22:01:56.438919 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m22:01:56.439574 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:01:56.440354 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m22:01:56.440825 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:01:56.442930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:01:56.630348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4e8b75a2d0>]}
[0m22:01:56.631148 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:01:56.634645 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:01:56.635456 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:01:56.636122 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_players)
[0m22:01:56.636680 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:01:56.651198 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:01:56.652085 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:01:56.715987 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:01:56.716733 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        map.opening_archetype,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games, /*Black and White Data Unioned -- only counting one piece color*/

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
FROM cte_pivot_piece_color
ORDER BY
    username DESC
    );
  
[0m22:01:56.717240 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:01:57.111549 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c5ec2a1f-23fd-4ede-a3c9-9600fb738a96&page=queryresults
[0m22:01:57.222101 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c5ec2a1f-23fd-4ede-a3c9-9600fb738a96&page=queryresults
[0m22:01:57.229266 [debug] [Thread-1 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:01:57.231588 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ea8fd18-828a-4ded-9064-cea7416976fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec555b7d0>]}
[0m22:01:57.232582 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 0.59s]
[0m22:01:57.233507 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:01:57.234381 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m22:01:57.237479 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:01:57.238394 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:01:57.238860 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m22:01:57.239300 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:01:57.239757 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m22:01:57.240229 [info ] [MainThread]: 
[0m22:01:57.240769 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.72 seconds (1.72s).
[0m22:01:57.241815 [debug] [MainThread]: Command end result
[0m22:01:57.281226 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:01:57.283420 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:01:57.291286 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:01:57.291572 [info ] [MainThread]: 
[0m22:01:57.291906 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:01:57.292187 [info ] [MainThread]: 
[0m22:01:57.292520 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Result of ORDER BY queries cannot be partitioned by field 'week_start_date'
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:01:57.292798 [info ] [MainThread]: 
[0m22:01:57.293077 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:01:57.293653 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.1567116, "process_in_blocks": "0", "process_kernel_time": 0.276134, "process_mem_max_rss": "228704", "process_out_blocks": "3152", "process_user_time": 3.562825}
[0m22:01:57.294015 [debug] [MainThread]: Command `dbt run` failed at 22:01:57.293937 after 3.16 seconds
[0m22:01:57.294329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec3978150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec39da250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4ec3983510>]}
[0m22:01:57.294648 [debug] [MainThread]: Flushing usage events
[0m22:01:57.709887 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:02:08.348833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b8c36350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b5c88bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b5ed9a10>]}


============================== 22:02:08.351882 | 355a7c3f-4f10-46af-9efc-4cee83ad0b6e ==============================
[0m22:02:08.351882 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:02:08.352321 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt clean', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:02:08.443918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '355a7c3f-4f10-46af-9efc-4cee83ad0b6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b5ce7950>]}
[0m22:02:08.452572 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.1508688, "process_in_blocks": "0", "process_kernel_time": 0.152652, "process_mem_max_rss": "102488", "process_out_blocks": "0", "process_user_time": 1.207261}
[0m22:02:08.452974 [debug] [MainThread]: Command `dbt clean` succeeded at 22:02:08.452891 after 0.15 seconds
[0m22:02:08.453291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b5ec6c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b971bc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f25b967bd50>]}
[0m22:02:08.453631 [debug] [MainThread]: Flushing usage events
[0m22:02:08.797305 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:06:04.128962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b1e94bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b1e944d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b1e97e10>]}


============================== 22:06:04.132223 | 8228910e-15c0-4030-aa1a-8fd68d75d972 ==============================
[0m22:06:04.132223 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:06:04.132717 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s weekly_chess_players', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:06:04.785504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe37a653f90>]}
[0m22:06:04.905308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b48cd6d0>]}
[0m22:06:04.905857 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:06:05.027355 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:06:05.027926 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m22:06:05.028227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3aff8cf10>]}
[0m22:06:06.053911 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m22:06:06.204741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe379d0c1d0>]}
[0m22:06:06.278169 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:06:06.280385 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:06:06.290070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3798f7150>]}
[0m22:06:06.290539 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:06:06.290846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe379a10fd0>]}
[0m22:06:06.292040 [info ] [MainThread]: 
[0m22:06:06.292341 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:06:06.292615 [info ] [MainThread]: 
[0m22:06:06.293034 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:06:06.294230 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:06:06.294922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:06:07.252478 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m22:06:07.253604 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m22:06:07.254162 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:06:07.254746 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m22:06:07.255213 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:06:07.256677 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:06:07.427289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe379d13210>]}
[0m22:06:07.428000 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:06:07.431350 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:06:07.432224 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:06:07.432908 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_players)
[0m22:06:07.433467 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:06:07.448771 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:06:07.449787 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:06:07.517987 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:06:07.518889 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        map.opening_archetype,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games, /*Black and White Data Unioned -- only counting one piece color*/

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
FROM cte_pivot_piece_color
    );
  
[0m22:06:07.519422 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:06:08.009651 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d3f9392f-1b62-4549-bdb0-513a1796b0ed&page=queryresults
[0m22:06:13.279153 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8228910e-15c0-4030-aa1a-8fd68d75d972', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe379ac2250>]}
[0m22:06:13.279769 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_marts.weekly_chess_players ......... [[32mCREATE TABLE (1.3m rows, 316.6 MiB processed)[0m in 5.85s]
[0m22:06:13.280280 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:06:13.281518 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:06:13.282059 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:06:13.282298 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m22:06:13.282532 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m22:06:13.282758 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:06:13.283006 [info ] [MainThread]: 
[0m22:06:13.283269 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.99 seconds (6.99s).
[0m22:06:13.283800 [debug] [MainThread]: Command end result
[0m22:06:13.302438 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:06:13.303823 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:06:13.308248 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:06:13.308502 [info ] [MainThread]: 
[0m22:06:13.308826 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:06:13.309065 [info ] [MainThread]: 
[0m22:06:13.309324 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:06:13.309872 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.234448, "process_in_blocks": "0", "process_kernel_time": 0.321186, "process_mem_max_rss": "236700", "process_out_blocks": "3144", "process_user_time": 4.422742}
[0m22:06:13.310278 [debug] [MainThread]: Command `dbt run` succeeded at 22:06:13.310188 after 9.23 seconds
[0m22:06:13.310604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b1e8c210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b1eee4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3b1e83f10>]}
[0m22:06:13.310934 [debug] [MainThread]: Flushing usage events
[0m22:06:13.720604 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:07:10.074233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd40ce90710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd40ce917d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd40ce90610>]}


============================== 22:07:10.077318 | fc2074ad-96f0-4c8c-8ba8-f509cc7a960c ==============================
[0m22:07:10.077318 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:07:10.077770 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m22:07:10.725290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3db8da090>]}
[0m22:07:10.834995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d90bf210>]}
[0m22:07:10.835555 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:07:10.955931 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:07:11.053791 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:07:11.054151 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:07:11.092216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d8ccc7d0>]}
[0m22:07:11.158724 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:07:11.160981 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:07:11.169473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d8ccd710>]}
[0m22:07:11.169933 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:07:11.170220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d8daec10>]}
[0m22:07:11.171687 [info ] [MainThread]: 
[0m22:07:11.172003 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:07:11.172246 [info ] [MainThread]: 
[0m22:07:11.172661 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:07:11.176668 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:07:11.177512 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:07:11.177968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:07:11.178320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:07:11.178773 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:07:11.180459 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:07:12.285134 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m22:07:12.285888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m22:07:12.286339 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:07:12.286993 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m22:07:12.287626 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:07:12.289344 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:07:12.471848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d8c98290>]}
[0m22:07:12.472581 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:07:12.475901 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m22:07:12.476574 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m22:07:12.477075 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m22:07:12.477490 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m22:07:12.486664 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m22:07:12.487246 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m22:07:12.501412 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:07:12.696000 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m22:07:12.696884 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_1st_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_2nd_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_1st_previous_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_2nd_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)) AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE() AS flag_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AS flag_prev_12_months

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m22:07:13.078555 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f89a82be-c077-4e85-ba22-bbd6c7f7c049&page=queryresults
[0m22:07:14.972726 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d8aeaf10>]}
[0m22:07:14.973462 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.49s]
[0m22:07:14.974110 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m22:07:14.975064 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m22:07:14.975599 [info ] [Thread-3 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m22:07:14.976047 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m22:07:14.976394 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m22:07:14.985609 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m22:07:14.986197 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m22:07:15.037253 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:07:15.240965 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m22:07:15.241848 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    cal.month_start_date                                   AS game_month,
    cal.month_year_type1                                   AS game_month_str,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t
LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
    ON t.game_date = cal.cal_date

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `game_month`, `game_month_str`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m22:07:15.380618 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:682410a6-271d-4423-8974-88e395e83da0&page=queryresults
[0m22:07:22.595017 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d8da3910>]}
[0m22:07:22.596029 [info ] [Thread-3 (]: 2 of 3 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (10.0 MiB processed)[0m in 7.62s]
[0m22:07:22.596982 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m22:07:22.598170 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_chess_players
[0m22:07:22.598920 [info ] [Thread-2 (]: 3 of 3 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:07:22.599589 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_players)
[0m22:07:22.600119 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:07:22.605603 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:07:22.606089 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:07:22.608280 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:07:23.295646 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:07:23.297201 [debug] [Thread-2 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players__dbt_tmp`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 21 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        map.opening_archetype,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.total      ELSE 0 END)   AS total_games, /*Black and White Data Unioned -- only counting one piece color*/

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`weekly_chess_players__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`weekly_chess_players` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`weekly_chess_players__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `black_win_count`, `black_loss_count`, `black_draw_count`)
    values
        (`week_start_date`, `week_number`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `black_win_count`, `black_loss_count`, `black_draw_count`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`weekly_chess_players__dbt_tmp`

  


  

    
[0m22:07:23.442796 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a07dbf8c-0312-44ea-8cea-7864de892864&page=queryresults
[0m22:07:30.966985 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc2074ad-96f0-4c8c-8ba8-f509cc7a960c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd3d0d17ed0>]}
[0m22:07:30.967814 [info ] [Thread-2 (]: 3 of 3 OK created sql incremental model dev_marts.weekly_chess_players ......... [[32mSCRIPT (5.7 MiB processed)[0m in 8.37s]
[0m22:07:30.968518 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:07:30.970223 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:07:30.971120 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:07:30.971581 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m22:07:30.972011 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:07:30.972432 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m22:07:30.972999 [info ] [MainThread]: 
[0m22:07:30.973494 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 19.80 seconds (19.80s).
[0m22:07:30.974977 [debug] [MainThread]: Command end result
[0m22:07:31.009920 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:07:31.011845 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:07:31.020330 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:07:31.020689 [info ] [MainThread]: 
[0m22:07:31.021087 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:07:31.021404 [info ] [MainThread]: 
[0m22:07:31.021754 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m22:07:31.022403 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 20.995687, "process_in_blocks": "0", "process_kernel_time": 0.339311, "process_mem_max_rss": "229432", "process_out_blocks": "2248", "process_user_time": 3.486626}
[0m22:07:31.022822 [debug] [MainThread]: Command `dbt run` succeeded at 22:07:31.022745 after 21.00 seconds
[0m22:07:31.023104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd40ce8bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd40ce882d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd410877dd0>]}
[0m22:07:31.023389 [debug] [MainThread]: Flushing usage events
[0m22:07:31.408399 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:13:58.367333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47ba94e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47ba8cd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47ba97c90>]}


============================== 22:13:58.370387 | 8adf5504-2f6a-41f5-9d4e-7338404d26aa ==============================
[0m22:13:58.370387 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:13:58.370853 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run -s weekly_chess_players --full-refresh', 'send_anonymous_usage_stats': 'True'}
[0m22:13:59.025944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe448396490>]}
[0m22:13:59.148245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47e4cdb90>]}
[0m22:13:59.148843 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:13:59.267487 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:13:59.375488 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:13:59.376058 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m22:13:59.623974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe443a24710>]}
[0m22:13:59.699122 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:13:59.701568 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:13:59.711452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe443865950>]}
[0m22:13:59.711933 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:13:59.712233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe44386a0d0>]}
[0m22:13:59.713542 [info ] [MainThread]: 
[0m22:13:59.713845 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:13:59.714103 [info ] [MainThread]: 
[0m22:13:59.714511 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:13:59.715578 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:13:59.716247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:14:00.629886 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m22:14:00.630827 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m22:14:00.631400 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:14:00.632268 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m22:14:00.632773 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:14:00.634787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:14:00.814074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4438cbbd0>]}
[0m22:14:00.814851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:14:00.818349 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:14:00.819228 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:14:00.819947 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_players)
[0m22:14:00.820507 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:14:00.838284 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:14:00.838855 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:14:00.875142 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:14:01.085442 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:14:01.086381 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        COALSESCE(map.opening_archetype, "Mapping Failed")                          AS opening_archetype,
        SUM(total_games)                                                            AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
FROM cte_pivot_piece_color
    );
  
[0m22:14:01.410378 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:812301cf-1128-44f4-b00e-d8ee92699808&page=queryresults
[0m22:14:01.504502 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:812301cf-1128-44f4-b00e-d8ee92699808&page=queryresults
[0m22:14:01.511686 [debug] [Thread-1 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Function not found: COALSESCE; Did you mean coalesce? at [52:9]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:14:01.514027 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8adf5504-2f6a-41f5-9d4e-7338404d26aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe443864650>]}
[0m22:14:01.515125 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 0.69s]
[0m22:14:01.516037 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:14:01.516889 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Function not found: COALSESCE; Did you mean coalesce? at [52:9]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m22:14:01.519630 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:14:01.520576 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:14:01.521028 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m22:14:01.521458 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m22:14:01.521897 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:14:01.522367 [info ] [MainThread]: 
[0m22:14:01.522888 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.81 seconds (1.81s).
[0m22:14:01.523879 [debug] [MainThread]: Command end result
[0m22:14:01.559820 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:14:01.561860 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:14:01.571079 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:14:01.571474 [info ] [MainThread]: 
[0m22:14:01.571920 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:14:01.572218 [info ] [MainThread]: 
[0m22:14:01.572557 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Function not found: COALSESCE; Did you mean coalesce? at [52:9]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:14:01.572823 [info ] [MainThread]: 
[0m22:14:01.573099 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:14:01.573662 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.2538629, "process_in_blocks": "0", "process_kernel_time": 0.326028, "process_mem_max_rss": "229028", "process_out_blocks": "3152", "process_user_time": 3.577914}
[0m22:14:01.574029 [debug] [MainThread]: Command `dbt run` failed at 22:14:01.573949 after 3.25 seconds
[0m22:14:01.574351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47ba8c150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47bae6150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe47ba97950>]}
[0m22:14:01.574666 [debug] [MainThread]: Flushing usage events
[0m22:14:02.030940 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:14:28.305924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4488f390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf448e74d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf448e7a10>]}


============================== 22:14:28.308503 | cff2e940-276c-4b33-9838-1c42bb13c63c ==============================
[0m22:14:28.308503 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:14:28.309006 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt run -s weekly_chess_players --full-refresh', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:14:28.946417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf10beea10>]}
[0m22:14:29.055838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf472cd890>]}
[0m22:14:29.056387 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:14:29.175326 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:14:29.279240 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:14:29.279793 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m22:14:29.528440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf10887dd0>]}
[0m22:14:29.599849 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:14:29.601935 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:14:29.610503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf105fcf90>]}
[0m22:14:29.610941 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:14:29.611214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf105aacd0>]}
[0m22:14:29.612303 [info ] [MainThread]: 
[0m22:14:29.612595 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:14:29.612854 [info ] [MainThread]: 
[0m22:14:29.613225 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:14:29.614325 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:14:29.614993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:14:30.538238 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m22:14:30.539219 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m22:14:30.539744 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:14:30.540563 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m22:14:30.541005 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:14:30.542802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:14:30.713453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf105aacd0>]}
[0m22:14:30.714178 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:14:30.717402 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:14:30.718238 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:14:30.718907 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_players)
[0m22:14:30.719469 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:14:30.737884 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:14:30.738450 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:14:30.774247 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:14:30.996004 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:14:30.996802 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                          AS opening_archetype,
        SUM(total_games)                                                            AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
FROM cte_pivot_piece_color
    );
  
[0m22:14:31.308056 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:997d7992-e3c3-4a06-8912-0f2d9e93f1db&page=queryresults
[0m22:14:35.834035 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cff2e940-276c-4b33-9838-1c42bb13c63c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf10a3e590>]}
[0m22:14:35.835098 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_marts.weekly_chess_players ......... [[32mCREATE TABLE (1.3m rows, 316.6 MiB processed)[0m in 5.11s]
[0m22:14:35.836804 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:14:35.838634 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:14:35.839552 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:14:35.839996 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m22:14:35.840418 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m22:14:35.840850 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:14:35.841310 [info ] [MainThread]: 
[0m22:14:35.841812 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.23 seconds (6.23s).
[0m22:14:35.842798 [debug] [MainThread]: Command end result
[0m22:14:35.877780 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:14:35.879883 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:14:35.888203 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:14:35.888577 [info ] [MainThread]: 
[0m22:14:35.888997 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:14:35.889314 [info ] [MainThread]: 
[0m22:14:35.889664 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:14:35.890321 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.6319566, "process_in_blocks": "0", "process_kernel_time": 0.35519, "process_mem_max_rss": "232244", "process_out_blocks": "3152", "process_user_time": 3.592226}
[0m22:14:35.890697 [debug] [MainThread]: Command `dbt run` succeeded at 22:14:35.890620 after 7.63 seconds
[0m22:14:35.890983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4488c410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4488c2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf48277ed0>]}
[0m22:14:35.891269 [debug] [MainThread]: Flushing usage events
[0m22:14:36.252199 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:42:30.222244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4e787ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4e77cb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4e784310>]}


============================== 22:42:30.224845 | 160d1406-de27-49d4-b259-ab66432150ac ==============================
[0m22:42:30.224845 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:42:30.225293 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run --full-refresh', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:42:30.876689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf16a5b410>]}
[0m22:42:30.999500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf511757d0>]}
[0m22:42:31.000102 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:42:31.127631 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:42:31.224994 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:42:31.225505 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m22:42:31.454648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf16540190>]}
[0m22:42:31.523134 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:42:31.525203 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:42:31.533691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf1641a210>]}
[0m22:42:31.534111 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:42:31.534389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf163cbc90>]}
[0m22:42:31.535830 [info ] [MainThread]: 
[0m22:42:31.536114 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:42:31.536353 [info ] [MainThread]: 
[0m22:42:31.536734 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:42:31.540541 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:42:31.541400 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:42:31.542017 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:42:31.542725 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:42:31.543236 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:42:31.544165 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:42:32.524886 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m22:42:32.525629 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m22:42:32.526311 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m22:42:32.526897 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:42:32.527465 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:42:32.528006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:42:32.691291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4ef1ec10>]}
[0m22:42:32.691815 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:42:32.694060 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m22:42:32.694816 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m22:42:32.695453 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m22:42:32.696005 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m22:42:32.709086 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m22:42:32.710044 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m22:42:32.723811 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:42:32.907045 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m22:42:32.908178 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_1st_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_2nd_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_1st_previous_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_2nd_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)) AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE() AS flag_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AS flag_prev_12_months

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m22:42:33.213991 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:4407b6cc-888d-4553-a26a-29910d244bee&page=queryresults
[0m22:42:35.106843 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf06578450>]}
[0m22:42:35.107913 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.41s]
[0m22:42:35.108913 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m22:42:35.110086 [debug] [Thread-3 (]: Began running node model.pipeline.stg__player_games
[0m22:42:35.111105 [info ] [Thread-3 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m22:42:35.111830 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m22:42:35.112397 [debug] [Thread-3 (]: Began compiling node model.pipeline.stg__player_games
[0m22:42:35.123065 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m22:42:35.123964 [debug] [Thread-3 (]: Began executing node model.pipeline.stg__player_games
[0m22:42:35.175426 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m22:42:35.345072 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m22:42:35.346409 [debug] [Thread-3 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    partition by game_date
    

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    cal.month_start_date                                   AS game_month,
    cal.month_year_type1                                   AS game_month_str,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t
LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
    ON t.game_date = cal.cal_date

WHERE 1=1 
  
  

    );
  
[0m22:42:35.848124 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7b1bc535-72a4-4081-82b9-75c5406567fc&page=queryresults
[0m22:42:45.126273 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf06567710>]}
[0m22:42:45.127262 [info ] [Thread-3 (]: 2 of 3 OK created sql incremental model dev_staging.stg__player_games .......... [[32mCREATE TABLE (4.1m rows, 262.9 MiB processed)[0m in 10.01s]
[0m22:42:45.128176 [debug] [Thread-3 (]: Finished running node model.pipeline.stg__player_games
[0m22:42:45.129480 [debug] [Thread-2 (]: Began running node model.pipeline.weekly_chess_players
[0m22:42:45.130368 [info ] [Thread-2 (]: 3 of 3 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:42:45.131080 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_players)
[0m22:42:45.131665 [debug] [Thread-2 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:42:45.138179 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:42:45.138859 [debug] [Thread-2 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:42:45.141646 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m22:42:45.332520 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:42:45.333852 [debug] [Thread-2 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                           AS opening_archetype,
        AVG(wagg.rating)                                                            AS avg_rating,
        SUM(total_games)                                                            AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      black_win_count,
      black_loss_count,
      black_draw_count,
      avg_rating,
FROM cte_pivot_piece_color
    );
  
[0m22:42:45.626828 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e4081b05-f654-40f8-8e25-fc908302d3e3&page=queryresults
[0m22:42:45.717707 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e4081b05-f654-40f8-8e25-fc908302d3e3&page=queryresults
[0m22:42:45.723448 [debug] [Thread-2 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Name rating not found inside wagg at [53:18]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:42:45.723985 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '160d1406-de27-49d4-b259-ab66432150ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf163168d0>]}
[0m22:42:45.724514 [error] [Thread-2 (]: 3 of 3 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 0.59s]
[0m22:42:45.725006 [debug] [Thread-2 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:42:45.725806 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Name rating not found inside wagg at [53:18]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m22:42:45.728708 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:42:45.729509 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:42:45.729978 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m22:42:45.730413 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:42:45.730858 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m22:42:45.731421 [info ] [MainThread]: 
[0m22:42:45.731974 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 14.19 seconds (14.19s).
[0m22:42:45.733463 [debug] [MainThread]: Command end result
[0m22:42:45.767348 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:42:45.769331 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:42:45.778050 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:42:45.778548 [info ] [MainThread]: 
[0m22:42:45.779098 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:42:45.779613 [info ] [MainThread]: 
[0m22:42:45.780092 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Name rating not found inside wagg at [53:18]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:42:45.780424 [info ] [MainThread]: 
[0m22:42:45.780771 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m22:42:45.781419 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 15.606536, "process_in_blocks": "0", "process_kernel_time": 0.363706, "process_mem_max_rss": "233764", "process_out_blocks": "3264", "process_user_time": 3.688979}
[0m22:42:45.781889 [debug] [MainThread]: Command `dbt run` failed at 22:42:45.781784 after 15.61 seconds
[0m22:42:45.782236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4e77c1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf4e7853d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf16a6b290>]}
[0m22:42:45.782517 [debug] [MainThread]: Flushing usage events
[0m22:42:46.145936 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:44:53.019773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea78497390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea7848cb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea784976d0>]}


============================== 22:44:53.022522 | 09b621e3-e1b0-46cc-a2ff-d42eda748cf5 ==============================
[0m22:44:53.022522 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:44:53.022975 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run -s weekly_chess_players --full-refresh', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:44:53.682221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea784e7a10>]}
[0m22:44:53.802645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea7aecd8d0>]}
[0m22:44:53.803194 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:44:53.920333 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:44:54.025290 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:44:54.025802 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m22:44:54.273411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea442e8a50>]}
[0m22:44:54.346103 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:44:54.348196 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:44:54.356633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea4425ae50>]}
[0m22:44:54.357126 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:44:54.357407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea44282690>]}
[0m22:44:54.358503 [info ] [MainThread]: 
[0m22:44:54.358803 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:44:54.359041 [info ] [MainThread]: 
[0m22:44:54.359414 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:44:54.360493 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:44:54.361187 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:44:55.257421 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m22:44:55.257926 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:44:55.258507 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m22:44:55.260139 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m22:44:55.260442 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:44:55.260920 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:44:55.428094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea441c6f10>]}
[0m22:44:55.428819 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:44:55.432094 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:44:55.432961 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:44:55.433640 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.weekly_chess_players)
[0m22:44:55.434197 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:44:55.449207 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:44:55.450113 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:44:55.513623 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:44:55.514382 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                           AS opening_archetype,
        AVG(wagg.avg_rating)                                                        AS avg_rating,
        SUM(total_games)                                                            AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)   AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)   AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)   AS white_draw_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)   AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)   AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)   AS black_draw_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m22:44:55.514890 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:44:55.990373 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:04991a93-aa14-4865-a55d-adfb2d944d68&page=queryresults
[0m22:44:56.096709 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:04991a93-aa14-4865-a55d-adfb2d944d68&page=queryresults
[0m22:44:56.103840 [debug] [Thread-1 (]: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Name accuracy not found inside wagg at [59:60]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:44:56.106184 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '09b621e3-e1b0-46cc-a2ff-d42eda748cf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea442694d0>]}
[0m22:44:56.107211 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_players ..... [[31mERROR[0m in 0.67s]
[0m22:44:56.108134 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:44:56.109022 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_players' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Name accuracy not found inside wagg at [59:60]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql.
[0m22:44:56.111676 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:44:56.112583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:44:56.113059 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m22:44:56.113494 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m22:44:56.113935 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:44:56.114400 [info ] [MainThread]: 
[0m22:44:56.114899 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.76 seconds (1.76s).
[0m22:44:56.115935 [debug] [MainThread]: Command end result
[0m22:44:56.151371 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:44:56.153448 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:44:56.162026 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:44:56.162514 [info ] [MainThread]: 
[0m22:44:56.162947 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:44:56.163277 [info ] [MainThread]: 
[0m22:44:56.163677 [error] [MainThread]:   Database Error in model weekly_chess_players (models/marts/weekly_chess_players.sql)
  Name accuracy not found inside wagg at [59:60]
  compiled code at target/run/pipeline/models/marts/weekly_chess_players.sql
[0m22:44:56.163937 [info ] [MainThread]: 
[0m22:44:56.164192 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m22:44:56.164713 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.1923726, "process_in_blocks": "0", "process_kernel_time": 0.315988, "process_mem_max_rss": "228472", "process_out_blocks": "3160", "process_user_time": 3.549769}
[0m22:44:56.165053 [debug] [MainThread]: Command `dbt run` failed at 22:44:56.164977 after 3.19 seconds
[0m22:44:56.165347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea7848c1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea3c6fed10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea3c6fe3d0>]}
[0m22:44:56.165643 [debug] [MainThread]: Flushing usage events
[0m22:44:56.528586 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:45:42.814473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aeea97450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aeeae7810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aeea97e50>]}


============================== 22:45:42.817864 | 45a333c0-fb39-4934-8930-f80699656637 ==============================
[0m22:45:42.817864 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:45:42.818353 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run -s weekly_chess_players --full-refresh', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:45:43.481068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab6c068d0>]}
[0m22:45:43.604631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af1484b10>]}
[0m22:45:43.605238 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:45:43.727454 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:45:43.832108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:45:43.832625 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m22:45:44.081366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab669f590>]}
[0m22:45:44.153559 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:45:44.155822 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:45:44.165004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab6590090>]}
[0m22:45:44.165461 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:45:44.165773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab6a38250>]}
[0m22:45:44.166975 [info ] [MainThread]: 
[0m22:45:44.167288 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:45:44.167557 [info ] [MainThread]: 
[0m22:45:44.167978 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:45:44.169180 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:45:44.169858 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:45:45.096430 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m22:45:45.097348 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m22:45:45.097877 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:45:45.098723 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m22:45:45.099271 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:45:45.101614 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:45:45.251646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab66baa10>]}
[0m22:45:45.252340 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:45:45.255641 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:45:45.256483 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:45:45.257180 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_players)
[0m22:45:45.257764 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:45:45.272764 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:45:45.273681 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:45:45.338228 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:45:45.339012 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.rules,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                               AS opening_archetype,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(total_games)                                                                AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m22:45:45.339534 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:45:45.880442 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:74328bcb-c46c-4b9e-b336-029f9b191342&page=queryresults
[0m22:45:50.798199 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45a333c0-fb39-4934-8930-f80699656637', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab665b750>]}
[0m22:45:50.798941 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_marts.weekly_chess_players ......... [[32mCREATE TABLE (1.3m rows, 354.5 MiB processed)[0m in 5.54s]
[0m22:45:50.799496 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:45:50.801212 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:45:50.802292 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:45:50.802808 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:45:50.803286 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m22:45:50.803772 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m22:45:50.804281 [info ] [MainThread]: 
[0m22:45:50.804854 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.64 seconds (6.64s).
[0m22:45:50.805924 [debug] [MainThread]: Command end result
[0m22:45:50.844283 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:45:50.847189 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:45:50.854943 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:45:50.855259 [info ] [MainThread]: 
[0m22:45:50.855631 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:45:50.855885 [info ] [MainThread]: 
[0m22:45:50.856169 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:45:50.856756 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.088536, "process_in_blocks": "0", "process_kernel_time": 0.325438, "process_mem_max_rss": "231596", "process_out_blocks": "3160", "process_user_time": 3.56988}
[0m22:45:50.857133 [debug] [MainThread]: Command `dbt run` succeeded at 22:45:50.857049 after 8.09 seconds
[0m22:45:50.857451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aeeae7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3aeea8c250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3ab408e350>]}
[0m22:45:50.857779 [debug] [MainThread]: Flushing usage events
[0m22:45:51.213813 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:51:15.953682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc11387890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc115c9c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc116e87d0>]}


============================== 22:51:15.956839 | 492353f1-ee1b-4797-a9d8-55036e9b464d ==============================
[0m22:51:15.956839 [info ] [MainThread]: Running with dbt=1.9.4
[0m22:51:15.957317 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run -s weekly_chess_players --full-refresh', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:51:16.638365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc113bba50>]}
[0m22:51:16.766343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc13dcd950>]}
[0m22:51:16.766974 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m22:51:16.890111 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m22:51:16.989465 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:51:16.990055 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_players.sql
[0m22:51:17.255183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbdd15b450>]}
[0m22:51:17.332618 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:51:17.334969 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:51:17.344963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbdd0859d0>]}
[0m22:51:17.345437 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m22:51:17.345751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbdd35bc10>]}
[0m22:51:17.346961 [info ] [MainThread]: 
[0m22:51:17.347262 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m22:51:17.347521 [info ] [MainThread]: 
[0m22:51:17.347948 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m22:51:17.349125 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m22:51:17.349731 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:18.309310 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m22:51:18.310235 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m22:51:18.310763 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:51:18.311709 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m22:51:18.312176 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:18.314365 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:18.479704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbdd67d350>]}
[0m22:51:18.480420 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:51:18.483739 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_players
[0m22:51:18.484653 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_players .............. [RUN]
[0m22:51:18.485353 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.weekly_chess_players)
[0m22:51:18.485969 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_players
[0m22:51:18.505240 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_players"
[0m22:51:18.505820 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_players
[0m22:51:18.546339 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:51:18.748074 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_players"
[0m22:51:18.748905 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_players: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_players"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_players`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_weekly_aggregate AS (
      SELECT
            cal.week_start_date,
            cal.week_number_type1                                     AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            t.opening,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT 
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                               AS opening_archetype,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(total_games)                                                                AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_weekly_aggregate wagg
    LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
        ON map.opening = wagg.opening
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m22:51:19.209450 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:8aab9fbd-9a29-4c22-9233-471021b9e559&page=queryresults
[0m22:51:23.827112 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '492353f1-ee1b-4797-a9d8-55036e9b464d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdbdcf9f7d0>]}
[0m22:51:23.828211 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_marts.weekly_chess_players ......... [[32mCREATE TABLE (1.3m rows, 354.5 MiB processed)[0m in 5.34s]
[0m22:51:23.829153 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_players
[0m22:51:23.831016 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m22:51:23.831965 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:51:23.832432 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_players' was properly closed.
[0m22:51:23.832886 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m22:51:23.833311 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m22:51:23.833792 [info ] [MainThread]: 
[0m22:51:23.834282 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.49 seconds (6.49s).
[0m22:51:23.835262 [debug] [MainThread]: Command end result
[0m22:51:23.871995 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m22:51:23.874031 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m22:51:23.882723 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m22:51:23.883100 [info ] [MainThread]: 
[0m22:51:23.883447 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:51:23.883717 [info ] [MainThread]: 
[0m22:51:23.883999 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m22:51:23.884590 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.975876, "process_in_blocks": "0", "process_kernel_time": 0.342263, "process_mem_max_rss": "232216", "process_out_blocks": "3160", "process_user_time": 3.628239}
[0m22:51:23.884972 [debug] [MainThread]: Command `dbt run` succeeded at 22:51:23.884889 after 7.98 seconds
[0m22:51:23.885303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc14fffe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc11373e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc14e17e10>]}
[0m22:51:23.885739 [debug] [MainThread]: Flushing usage events
[0m22:51:24.262195 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:05:18.827673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb438c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb667ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb679e10>]}


============================== 23:05:18.830273 | 5f69a79f-b043-4178-91a2-3e71a51aa2e2 ==============================
[0m23:05:18.830273 [info ] [MainThread]: Running with dbt=1.9.4
[0m23:05:18.830760 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s calendar', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:05:19.492954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcbbc7c10>]}
[0m23:05:19.613555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcde716d0>]}
[0m23:05:19.614111 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m23:05:19.741479 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m23:05:19.848792 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m23:05:19.849489 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m23:05:19.849812 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m23:05:20.158530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab930e9e90>]}
[0m23:05:20.229585 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m23:05:20.231669 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m23:05:20.240212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab936650d0>]}
[0m23:05:20.240654 [info ] [MainThread]: Found 3 models, 1 seed, 1 source, 492 macros
[0m23:05:20.240938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab93047350>]}
[0m23:05:20.242029 [info ] [MainThread]: 
[0m23:05:20.242325 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m23:05:20.242607 [info ] [MainThread]: 
[0m23:05:20.243045 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m23:05:20.244248 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m23:05:20.244916 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:05:21.189368 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m23:05:21.190049 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m23:05:21.190679 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:05:21.191419 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m23:05:21.191921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:05:21.194216 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:05:21.390934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab93349850>]}
[0m23:05:21.391660 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:05:21.394850 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m23:05:21.395727 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m23:05:21.396378 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m23:05:21.396945 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m23:05:21.410412 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m23:05:21.411432 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m23:05:21.425986 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m23:05:21.639632 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m23:05:21.641368 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY FROM cal_date)                                      AS day_digit
        , EXTRACT(MONTH FROM cal_date)                                    AS month_digit
        , EXTRACT(YEAR FROM cal_date)                                     AS year_digit
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type1
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_type2

          /* Weekly Formats - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_number_type1
        , EXTRACT(ISOWEEK FROM cal_date)                                  AS iso_week_number_type2
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end

          /* Weekly Formats - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_type1
        , EXTRACT(WEEK(SUNDAY)    FROM cal_date)                          AS week_number_type2
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date

        /* Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY) AS flag_current_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_1st_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY) AS flag_2nd_previous_month
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE()) AS flag_current_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_1st_previous_month_last_year
        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) AS flag_2nd_previous_month_last_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE()) AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)) AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR)) AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE() AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AS flag_previous_last_12_months

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m23:05:22.519093 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:5595f237-6ca9-42db-87a7-67d6b0ed35e2&page=queryresults
[0m23:05:24.476131 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f69a79f-b043-4178-91a2-3e71a51aa2e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab92f2a350>]}
[0m23:05:24.477205 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.08s]
[0m23:05:24.478157 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m23:05:24.480236 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m23:05:24.481870 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:05:24.482452 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m23:05:24.482982 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m23:05:24.483479 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m23:05:24.484047 [info ] [MainThread]: 
[0m23:05:24.484542 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.24 seconds (4.24s).
[0m23:05:24.485362 [debug] [MainThread]: Command end result
[0m23:05:24.521471 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m23:05:24.523724 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m23:05:24.531129 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m23:05:24.531453 [info ] [MainThread]: 
[0m23:05:24.531807 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:05:24.532073 [info ] [MainThread]: 
[0m23:05:24.532376 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:05:24.533000 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.7525196, "process_in_blocks": "0", "process_kernel_time": 0.362767, "process_mem_max_rss": "232592", "process_out_blocks": "3184", "process_user_time": 3.621303}
[0m23:05:24.533401 [debug] [MainThread]: Command `dbt run` succeeded at 23:05:24.533315 after 5.75 seconds
[0m23:05:24.533746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb430150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb490810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fabcb790390>]}
[0m23:05:24.534076 [debug] [MainThread]: Flushing usage events
[0m23:05:24.902372 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:56:35.254017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f454808af50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f454808b710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f454808b450>]}


============================== 12:56:35.264617 | e8938d8b-a793-4b06-996e-5b5d407bb153 ==============================
[0m12:56:35.264617 [info ] [MainThread]: Running with dbt=1.9.4
[0m12:56:35.265086 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run -s calendar', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:56:38.040782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45147fb350>]}
[0m12:56:38.162000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f454aadd6d0>]}
[0m12:56:38.162595 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m12:56:38.325870 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m12:56:38.570236 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 4 files changed.
[0m12:56:38.571028 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/quarterly_chess_player_metrics.sql
[0m12:56:38.571910 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m12:56:38.572377 [debug] [MainThread]: Partial parsing: deleted file: pipeline://models/marts/weekly_chess_players.sql
[0m12:56:38.572906 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m12:56:38.573568 [debug] [MainThread]: Partial parsing: updated file: pipeline://macros/sql_macros.sql
[0m12:56:38.574077 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/stg__player_games.sql
[0m12:56:38.908595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f450ff756d0>]}
[0m12:56:38.988126 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m12:56:38.991494 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m12:56:39.020962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f450fdf2f10>]}
[0m12:56:39.021400 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m12:56:39.021707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4548e45190>]}
[0m12:56:39.022839 [info ] [MainThread]: 
[0m12:56:39.023147 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:56:39.023396 [info ] [MainThread]: 
[0m12:56:39.023795 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m12:56:39.024796 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m12:56:39.025374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:56:41.119761 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m12:56:41.120687 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m12:56:41.121133 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:56:41.121823 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m12:56:41.123677 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:56:41.125572 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:56:41.338065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f450fc76ad0>]}
[0m12:56:41.338761 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:56:41.346369 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m12:56:41.347230 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m12:56:41.347922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m12:56:41.348477 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m12:56:41.361963 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m12:56:41.363766 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m12:56:41.376590 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:56:41.610495 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m12:56:41.614870 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                             AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                             AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                             AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                             AS month
        , EXTRACT(QUARTER      FROM cal_date)                             AS quarter
        , EXTRACT(YEAR         FROM cal_date)                             AS year
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                 AS year_quarter

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m12:56:42.155860 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:a09a763e-24e0-453c-ba69-aa0ecf457780&page=queryresults
[0m12:56:44.783774 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8938d8b-a793-4b06-996e-5b5d407bb153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4514306690>]}
[0m12:56:44.784887 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.43s]
[0m12:56:44.785880 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m12:56:44.787664 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:56:44.788486 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:56:44.788924 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m12:56:44.789224 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m12:56:44.789514 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m12:56:44.789843 [info ] [MainThread]: 
[0m12:56:44.790186 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.77 seconds (5.77s).
[0m12:56:44.790865 [debug] [MainThread]: Command end result
[0m12:56:44.809748 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m12:56:44.810954 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m12:56:44.815247 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m12:56:44.815509 [info ] [MainThread]: 
[0m12:56:44.815833 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:56:44.816076 [info ] [MainThread]: 
[0m12:56:44.816334 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m12:56:44.816896 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.606232, "process_in_blocks": "276264", "process_kernel_time": 0.516685, "process_mem_max_rss": "231728", "process_out_blocks": "3304", "process_user_time": 3.818878}
[0m12:56:44.817261 [debug] [MainThread]: Command `dbt run` succeeded at 12:56:44.817181 after 9.61 seconds
[0m12:56:44.817570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f45480e0c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f454807f2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4548d9b2d0>]}
[0m12:56:44.817886 [debug] [MainThread]: Flushing usage events
[0m12:56:45.178460 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:08:06.883774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f944790d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f946afc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f94470a90>]}


============================== 13:08:06.886749 | 785cfd35-82aa-4afb-8695-4de1887666ab ==============================
[0m13:08:06.886749 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:08:06.887187 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run -s calendar', 'send_anonymous_usage_stats': 'True'}
[0m13:08:07.537989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f60be5f50>]}
[0m13:08:07.661075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f96e71610>]}
[0m13:08:07.661695 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:08:07.781753 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:08:07.878890 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:08:07.879596 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m13:08:08.167392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f606123d0>]}
[0m13:08:08.233126 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:08:08.235218 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:08:08.243826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f6014cbd0>]}
[0m13:08:08.244251 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m13:08:08.244535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f602aa790>]}
[0m13:08:08.245653 [info ] [MainThread]: 
[0m13:08:08.245931 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:08:08.246170 [info ] [MainThread]: 
[0m13:08:08.246561 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:08:08.247515 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:08:08.248201 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:09.240877 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m13:08:09.241796 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m13:08:09.242509 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:08:09.243286 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m13:08:09.243880 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:09.246203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:09.461353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f602d32d0>]}
[0m13:08:09.462156 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:08:09.465647 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m13:08:09.466511 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m13:08:09.467205 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m13:08:09.467803 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m13:08:09.483104 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m13:08:09.484134 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m13:08:09.501376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:08:09.716144 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m13:08:09.718430 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                             AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                             AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                             AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                             AS month
        , EXTRACT(QUARTER      FROM cal_date)                             AS quarter
        , EXTRACT(YEAR         FROM cal_date)                             AS year
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                 AS year_quarter

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m13:08:10.181992 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2ae3c984-9760-4df1-8d8b-cbb73837e774&page=queryresults
[0m13:08:12.771094 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '785cfd35-82aa-4afb-8695-4de1887666ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f6026c0d0>]}
[0m13:08:12.772130 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.30s]
[0m13:08:12.773991 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m13:08:12.775771 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:08:12.776732 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:08:12.777194 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m13:08:12.777662 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m13:08:12.778092 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m13:08:12.778565 [info ] [MainThread]: 
[0m13:08:12.779074 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.53 seconds (4.53s).
[0m13:08:12.780052 [debug] [MainThread]: Command end result
[0m13:08:12.815472 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:08:12.817516 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:08:12.825402 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m13:08:12.825764 [info ] [MainThread]: 
[0m13:08:12.826179 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:08:12.826499 [info ] [MainThread]: 
[0m13:08:12.826831 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:08:12.827338 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.991716, "process_in_blocks": "0", "process_kernel_time": 0.311201, "process_mem_max_rss": "232104", "process_out_blocks": "3344", "process_user_time": 3.591912}
[0m13:08:12.827684 [debug] [MainThread]: Command `dbt run` succeeded at 13:08:12.827608 after 5.99 seconds
[0m13:08:12.827974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f946be410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f97e77f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f98513e50>]}
[0m13:08:12.828267 [debug] [MainThread]: Flushing usage events
[0m13:08:13.251705 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:10:31.250689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedb8fa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedb8f3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedd7a310>]}


============================== 13:10:31.253686 | d594d21d-e7d8-4f5b-9e54-38426ddd848d ==============================
[0m13:10:31.253686 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:10:31.254127 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run -s calendar', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:10:31.913548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ab5e2ae10>]}
[0m13:10:32.034117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af0575810>]}
[0m13:10:32.034725 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:10:32.152012 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:10:32.248423 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:10:32.249107 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m13:10:32.536425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ab57faa50>]}
[0m13:10:32.604710 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:10:32.606799 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:10:32.615350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ab56f6150>]}
[0m13:10:32.615787 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m13:10:32.616075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aee9053d0>]}
[0m13:10:32.617167 [info ] [MainThread]: 
[0m13:10:32.617452 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:10:32.617719 [info ] [MainThread]: 
[0m13:10:32.618111 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:10:32.619156 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:10:32.619811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:10:33.531767 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m13:10:33.532678 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m13:10:33.533265 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:10:33.533877 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m13:10:33.534444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:10:33.536472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:10:33.722870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ab57641d0>]}
[0m13:10:33.723340 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:10:33.726158 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m13:10:33.727078 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m13:10:33.727782 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m13:10:33.728383 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m13:10:33.743703 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m13:10:33.744671 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m13:10:33.759693 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:10:33.953500 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m13:10:33.955970 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                             AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                             AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                             AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                             AS month
        , EXTRACT(QUARTER      FROM cal_date)                             AS quarter
        , EXTRACT(YEAR         FROM cal_date)                             AS year
        , FORMAT_DATE('%B', cal_date)                                     AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                 AS year_quarter

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                     AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                               AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))               AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))               AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                       AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                   AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)         AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                 AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                              AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)    AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m13:10:34.440142 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ce043842-c31b-4041-96fb-9de9d1229628&page=queryresults
[0m13:10:36.756410 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd594d21d-e7d8-4f5b-9e54-38426ddd848d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ab5d8a690>]}
[0m13:10:36.757442 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.03s]
[0m13:10:36.758426 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m13:10:36.760039 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:10:36.760542 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:10:36.760880 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m13:10:36.761325 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m13:10:36.761766 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m13:10:36.762217 [info ] [MainThread]: 
[0m13:10:36.762718 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.14 seconds (4.14s).
[0m13:10:36.763706 [debug] [MainThread]: Command end result
[0m13:10:36.791952 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:10:36.793205 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:10:36.797996 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m13:10:36.798265 [info ] [MainThread]: 
[0m13:10:36.798619 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:10:36.798880 [info ] [MainThread]: 
[0m13:10:36.799159 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:10:36.799716 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.5914774, "process_in_blocks": "0", "process_kernel_time": 0.327631, "process_mem_max_rss": "232788", "process_out_blocks": "3336", "process_user_time": 3.57135}
[0m13:10:36.800083 [debug] [MainThread]: Command `dbt run` succeeded at 13:10:36.800002 after 5.59 seconds
[0m13:10:36.800400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedb34210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedb349d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1aedd27ed0>]}
[0m13:10:36.800721 [debug] [MainThread]: Flushing usage events
[0m13:10:37.154772 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:43:18.816768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6acd93290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6acdb5610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6adaab1d0>]}


============================== 13:43:18.819878 | 8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027 ==============================
[0m13:43:18.819878 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:43:18.820327 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run -s calendar', 'send_anonymous_usage_stats': 'True'}
[0m13:43:19.484446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6ace06450>]}
[0m13:43:19.605379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6af7cd850>]}
[0m13:43:19.605941 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:43:19.725480 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:43:19.832368 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:43:19.833178 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m13:43:19.833498 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m13:43:20.146694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe678d2b590>]}
[0m13:43:20.216195 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:43:20.218498 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:43:20.227756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe67897c2d0>]}
[0m13:43:20.228221 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m13:43:20.228523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6789aefd0>]}
[0m13:43:20.229709 [info ] [MainThread]: 
[0m13:43:20.230000 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:43:20.230254 [info ] [MainThread]: 
[0m13:43:20.230673 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:43:20.231763 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m13:43:20.232455 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:43:21.263342 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m13:43:21.264076 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m13:43:21.264465 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:43:21.265151 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m13:43:21.265655 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:43:21.267388 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:43:21.443448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe678a13c90>]}
[0m13:43:21.444150 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:43:21.447405 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m13:43:21.448240 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m13:43:21.448907 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m13:43:21.449456 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m13:43:21.464176 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m13:43:21.465123 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m13:43:21.479025 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:43:21.701127 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m13:43:21.703684 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m13:43:22.238612 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d9db482e-d771-40ce-acaa-1e40b0c97229&page=queryresults
[0m13:43:24.596564 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8be8b6c3-90bf-4e47-ab4e-2aa1dee8f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe679016c50>]}
[0m13:43:24.597615 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.15s]
[0m13:43:24.599457 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m13:43:24.601316 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:43:24.602259 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:43:24.602724 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m13:43:24.603159 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m13:43:24.603594 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m13:43:24.604062 [info ] [MainThread]: 
[0m13:43:24.604549 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.37 seconds (4.37s).
[0m13:43:24.605662 [debug] [MainThread]: Command end result
[0m13:43:24.643291 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:43:24.645350 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:43:24.653957 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m13:43:24.654346 [info ] [MainThread]: 
[0m13:43:24.654819 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:43:24.655167 [info ] [MainThread]: 
[0m13:43:24.655496 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:43:24.656056 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.88679, "process_in_blocks": "0", "process_kernel_time": 0.340069, "process_mem_max_rss": "232392", "process_out_blocks": "3360", "process_user_time": 3.639631}
[0m13:43:24.656422 [debug] [MainThread]: Command `dbt run` succeeded at 13:43:24.656342 after 5.89 seconds
[0m13:43:24.656735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6b09fbe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6acfe19d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6b0817fd0>]}
[0m13:43:24.657042 [debug] [MainThread]: Flushing usage events
[0m13:43:25.004767 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:04:08.057713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8f36e010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8cbca310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8c9d2410>]}


============================== 14:04:08.060662 | a90df7d0-e50d-46b3-9946-fe82a249fec3 ==============================
[0m14:04:08.060662 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:04:08.061110 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:04:08.708860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc5b1c8350>]}
[0m14:04:08.819733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8f374c50>]}
[0m14:04:08.820333 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:04:08.939059 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:04:09.038801 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:04:09.039314 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/quarterly_chess_player_metrics.sql
[0m14:04:09.271909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8e522d90>]}
[0m14:04:09.339663 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:04:09.341725 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:04:09.350179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc587fc8d0>]}
[0m14:04:09.350626 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m14:04:09.350905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc58ba90d0>]}
[0m14:04:09.352219 [info ] [MainThread]: 
[0m14:04:09.352503 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:04:09.352763 [info ] [MainThread]: 
[0m14:04:09.353138 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:04:09.356981 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:04:09.357875 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:04:09.358489 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:04:09.359174 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:04:09.359669 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:04:09.360430 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:04:10.370308 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:04:10.371097 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:04:10.371815 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:04:10.372407 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:04:10.372986 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:04:10.373481 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:04:10.543012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc58882d10>]}
[0m14:04:10.543720 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:04:10.547039 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:04:10.547617 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m14:04:10.548309 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:04:10.548989 [info ] [Thread-2 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:04:10.549553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m14:04:10.550085 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.stg__player_games)
[0m14:04:10.550563 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:04:10.551058 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m14:04:10.564900 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:04:10.574755 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:04:10.575774 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:04:10.592710 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m14:04:10.596478 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:04:10.650580 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:04:10.845344 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:04:10.847729 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:04:10.919040 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:04:10.921876 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:04:11.113048 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:88f7e421-9a96-4064-bbec-51c4a751affe&page=queryresults
[0m14:04:11.327093 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:58f05eb2-f5d9-4b5f-bebd-443bd90b6a69&page=queryresults
[0m14:04:19.198161 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc585b2290>]}
[0m14:04:19.198746 [info ] [Thread-2 (]: 2 of 3 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (31.5 MiB processed)[0m in 8.65s]
[0m14:04:19.199225 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m14:04:22.794346 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc58659c90>]}
[0m14:04:22.795337 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 12.24s]
[0m14:04:22.796246 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:04:22.797421 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m14:04:22.798171 [info ] [Thread-4 (]: 3 of 3 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m14:04:22.798947 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m14:04:22.799486 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m14:04:22.807530 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:04:22.809997 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m14:04:22.814843 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:04:22.816394 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            map.opening_archetype,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                               AS opening_archetype,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(total_games)                                                                AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_weekly_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      opening_archetype,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:04:22.817641 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:04:23.159399 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7df86feb-82f6-416e-9b34-6570ce81ecdb&page=queryresults
[0m14:04:23.160278 [error] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7df86feb-82f6-416e-9b34-6570ce81ecdb&page=queryresults
[0m14:04:23.196938 [debug] [Thread-4 (]: Database Error in model quarterly_chess_player_metrics (models/marts/quarterly_chess_player_metrics.sql)
  Table "cte_weekly_aggregate" must be qualified with a dataset (e.g. dataset.table).
  compiled code at target/run/pipeline/models/marts/quarterly_chess_player_metrics.sql
[0m14:04:23.197779 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a90df7d0-e50d-46b3-9946-fe82a249fec3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc58080cd0>]}
[0m14:04:23.198725 [error] [Thread-4 (]: 3 of 3 ERROR creating sql incremental model dev_marts.quarterly_chess_player_metrics  [[31mERROR[0m in 0.40s]
[0m14:04:23.199639 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m14:04:23.200486 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.quarterly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in model quarterly_chess_player_metrics (models/marts/quarterly_chess_player_metrics.sql)
  Table "cte_weekly_aggregate" must be qualified with a dataset (e.g. dataset.table).
  compiled code at target/run/pipeline/models/marts/quarterly_chess_player_metrics.sql.
[0m14:04:23.203213 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:04:23.204221 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:04:23.204755 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:04:23.205247 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:04:23.205746 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m14:04:23.206231 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m14:04:23.206864 [info ] [MainThread]: 
[0m14:04:23.207429 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 13.85 seconds (13.85s).
[0m14:04:23.209134 [debug] [MainThread]: Command end result
[0m14:04:23.247490 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:04:23.249512 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:04:23.256827 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:04:23.257129 [info ] [MainThread]: 
[0m14:04:23.257435 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:04:23.257707 [info ] [MainThread]: 
[0m14:04:23.258034 [error] [MainThread]:   Database Error in model quarterly_chess_player_metrics (models/marts/quarterly_chess_player_metrics.sql)
  Table "cte_weekly_aggregate" must be qualified with a dataset (e.g. dataset.table).
  compiled code at target/run/pipeline/models/marts/quarterly_chess_player_metrics.sql
[0m14:04:23.258300 [info ] [MainThread]: 
[0m14:04:23.258570 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m14:04:23.259140 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 15.247399, "process_in_blocks": "624", "process_kernel_time": 0.359833, "process_mem_max_rss": "233560", "process_out_blocks": "3464", "process_user_time": 3.720905}
[0m14:04:23.259505 [debug] [MainThread]: Command `dbt run` failed at 14:04:23.259424 after 15.25 seconds
[0m14:04:23.259830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc90377dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc90417e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc8c978910>]}
[0m14:04:23.260139 [debug] [MainThread]: Flushing usage events
[0m14:04:23.610701 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:06:33.057010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7527dcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa752d6550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa752d6690>]}


============================== 14:06:33.060257 | 45579c72-7488-4250-a636-ebc0ad2f82ae ==============================
[0m14:06:33.060257 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:06:33.060750 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:06:33.706534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa429c8dd0>]}
[0m14:06:33.829023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa4140b910>]}
[0m14:06:33.829666 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:06:33.951436 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:06:34.058781 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:06:34.059333 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/quarterly_chess_player_metrics.sql
[0m14:06:34.305687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa40f589d0>]}
[0m14:06:34.379274 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:06:34.381571 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:06:34.390917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa40f91f90>]}
[0m14:06:34.391422 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m14:06:34.391744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa4114d390>]}
[0m14:06:34.393193 [info ] [MainThread]: 
[0m14:06:34.393529 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:06:34.393805 [info ] [MainThread]: 
[0m14:06:34.394224 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:06:34.398436 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:06:34.399271 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:06:34.400086 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:06:34.400627 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:06:34.401170 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:06:34.401710 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:06:35.358491 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:06:35.359394 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:06:35.359890 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:06:35.360598 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:06:35.361079 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:06:35.363137 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:06:35.534946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa752f7d50>]}
[0m14:06:35.535635 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:06:35.539061 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:06:35.539660 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m14:06:35.540455 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:06:35.541147 [info ] [Thread-2 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:06:35.541859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m14:06:35.542440 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.stg__player_games)
[0m14:06:35.542899 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:06:35.543421 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m14:06:35.568287 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:06:35.563964 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:06:35.569726 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m14:06:35.570239 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:06:35.624411 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:06:35.639746 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:06:35.880848 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:06:35.889809 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:06:35.892166 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:06:35.894442 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:06:36.069236 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:afffa67f-2536-4533-9147-a3ffaaf31b77&page=queryresults
[0m14:06:36.321380 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d97aecbb-c012-400e-b053-7264f7c30d9d&page=queryresults
[0m14:06:38.746261 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa40fa6310>]}
[0m14:06:38.747078 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.20s]
[0m14:06:38.747777 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:06:43.755482 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa40f5b550>]}
[0m14:06:43.756560 [info ] [Thread-2 (]: 2 of 3 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (31.5 MiB processed)[0m in 8.21s]
[0m14:06:43.758112 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m14:06:43.759458 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m14:06:43.760432 [info ] [Thread-4 (]: 3 of 3 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m14:06:43.761324 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m14:06:43.761948 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m14:06:43.771299 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:06:43.772315 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m14:06:43.777782 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:06:43.779079 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            map.opening_archetype,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        COALESCE(map.opening_archetype, "Mapping Failed")                               AS opening_archetype,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(total_games)                                                                AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:06:43.780046 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:06:44.295043 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:544400cc-7654-47d3-bc3e-c4dbbd162595&page=queryresults
[0m14:06:44.495024 [error] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:544400cc-7654-47d3-bc3e-c4dbbd162595&page=queryresults
[0m14:06:44.512119 [debug] [Thread-4 (]: Database Error in model quarterly_chess_player_metrics (models/marts/quarterly_chess_player_metrics.sql)
  Unrecognized name: map at [53:18]
  compiled code at target/run/pipeline/models/marts/quarterly_chess_player_metrics.sql
[0m14:06:44.513016 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45579c72-7488-4250-a636-ebc0ad2f82ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa40467c50>]}
[0m14:06:44.514053 [error] [Thread-4 (]: 3 of 3 ERROR creating sql incremental model dev_marts.quarterly_chess_player_metrics  [[31mERROR[0m in 0.75s]
[0m14:06:44.514953 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m14:06:44.515946 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.quarterly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in model quarterly_chess_player_metrics (models/marts/quarterly_chess_player_metrics.sql)
  Unrecognized name: map at [53:18]
  compiled code at target/run/pipeline/models/marts/quarterly_chess_player_metrics.sql.
[0m14:06:44.518648 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:06:44.519485 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:06:44.519954 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:06:44.520390 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m14:06:44.520836 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:06:44.521263 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m14:06:44.521827 [info ] [MainThread]: 
[0m14:06:44.522329 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 10.13 seconds (10.13s).
[0m14:06:44.523915 [debug] [MainThread]: Command end result
[0m14:06:44.565260 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:06:44.567490 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:06:44.574232 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:06:44.574517 [info ] [MainThread]: 
[0m14:06:44.574847 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:06:44.575123 [info ] [MainThread]: 
[0m14:06:44.575450 [error] [MainThread]:   Database Error in model quarterly_chess_player_metrics (models/marts/quarterly_chess_player_metrics.sql)
  Unrecognized name: map at [53:18]
  compiled code at target/run/pipeline/models/marts/quarterly_chess_player_metrics.sql
[0m14:06:44.575723 [info ] [MainThread]: 
[0m14:06:44.576003 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m14:06:44.576559 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 11.567099, "process_in_blocks": "448", "process_kernel_time": 0.315485, "process_mem_max_rss": "234236", "process_out_blocks": "3464", "process_user_time": 3.825397}
[0m14:06:44.576938 [debug] [MainThread]: Command `dbt run` failed at 14:06:44.576855 after 11.57 seconds
[0m14:06:44.577254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa78c77e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa78d17f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa75287690>]}
[0m14:06:44.577563 [debug] [MainThread]: Flushing usage events
[0m14:06:44.953504 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:08:23.975449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f4e239fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f4e47a390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f4e467ad0>]}


============================== 14:08:23.978717 | 7e8159fc-91d8-4206-b2bf-4e1f4d91d398 ==============================
[0m14:08:23.978717 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:08:23.979196 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:08:24.640669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f17351d90>]}
[0m14:08:24.763284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f50c81350>]}
[0m14:08:24.763847 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:08:24.885765 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:08:24.984486 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:08:24.985001 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/quarterly_chess_player_metrics.sql
[0m14:08:25.210640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f16063050>]}
[0m14:08:25.276578 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:08:25.278720 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:08:25.287521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f16192350>]}
[0m14:08:25.287993 [info ] [MainThread]: Found 1 seed, 3 models, 1 source, 492 macros
[0m14:08:25.288284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f15ed7a10>]}
[0m14:08:25.289812 [info ] [MainThread]: 
[0m14:08:25.290149 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:08:25.290389 [info ] [MainThread]: 
[0m14:08:25.290787 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:08:25.294855 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:08:25.295806 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:08:25.296675 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:08:25.297267 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:25.297884 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:25.298450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:26.246089 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:08:26.246734 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:08:26.247367 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:08:26.247777 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:08:26.248429 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:08:26.248854 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:08:26.409821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f1611d850>]}
[0m14:08:26.410565 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:08:26.413963 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:08:26.414417 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m14:08:26.414975 [info ] [Thread-1 (]: 1 of 3 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:08:26.415497 [info ] [Thread-2 (]: 2 of 3 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:08:26.415980 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m14:08:26.416382 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m14:08:26.416836 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:08:26.417206 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m14:08:26.424772 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:08:26.432278 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:08:26.433004 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m14:08:26.433411 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:08:26.473437 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:08:26.493718 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:08:26.755605 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:08:26.768895 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:08:26.770713 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:08:26.773166 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:08:26.983580 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e58b2ccb-2fd9-482d-9973-ed127b6697ec&page=queryresults
[0m14:08:27.244233 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:06db0000-cbae-4c0f-a080-ca6891ea1053&page=queryresults
[0m14:08:29.796185 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f15d7fad0>]}
[0m14:08:29.796993 [info ] [Thread-1 (]: 1 of 3 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.38s]
[0m14:08:29.797692 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:08:34.544532 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f15e23790>]}
[0m14:08:34.545502 [info ] [Thread-2 (]: 2 of 3 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (31.5 MiB processed)[0m in 8.13s]
[0m14:08:34.546403 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m14:08:34.547418 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m14:08:34.548108 [info ] [Thread-4 (]: 3 of 3 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m14:08:34.548938 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m14:08:34.549389 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m14:08:34.555019 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:08:34.555654 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m14:08:34.559053 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:08:34.559980 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            COALESCE(map.opening_archetype, "Mapping Failed")         AS opening_archetype,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        wagg.opening_archetype,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(total_games)                                                                AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:08:34.560644 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:08:35.095121 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:03094e09-6881-4895-82c2-6ced5c03df31&page=queryresults
[0m14:08:40.434369 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7e8159fc-91d8-4206-b2bf-4e1f4d91d398', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f1418d590>]}
[0m14:08:40.435303 [info ] [Thread-4 (]: 3 of 3 OK created sql incremental model dev_marts.quarterly_chess_player_metrics  [[32mCREATE TABLE (888.8k rows, 354.5 MiB processed)[0m in 5.89s]
[0m14:08:40.435937 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m14:08:40.437639 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:08:40.438536 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:08:40.439007 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:08:40.439450 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m14:08:40.439894 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:08:40.440317 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m14:08:40.440879 [info ] [MainThread]: 
[0m14:08:40.441383 [info ] [MainThread]: Finished running 2 incremental models, 1 table model in 0 hours 0 minutes and 15.15 seconds (15.15s).
[0m14:08:40.442875 [debug] [MainThread]: Command end result
[0m14:08:40.479426 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:08:40.481623 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:08:40.489915 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:08:40.490198 [info ] [MainThread]: 
[0m14:08:40.490519 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:08:40.490785 [info ] [MainThread]: 
[0m14:08:40.491058 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:08:40.491626 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.562874, "process_in_blocks": "0", "process_kernel_time": 0.341021, "process_mem_max_rss": "233180", "process_out_blocks": "3464", "process_user_time": 3.728402}
[0m14:08:40.491994 [debug] [MainThread]: Command `dbt run` succeeded at 14:08:40.491911 after 16.56 seconds
[0m14:08:40.492302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f4e28b510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f4e423e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0f52317d90>]}
[0m14:08:40.492617 [debug] [MainThread]: Flushing usage events
[0m14:08:40.838913 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:22:58.622956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb089fb1f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb089fb3590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb089fb1450>]}


============================== 14:22:58.625703 | 0190cb40-459a-4800-b471-c985ec70865e ==============================
[0m14:22:58.625703 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:22:58.626199 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:22:59.296801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb08a01d150>]}
[0m14:22:59.418918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb08ca4dd90>]}
[0m14:22:59.419463 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:22:59.540268 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:22:59.639858 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m14:22:59.640322 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/weekly_chess_player_metrics.sql
[0m14:22:59.640663 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/quarterly_chess_player_metrics.sql
[0m14:22:59.892433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb051df6650>]}
[0m14:22:59.966739 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:22:59.969060 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:22:59.978602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb051e3fe50>]}
[0m14:22:59.979068 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m14:22:59.979367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb051f1c9d0>]}
[0m14:22:59.980848 [info ] [MainThread]: 
[0m14:22:59.981148 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:22:59.981409 [info ] [MainThread]: 
[0m14:22:59.981828 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:22:59.985995 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:22:59.986929 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:22:59.987487 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:22:59.988320 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:22:59.988873 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:22:59.989827 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:23:01.206510 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:23:01.207371 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:23:01.208326 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:23:01.209024 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:23:01.209788 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:23:01.210401 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:23:01.395082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb051ff2f10>]}
[0m14:23:01.395908 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:23:01.399710 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:23:01.400238 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m14:23:01.400922 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:23:01.401604 [info ] [Thread-2 (]: 2 of 4 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:23:01.402251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m14:23:01.402779 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m14:23:01.403282 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:23:01.403755 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m14:23:01.413730 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:23:01.423868 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:23:01.424718 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:23:01.430275 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m14:23:01.441452 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:23:01.493459 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:23:01.709287 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:23:01.737337 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:23:01.749779 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:23:01.753209 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:23:01.920865 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:49c07f1f-b9c2-4d8f-aec4-960382e4d8e0&page=queryresults
[0m14:23:02.204504 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:97ac3f15-e81a-4dbd-90d6-e784f4f2bc7a&page=queryresults
[0m14:23:04.764879 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0502456d0>]}
[0m14:23:04.766012 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.36s]
[0m14:23:04.767015 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:23:09.512936 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb051d02750>]}
[0m14:23:09.513907 [info ] [Thread-2 (]: 2 of 4 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (31.5 MiB processed)[0m in 8.11s]
[0m14:23:09.514838 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m14:23:09.516203 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m14:23:09.516889 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m14:23:09.517728 [info ] [Thread-4 (]: 3 of 4 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m14:23:09.518506 [info ] [Thread-3 (]: 4 of 4 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m14:23:09.519393 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m14:23:09.520005 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_player_metrics)
[0m14:23:09.520581 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m14:23:09.521150 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m14:23:09.530003 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:23:09.537489 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m14:23:09.538269 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m14:23:09.538742 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m14:23:09.542778 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:23:09.547403 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m14:23:09.550156 [debug] [Thread-3 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:23:09.551104 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:23:10.767770 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:23:10.769424 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            COALESCE(map.opening_archetype, "Mapping Failed")         AS opening_archetype,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        wagg.opening_archetype,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(quarter_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.quarter_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)
    values
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`

  


  

    
[0m14:23:10.917462 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c7e231e0-dfb5-4a45-a536-cf582d691409&page=queryresults
[0m14:23:11.090483 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:646a1e55-789b-4269-a879-48428ac792e0&page=queryresults
[0m14:23:11.284872 [error] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:646a1e55-789b-4269-a879-48428ac792e0&page=queryresults
[0m14:23:11.293989 [debug] [Thread-3 (]: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name piece_color not found inside wagg at [51:28]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:23:11.294847 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb041eed710>]}
[0m14:23:11.295833 [error] [Thread-3 (]: 4 of 4 ERROR creating sql incremental model dev_marts.weekly_chess_player_metrics  [[31mERROR[0m in 1.77s]
[0m14:23:11.296871 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m14:23:11.297901 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name piece_color not found inside wagg at [51:28]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql.
[0m14:23:20.164669 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0190cb40-459a-4800-b471-c985ec70865e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb041da3bd0>]}
[0m14:23:20.165652 [info ] [Thread-4 (]: 3 of 4 OK created sql incremental model dev_marts.quarterly_chess_player_metrics  [[32mSCRIPT (22.2 MiB processed)[0m in 10.65s]
[0m14:23:20.166573 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m14:23:20.168345 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:23:20.169140 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:23:20.169579 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:23:20.170015 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:23:20.170435 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m14:23:20.170871 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m14:23:20.171418 [info ] [MainThread]: 
[0m14:23:20.171925 [info ] [MainThread]: Finished running 3 incremental models, 1 table model in 0 hours 0 minutes and 20.19 seconds (20.19s).
[0m14:23:20.173632 [debug] [MainThread]: Command end result
[0m14:23:20.212002 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:23:20.214035 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:23:20.221598 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:23:20.221901 [info ] [MainThread]: 
[0m14:23:20.222214 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:23:20.222485 [info ] [MainThread]: 
[0m14:23:20.222846 [error] [MainThread]:   Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name piece_color not found inside wagg at [51:28]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:23:20.223116 [info ] [MainThread]: 
[0m14:23:20.223394 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m14:23:20.223956 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 21.668457, "process_in_blocks": "0", "process_kernel_time": 0.372011, "process_mem_max_rss": "234512", "process_out_blocks": "3552", "process_user_time": 3.865542}
[0m14:23:20.224345 [debug] [MainThread]: Command `dbt run` failed at 14:23:20.224264 after 21.67 seconds
[0m14:23:20.224674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb08a0081d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb08db1bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb08da7be90>]}
[0m14:23:20.225023 [debug] [MainThread]: Flushing usage events
[0m14:23:20.644487 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:04.637288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69abf83c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69abfd33d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69ac1c9b50>]}


============================== 14:24:04.640397 | fff592eb-05de-489e-9462-0e4816814e67 ==============================
[0m14:24:04.640397 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:24:04.640843 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:24:05.300999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69ace04c10>]}
[0m14:24:05.422814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69ae975610>]}
[0m14:24:05.423373 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:24:05.544634 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:24:05.642185 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:24:05.642706 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_player_metrics.sql
[0m14:24:05.882609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6973b31e90>]}
[0m14:24:05.950887 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:24:05.952988 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:24:05.961885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6973a35b50>]}
[0m14:24:05.962344 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m14:24:05.962638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69acd41050>]}
[0m14:24:05.964186 [info ] [MainThread]: 
[0m14:24:05.964523 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:24:05.964795 [info ] [MainThread]: 
[0m14:24:05.965194 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:24:05.969053 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:24:05.969881 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:24:05.970493 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:05.971133 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:24:05.971665 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:05.972254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:06.936525 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:24:06.937164 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:06.937748 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:24:06.938343 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:24:06.940308 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:06.940709 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:07.108777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69780df310>]}
[0m14:24:07.109477 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:07.112672 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:24:07.113193 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m14:24:07.113808 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:24:07.114385 [info ] [Thread-2 (]: 2 of 4 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:24:07.114886 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m14:24:07.115421 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m14:24:07.115888 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:24:07.116412 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m14:24:07.130460 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:24:07.135801 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:24:07.136359 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:24:07.136651 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m14:24:07.148924 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:07.183328 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:24:07.406159 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:24:07.433180 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:24:07.435497 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:24:07.438385 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m14:24:07.599394 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:72cd9886-b953-4735-80f7-0daf24c37d3f&page=queryresults
[0m14:24:07.874054 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f1a5d9e1-ef34-48f6-8d23-092bb92f8dce&page=queryresults
[0m14:24:10.106157 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69700d7610>]}
[0m14:24:10.107205 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.99s]
[0m14:24:10.108226 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:24:15.229013 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6973aa1150>]}
[0m14:24:15.229977 [info ] [Thread-2 (]: 2 of 4 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (31.5 MiB processed)[0m in 8.11s]
[0m14:24:15.230887 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m14:24:15.232054 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m14:24:15.232677 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m14:24:15.233470 [info ] [Thread-4 (]: 3 of 4 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m14:24:15.234147 [info ] [Thread-3 (]: 4 of 4 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m14:24:15.235010 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m14:24:15.235605 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.weekly_chess_player_metrics)
[0m14:24:15.236188 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m14:24:15.236758 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m14:24:15.245216 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:24:15.252243 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m14:24:15.252997 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m14:24:15.253445 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m14:24:15.258088 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m14:24:15.261924 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:24:15.264450 [debug] [Thread-3 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:24:15.264975 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:24:15.454762 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:24:15.456299 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description="""""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            COALESCE(map.opening_archetype, "Mapping Failed")         AS opening_archetype,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        wagg.opening_archetype,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(quarter_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.quarter_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)
    values
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`

  


  

    
[0m14:24:15.605384 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:35eaaa19-bea9-4b01-b588-ba451e065ab6&page=queryresults
[0m14:24:15.760717 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7e0340fe-9dbc-4f16-b72a-6d7ceeebef0b&page=queryresults
[0m14:24:15.958104 [error] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7e0340fe-9dbc-4f16-b72a-6d7ceeebef0b&page=queryresults
[0m14:24:15.967220 [debug] [Thread-3 (]: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name avg_accuracy not found inside wagg at [55:60]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:24:15.968018 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6953d78690>]}
[0m14:24:15.968974 [error] [Thread-3 (]: 4 of 4 ERROR creating sql incremental model dev_marts.weekly_chess_player_metrics  [[31mERROR[0m in 0.73s]
[0m14:24:15.969895 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m14:24:15.970770 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name avg_accuracy not found inside wagg at [55:60]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql.
[0m14:24:23.614749 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fff592eb-05de-489e-9462-0e4816814e67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69700e9490>]}
[0m14:24:23.615750 [info ] [Thread-4 (]: 3 of 4 OK created sql incremental model dev_marts.quarterly_chess_player_metrics  [[32mSCRIPT (12.7 MiB processed)[0m in 8.38s]
[0m14:24:23.616680 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m14:24:23.618470 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:24:23.619398 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:23.619861 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:24:23.620291 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:24:23.620731 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m14:24:23.621156 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m14:24:23.621746 [info ] [MainThread]: 
[0m14:24:23.622362 [info ] [MainThread]: Finished running 3 incremental models, 1 table model in 0 hours 0 minutes and 17.66 seconds (17.66s).
[0m14:24:23.624151 [debug] [MainThread]: Command end result
[0m14:24:23.662371 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:24:23.664428 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:24:23.671880 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:24:23.672272 [info ] [MainThread]: 
[0m14:24:23.672594 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:24:23.672863 [info ] [MainThread]: 
[0m14:24:23.673190 [error] [MainThread]:   Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name avg_accuracy not found inside wagg at [55:60]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:24:23.673453 [info ] [MainThread]: 
[0m14:24:23.673737 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 TOTAL=4
[0m14:24:23.674308 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 19.083467, "process_in_blocks": "0", "process_kernel_time": 0.301673, "process_mem_max_rss": "234876", "process_out_blocks": "3560", "process_user_time": 3.822337}
[0m14:24:23.674683 [debug] [MainThread]: Command `dbt run` failed at 14:24:23.674602 after 19.08 seconds
[0m14:24:23.675003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69abf80a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69abf83310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f69acd5fe10>]}
[0m14:24:23.675321 [debug] [MainThread]: Flushing usage events
[0m14:24:24.091051 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:22.167985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0c94bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0c95210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0c96e50>]}


============================== 14:25:22.170912 | 6be552af-21c3-42ae-aaa9-f0f46b88b9d3 ==============================
[0m14:25:22.170912 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:25:22.171356 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run -s weekly_chess_player_metrics.sql', 'send_anonymous_usage_stats': 'True'}
[0m14:25:22.828458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0c9cf10>]}
[0m14:25:22.947850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb36cdb10>]}
[0m14:25:22.948404 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:25:23.066397 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:25:23.166884 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:25:23.167400 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_player_metrics.sql
[0m14:25:23.398431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbc7c9afdd0>]}
[0m14:25:23.465817 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:25:23.467928 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:25:23.476930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbc7cf4d210>]}
[0m14:25:23.477397 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m14:25:23.477703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbc7cb24f50>]}
[0m14:25:23.479068 [info ] [MainThread]: 
[0m14:25:23.479376 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:25:23.479642 [info ] [MainThread]: 
[0m14:25:23.480056 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:25:23.481127 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:25:23.481798 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:24.385440 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:25:24.386348 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m14:25:24.386831 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:24.387622 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m14:25:24.388227 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:24.390362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:24.565245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbc7c8dfc90>]}
[0m14:25:24.566038 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:24.569642 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m14:25:24.570501 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m14:25:24.571184 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_player_metrics)
[0m14:25:24.571755 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m14:25:24.586289 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m14:25:24.587184 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m14:25:24.651017 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m14:25:24.651793 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.avg_accuracy)                                       AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:25:24.652303 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:25:25.212279 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:869354c4-2971-47bb-87b6-9b2066cc4f10&page=queryresults
[0m14:25:25.425256 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:869354c4-2971-47bb-87b6-9b2066cc4f10&page=queryresults
[0m14:25:25.434210 [debug] [Thread-1 (]: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name avg_accuracy not found inside t at [27:19]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:25:25.436626 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6be552af-21c3-42ae-aaa9-f0f46b88b9d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbc74ebab90>]}
[0m14:25:25.437660 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_player_metrics  [[31mERROR[0m in 0.86s]
[0m14:25:25.438566 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m14:25:25.439558 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name avg_accuracy not found inside t at [27:19]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql.
[0m14:25:25.442230 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:25:25.443092 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:25.443541 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m14:25:25.443991 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m14:25:25.444416 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m14:25:25.444889 [info ] [MainThread]: 
[0m14:25:25.445382 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.96 seconds (1.96s).
[0m14:25:25.446363 [debug] [MainThread]: Command end result
[0m14:25:25.485755 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:25:25.488121 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:25:25.495963 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:25:25.496245 [info ] [MainThread]: 
[0m14:25:25.496571 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:25:25.496849 [info ] [MainThread]: 
[0m14:25:25.497181 [error] [MainThread]:   Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Name avg_accuracy not found inside t at [27:19]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:25:25.497440 [info ] [MainThread]: 
[0m14:25:25.497724 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:25:25.498269 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.3779287, "process_in_blocks": "0", "process_kernel_time": 0.318517, "process_mem_max_rss": "228980", "process_out_blocks": "3312", "process_user_time": 3.538065}
[0m14:25:25.498643 [debug] [MainThread]: Command `dbt run` failed at 14:25:25.498551 after 3.38 seconds
[0m14:25:25.498962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0c96ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0ce7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbcb0ce5d90>]}
[0m14:25:25.499282 [debug] [MainThread]: Flushing usage events
[0m14:25:25.847272 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:43.815158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd317fe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd32b63d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd317c290>]}


============================== 14:25:43.818133 | 6e89862c-d4db-4a83-8715-2eaaab2d29d2 ==============================
[0m14:25:43.818133 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:25:43.818577 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run -s weekly_chess_player_metrics.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:25:44.480952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b9b3fa250>]}
[0m14:25:44.601740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd5b74c50>]}
[0m14:25:44.602295 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:25:44.724160 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:25:44.832588 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:25:44.833138 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_player_metrics.sql
[0m14:25:45.080748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b9aef2810>]}
[0m14:25:45.154885 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:25:45.157190 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:25:45.166534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b9ad5a510>]}
[0m14:25:45.167004 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m14:25:45.167304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b9adcf090>]}
[0m14:25:45.168656 [info ] [MainThread]: 
[0m14:25:45.168952 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:25:45.169206 [info ] [MainThread]: 
[0m14:25:45.169617 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:25:45.170686 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:25:45.171331 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:47.826004 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:25:47.826952 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m14:25:47.827418 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:25:47.828274 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m14:25:47.828735 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:47.830457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:48.013248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b9b3fe310>]}
[0m14:25:48.013973 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:48.017191 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m14:25:48.017743 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m14:25:48.018196 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_player_metrics)
[0m14:25:48.018632 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m14:25:48.027554 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m14:25:48.028136 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m14:25:48.097979 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m14:25:48.098823 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    partition by week_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:25:48.099379 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:25:48.574472 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2a95a19e-3431-42cd-a26b-7a5ffa996aa1&page=queryresults
[0m14:25:48.686563 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2a95a19e-3431-42cd-a26b-7a5ffa996aa1&page=queryresults
[0m14:25:48.692600 [debug] [Thread-1 (]: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Unrecognized name: opening_archetype at [9:26]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:25:48.694455 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e89862c-d4db-4a83-8715-2eaaab2d29d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b9acc6290>]}
[0m14:25:48.695221 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model dev_marts.weekly_chess_player_metrics  [[31mERROR[0m in 0.68s]
[0m14:25:48.695949 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m14:25:48.696726 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.weekly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Unrecognized name: opening_archetype at [9:26]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql.
[0m14:25:48.699614 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:25:48.700516 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:48.700973 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m14:25:48.701413 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m14:25:48.701857 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m14:25:48.702323 [info ] [MainThread]: 
[0m14:25:48.702866 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.53 seconds (3.53s).
[0m14:25:48.703840 [debug] [MainThread]: Command end result
[0m14:25:48.740471 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:25:48.742437 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:25:48.750088 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:25:48.750429 [info ] [MainThread]: 
[0m14:25:48.750793 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:25:48.751043 [info ] [MainThread]: 
[0m14:25:48.751342 [error] [MainThread]:   Database Error in model weekly_chess_player_metrics (models/marts/weekly_chess_player_metrics.sql)
  Unrecognized name: opening_archetype at [9:26]
  compiled code at target/run/pipeline/models/marts/weekly_chess_player_metrics.sql
[0m14:25:48.751580 [info ] [MainThread]: 
[0m14:25:48.751839 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:25:48.752340 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.9836755, "process_in_blocks": "0", "process_kernel_time": 0.319559, "process_mem_max_rss": "228292", "process_out_blocks": "3304", "process_user_time": 3.537854}
[0m14:25:48.752687 [debug] [MainThread]: Command `dbt run` failed at 14:25:48.752609 after 4.98 seconds
[0m14:25:48.752985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd317c150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd31dcb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6bd31dc310>]}
[0m14:25:48.753271 [debug] [MainThread]: Flushing usage events
[0m14:25:49.095780 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:29.257223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33db2bd850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33db2803d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33db4ca510>]}


============================== 14:26:29.260479 | 88a1aaab-542b-442d-be29-f7c9143eb53d ==============================
[0m14:26:29.260479 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:26:29.260941 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run -s weekly_chess_player_metrics.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:26:29.882178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a3438990>]}
[0m14:26:30.002664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33ddccd750>]}
[0m14:26:30.003217 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:26:30.120545 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:26:30.217799 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:26:30.218341 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/marts/weekly_chess_player_metrics.sql
[0m14:26:30.453120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a2f3a610>]}
[0m14:26:30.519116 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:26:30.521192 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:26:30.530426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a2f80310>]}
[0m14:26:30.530894 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m14:26:30.531206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a2fc2fd0>]}
[0m14:26:30.532570 [info ] [MainThread]: 
[0m14:26:30.532914 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:26:30.533180 [info ] [MainThread]: 
[0m14:26:30.533617 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:26:30.534707 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:26:30.535367 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:31.418511 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:26:31.419326 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:31.420024 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m14:26:31.422395 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m14:26:31.422873 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:31.423435 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:31.599956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a3221a10>]}
[0m14:26:31.600690 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:31.604197 [debug] [Thread-1 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m14:26:31.604832 [info ] [Thread-1 (]: 1 of 1 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m14:26:31.605308 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.weekly_chess_player_metrics)
[0m14:26:31.605702 [debug] [Thread-1 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m14:26:31.615177 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m14:26:31.615788 [debug] [Thread-1 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m14:26:31.679007 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m14:26:31.679776 [debug] [Thread-1 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    partition by week_start_date
    cluster by username, time_class

    OPTIONS(
      description=""""""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:26:31.680280 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:26:32.261569 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e0ea3cb3-80d3-4c8c-9a08-641322f07d94&page=queryresults
[0m14:26:37.199297 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88a1aaab-542b-442d-be29-f7c9143eb53d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a2f3b650>]}
[0m14:26:37.200378 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model dev_marts.weekly_chess_player_metrics .. [[32mCREATE TABLE (660.4k rows, 234.6 MiB processed)[0m in 5.59s]
[0m14:26:37.201330 [debug] [Thread-1 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m14:26:37.203423 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:26:37.204350 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:37.204853 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m14:26:37.205321 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m14:26:37.205820 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m14:26:37.206323 [info ] [MainThread]: 
[0m14:26:37.206876 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.67 seconds (6.67s).
[0m14:26:37.207936 [debug] [MainThread]: Command end result
[0m14:26:37.248829 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:26:37.250926 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:26:37.257334 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:26:37.257610 [info ] [MainThread]: 
[0m14:26:37.257935 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:26:37.258180 [info ] [MainThread]: 
[0m14:26:37.258439 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:26:37.258969 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 8.050206, "process_in_blocks": "0", "process_kernel_time": 0.33008, "process_mem_max_rss": "231744", "process_out_blocks": "3312", "process_user_time": 3.506646}
[0m14:26:37.259315 [debug] [MainThread]: Command `dbt run` succeeded at 14:26:37.259236 after 8.05 seconds
[0m14:26:37.259614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33db2e1350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33db5e83d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a322f410>]}
[0m14:26:37.259907 [debug] [MainThread]: Flushing usage events
[0m14:26:37.631051 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:12:28.154066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5cf8b690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5cf8af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5cf8b3d0>]}


============================== 16:12:28.157139 | 0c1a5729-6815-46c4-9e12-8aaa800ed2ab ==============================
[0m16:12:28.157139 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:12:28.157591 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:12:28.791649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea2a716750>]}
[0m16:12:28.901051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5f9e15d0>]}
[0m16:12:28.901636 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:12:29.019691 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:12:29.120691 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:12:29.121548 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/schema.yml
[0m16:12:29.425616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28c67bd0>]}
[0m16:12:29.491377 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:12:29.493475 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:12:29.502530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28c9a550>]}
[0m16:12:29.502976 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m16:12:29.503255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5eb83d50>]}
[0m16:12:29.504640 [info ] [MainThread]: 
[0m16:12:29.504919 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:12:29.505153 [info ] [MainThread]: 
[0m16:12:29.505536 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:12:29.509339 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:12:29.510172 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:12:29.510817 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:12:29.511418 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m16:12:29.512003 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:12:29.512927 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:12:30.563298 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m16:12:30.564205 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m16:12:30.564704 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:12:30.565426 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m16:12:30.565951 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:12:30.567973 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:12:30.752691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28c44b90>]}
[0m16:12:30.753430 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:12:30.756907 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:12:30.757467 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:12:30.758171 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m16:12:30.758760 [info ] [Thread-2 (]: 2 of 4 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m16:12:30.759251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m16:12:30.759763 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.stg__player_games)
[0m16:12:30.760200 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:12:30.760664 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:12:30.780410 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:12:30.791380 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:12:30.792288 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:12:30.792896 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:12:30.850754 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:12:30.874574 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:12:31.083019 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m16:12:31.117784 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m16:12:31.119871 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months/quarters/years"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m16:12:31.123007 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    OPTIONS(
      description="""Stage model that transforms raw chess game data into a player-level view,  with one row per player per game (both white and black perspectives).  Includes standardizations for result classification (win/loss/draw),  basic game metadata, and parsed opening names. This model supports  incremental loads by game date for efficient backfills.\n""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m16:12:31.304990 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0d3ed558-f7b2-4126-aa94-798735d56242&page=queryresults
[0m16:12:31.610345 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6dd9492c-c0fd-4cc5-aad0-3529eed4b0fa&page=queryresults
[0m16:12:34.140130 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28b2d110>]}
[0m16:12:34.141156 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.38s]
[0m16:12:34.142110 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:12:41.012797 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28c9a410>]}
[0m16:12:41.013497 [info ] [Thread-2 (]: 2 of 4 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (31.5 MiB processed)[0m in 10.25s]
[0m16:12:41.014125 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:12:41.015103 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:12:41.015833 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:12:41.016577 [info ] [Thread-4 (]: 3 of 4 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m16:12:41.017379 [info ] [Thread-3 (]: 4 of 4 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m16:12:41.018209 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m16:12:41.018809 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_player_metrics)
[0m16:12:41.019389 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:12:41.019962 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:12:41.028328 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:12:41.035923 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:12:41.036950 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:12:41.037497 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:12:41.041525 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:12:41.045367 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:12:41.218263 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:12:41.220003 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description="""Aggregates player game statistics by quarter, time class, and opening archetype. This model provides breakdowns of game outcomes (win/loss/draw) and average accuracy by piece color (white/black) for rated standard chess games. It uses calendar and opening mapping tables to enrich the game data, and filters to only include recent games based on configurable date logic.\n""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            COALESCE(map.opening_archetype, "Mapping Failed")         AS opening_archetype,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        wagg.opening_archetype,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(quarter_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.quarter_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)
    values
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`

  


  

    
[0m16:12:41.256546 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m16:12:41.258103 [debug] [Thread-3 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`
      
    partition by week_start_date
    cluster by username, time_class

    OPTIONS(
      description="""Weekly aggregate statistics of rated standard chess games by player, piece color, and time class. This model calculates performance metrics such as win/loss/draw counts, accuracy, and rating, separated by white and black games. It uses the calendar table to align games to ISO week start dates and is designed for incremental refresh with weekly partitioning and clustering.\n""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `time_class`, `avg_rating`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)
    values
        (`week_start_date`, `week_number`, `username`, `time_class`, `avg_rating`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`

  


  

    
[0m16:12:41.398103 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2b24075e-078a-4871-be5b-9af121a68b5a&page=queryresults
[0m16:12:41.431579 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:daff8d29-45e9-4184-a6e1-93222dfc410c&page=queryresults
[0m16:12:49.335107 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28a960d0>]}
[0m16:12:49.336183 [info ] [Thread-3 (]: 4 of 4 OK created sql incremental model dev_marts.weekly_chess_player_metrics .. [[32mSCRIPT (8.0 MiB processed)[0m in 8.32s]
[0m16:12:49.337140 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:12:50.110836 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c1a5729-6815-46c4-9e12-8aaa800ed2ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea28a05c10>]}
[0m16:12:50.111630 [info ] [Thread-4 (]: 3 of 4 OK created sql incremental model dev_marts.quarterly_chess_player_metrics  [[32mSCRIPT (12.7 MiB processed)[0m in 9.09s]
[0m16:12:50.112265 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:12:50.114018 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:12:50.114925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:12:50.115434 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m16:12:50.115952 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m16:12:50.116422 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m16:12:50.116910 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m16:12:50.117527 [info ] [MainThread]: 
[0m16:12:50.118105 [info ] [MainThread]: Finished running 3 incremental models, 1 table model in 0 hours 0 minutes and 20.61 seconds (20.61s).
[0m16:12:50.120006 [debug] [MainThread]: Command end result
[0m16:12:50.160023 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:12:50.162418 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:12:50.168391 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:12:50.168711 [info ] [MainThread]: 
[0m16:12:50.169013 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:12:50.169261 [info ] [MainThread]: 
[0m16:12:50.169522 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m16:12:50.170050 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 22.063105, "process_in_blocks": "0", "process_kernel_time": 0.349404, "process_mem_max_rss": "234900", "process_out_blocks": "3632", "process_user_time": 3.853127}
[0m16:12:50.170403 [debug] [MainThread]: Command `dbt run` succeeded at 16:12:50.170326 after 22.06 seconds
[0m16:12:50.170699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5cf77f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea5d1ca650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fea60977d90>]}
[0m16:12:50.170987 [debug] [MainThread]: Flushing usage events
[0m16:12:50.614477 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:39:27.322234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc8981690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc89d3ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc8ce4510>]}


============================== 18:39:27.325753 | edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8 ==============================
[0m18:39:27.325753 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:39:27.326240 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s calendar', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:39:30.198229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d94d589d0>]}
[0m18:39:30.330627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dcb375bd0>]}
[0m18:39:30.331629 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:39:30.495631 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:39:30.760623 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:39:30.761681 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m18:39:31.107546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d94748910>]}
[0m18:39:31.181927 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:39:31.185527 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:39:31.217143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d94470cd0>]}
[0m18:39:31.217606 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m18:39:31.217899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d945de3d0>]}
[0m18:39:31.219093 [info ] [MainThread]: 
[0m18:39:31.219374 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:39:31.219615 [info ] [MainThread]: 
[0m18:39:31.220014 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:39:31.220903 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:39:31.221552 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:33.138117 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:39:33.139022 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m18:39:33.139677 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:39:33.140426 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m18:39:33.141018 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:33.143394 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:39:33.337726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d9489fb50>]}
[0m18:39:33.338547 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:39:33.348580 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:39:33.349320 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:39:33.349724 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m18:39:33.350044 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:39:33.357487 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:39:33.358958 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:39:33.372154 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:39:33.566902 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:39:33.570308 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months/quarters/years"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                                                                      AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                                                                      AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:39:34.001373 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:cc0e8157-59f7-44f3-ab46-d81fbdf70301&page=queryresults
[0m18:39:34.226520 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:cc0e8157-59f7-44f3-ab46-d81fbdf70301&page=queryresults
[0m18:39:34.268415 [debug] [Thread-1 (]: Database Error in model calendar (models/universal/calendar.sql)
  No matching signature for operator AND
    Argument types: BOOL, DATE
    Signature: BOOL AND ([BOOL, ...])
      Argument 2: Unable to coerce type DATE to expected type BOOL at [114:11]
  compiled code at target/run/pipeline/models/universal/calendar.sql
[0m18:39:34.271392 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'edd12d6f-23f7-4dfb-8e4c-31dd52a42ad8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4d94593750>]}
[0m18:39:34.272513 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model dev_universal.calendar ................... [[31mERROR[0m in 0.92s]
[0m18:39:34.273498 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:39:34.274398 [debug] [Thread-7 (]: Marking all children of 'model.pipeline.calendar' to be skipped because of status 'error'.  Reason: Database Error in model calendar (models/universal/calendar.sql)
  No matching signature for operator AND
    Argument types: BOOL, DATE
    Signature: BOOL AND ([BOOL, ...])
      Argument 2: Unable to coerce type DATE to expected type BOOL at [114:11]
  compiled code at target/run/pipeline/models/universal/calendar.sql.
[0m18:39:34.277014 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:39:34.277980 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:39:34.278440 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m18:39:34.278894 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m18:39:34.279321 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:39:34.279804 [info ] [MainThread]: 
[0m18:39:34.280300 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.06 seconds (3.06s).
[0m18:39:34.281342 [debug] [MainThread]: Command end result
[0m18:39:34.320799 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:39:34.322859 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:39:34.328782 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:39:34.329062 [info ] [MainThread]: 
[0m18:39:34.329374 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:39:34.329627 [info ] [MainThread]: 
[0m18:39:34.329941 [error] [MainThread]:   Database Error in model calendar (models/universal/calendar.sql)
  No matching signature for operator AND
    Argument types: BOOL, DATE
    Signature: BOOL AND ([BOOL, ...])
      Argument 2: Unable to coerce type DATE to expected type BOOL at [114:11]
  compiled code at target/run/pipeline/models/universal/calendar.sql
[0m18:39:34.330198 [info ] [MainThread]: 
[0m18:39:34.330458 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:39:34.331024 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 7.057461, "process_in_blocks": "185792", "process_kernel_time": 0.446099, "process_mem_max_rss": "228448", "process_out_blocks": "3480", "process_user_time": 3.887693}
[0m18:39:34.331384 [debug] [MainThread]: Command `dbt run` failed at 18:39:34.331305 after 7.06 seconds
[0m18:39:34.331712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc9697650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc8981210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4dc89810d0>]}
[0m18:39:34.332010 [debug] [MainThread]: Flushing usage events
[0m18:39:34.682667 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:48:48.476651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7d287c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7d287a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7d284690>]}


============================== 18:48:48.479925 | 169b160d-83da-4958-a09c-580dec8d0c46 ==============================
[0m18:48:48.479925 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:48:48.480392 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run -s calendar', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:48:49.169372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa494a7c50>]}
[0m18:48:49.297380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7fccd850>]}
[0m18:48:49.298144 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:48:49.422072 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:48:49.528198 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:48:49.528764 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m18:48:49.859383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7e425cd0>]}
[0m18:48:49.936598 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:48:49.938987 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:48:49.948812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa48edc410>]}
[0m18:48:49.949291 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m18:48:49.949637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa48fad390>]}
[0m18:48:49.950870 [info ] [MainThread]: 
[0m18:48:49.951149 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:48:49.951387 [info ] [MainThread]: 
[0m18:48:49.951781 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:48:49.952646 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:48:49.953073 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:48:50.943726 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m18:48:50.944596 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m18:48:50.945253 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:48:50.945932 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m18:48:50.946483 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:48:50.948704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:48:51.156054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa48fa3050>]}
[0m18:48:51.156780 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:48:51.160182 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:48:51.161160 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:48:51.161913 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m18:48:51.162528 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:48:51.178522 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:48:51.179532 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:48:51.194594 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:48:51.410875 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:48:51.413425 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months/quarters/years"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                                                                      AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:48:51.932054 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f72f36fc-06c6-44bb-9ff5-8d85f7db028c&page=queryresults
[0m18:48:54.704443 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '169b160d-83da-4958-a09c-580dec8d0c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa49071910>]}
[0m18:48:54.705519 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.54s]
[0m18:48:54.706463 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:48:54.708256 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:48:54.709201 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:48:54.709656 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m18:48:54.710105 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:48:54.710533 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m18:48:54.711013 [info ] [MainThread]: 
[0m18:48:54.711503 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.76 seconds (4.76s).
[0m18:48:54.712485 [debug] [MainThread]: Command end result
[0m18:48:54.747626 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:48:54.749424 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:48:54.755110 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:48:54.755370 [info ] [MainThread]: 
[0m18:48:54.755691 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:48:54.755949 [info ] [MainThread]: 
[0m18:48:54.756210 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:48:54.756729 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.3278227, "process_in_blocks": "1928", "process_kernel_time": 0.352905, "process_mem_max_rss": "232280", "process_out_blocks": "3480", "process_user_time": 3.691606}
[0m18:48:54.757090 [debug] [MainThread]: Command `dbt run` succeeded at 18:48:54.757013 after 6.33 seconds
[0m18:48:54.757387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7df9b390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7d2dd110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa7d4c98d0>]}
[0m18:48:54.757691 [debug] [MainThread]: Flushing usage events
[0m18:48:55.125241 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:53:39.641006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3ba91a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3ba884d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3ba88450>]}


============================== 18:53:39.644080 | 74dc8b43-2add-4aef-89aa-f97fa5eb5d93 ==============================
[0m18:53:39.644080 [info ] [MainThread]: Running with dbt=1.9.4
[0m18:53:39.644530 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run -s calendar', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:53:40.329740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3ba88690>]}
[0m18:53:40.458984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3e3e93d0>]}
[0m18:53:40.459596 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m18:53:40.582926 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m18:53:40.712870 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:53:40.713597 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/universal/calendar.sql
[0m18:53:41.040910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a03613990>]}
[0m18:53:41.116715 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:53:41.119082 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:53:41.129098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a03675490>]}
[0m18:53:41.129566 [info ] [MainThread]: Found 1 seed, 4 models, 1 source, 492 macros
[0m18:53:41.129880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a036dd250>]}
[0m18:53:41.131419 [info ] [MainThread]: 
[0m18:53:41.131868 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m18:53:41.132142 [info ] [MainThread]: 
[0m18:53:41.132586 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m18:53:41.133691 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m18:53:41.134379 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:53:42.142911 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m18:53:42.143912 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m18:53:42.144438 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:53:42.145272 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m18:53:42.145812 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:53:42.147394 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:53:42.317239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a036239d0>]}
[0m18:53:42.318019 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:53:42.321562 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m18:53:42.322443 [info ] [Thread-1 (]: 1 of 1 START sql table model dev_universal.calendar ............................ [RUN]
[0m18:53:42.323142 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m18:53:42.323697 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m18:53:42.338270 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m18:53:42.339080 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m18:53:42.352845 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:53:42.587855 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m18:53:42.590223 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months/quarters/years"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                                                                      AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m18:53:43.041786 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7c3185b7-b127-4e5a-9db1-f24864095a21&page=queryresults
[0m18:53:45.252001 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74dc8b43-2add-4aef-89aa-f97fa5eb5d93', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a03799690>]}
[0m18:53:45.253198 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.93s]
[0m18:53:45.254276 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m18:53:45.256261 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:53:45.257331 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:53:45.257851 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m18:53:45.258480 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m18:53:45.259025 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_marts' was properly closed.
[0m18:53:45.259547 [info ] [MainThread]: 
[0m18:53:45.260154 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.13 seconds (4.13s).
[0m18:53:45.261404 [debug] [MainThread]: Command end result
[0m18:53:45.309705 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m18:53:45.311514 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m18:53:45.317143 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m18:53:45.317481 [info ] [MainThread]: 
[0m18:53:45.317843 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:53:45.318174 [info ] [MainThread]: 
[0m18:53:45.318523 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:53:45.319158 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.72688, "process_in_blocks": "0", "process_kernel_time": 0.349654, "process_mem_max_rss": "232368", "process_out_blocks": "3480", "process_user_time": 3.731279}
[0m18:53:45.319536 [debug] [MainThread]: Command `dbt run` succeeded at 18:53:45.319454 after 5.73 seconds
[0m18:53:45.319860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3bae9650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3bcd6550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2a3ba7fed0>]}
[0m18:53:45.320231 [debug] [MainThread]: Flushing usage events
[0m18:53:45.685663 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:40:26.151505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff0d844d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff0d7cb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff0d84a10>]}


============================== 13:40:26.162473 | ef4c219c-041f-423b-96e0-d76100f4f774 ==============================
[0m13:40:26.162473 [info ] [MainThread]: Running with dbt=1.9.4
[0m13:40:26.162910 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:40:29.027067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ef4c219c-041f-423b-96e0-d76100f4f774', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfbcf2bc50>]}
[0m13:40:29.149689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ef4c219c-041f-423b-96e0-d76100f4f774', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff37cd7d0>]}
[0m13:40:29.150283 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m13:40:29.306400 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m13:40:29.550664 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m13:40:29.551125 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test_total_game_count.sql
[0m13:40:29.719700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef4c219c-041f-423b-96e0-d76100f4f774', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfbcf1e850>]}
[0m13:40:29.797694 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:40:29.801609 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:40:29.852414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef4c219c-041f-423b-96e0-d76100f4f774', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfbc9adfd0>]}
[0m13:40:29.852891 [info ] [MainThread]: Found 1 seed, 4 models, 1 test, 1 source, 492 macros
[0m13:40:29.853194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef4c219c-041f-423b-96e0-d76100f4f774', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfbcbaacd0>]}
[0m13:40:29.854643 [info ] [MainThread]: 
[0m13:40:29.854943 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:40:29.855201 [info ] [MainThread]: 
[0m13:40:29.855625 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m13:40:29.859875 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m13:40:29.860799 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m13:40:29.861267 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:40:29.861989 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m13:40:29.862320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:40:29.862979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:40:31.934453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef4c219c-041f-423b-96e0-d76100f4f774', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfbd07d790>]}
[0m13:40:31.935140 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:40:31.945453 [debug] [Thread-1 (]: Began running node test.pipeline.test_total_game_count
[0m13:40:31.946128 [info ] [Thread-1 (]: 1 of 1 START test test_total_game_count ........................................ [RUN]
[0m13:40:31.946801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.pipeline.test_total_game_count)
[0m13:40:31.947346 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_total_game_count
[0m13:40:31.958835 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_total_game_count"
[0m13:40:31.960135 [debug] [Thread-1 (]: Began executing node test.pipeline.test_total_game_count
[0m13:40:31.977176 [debug] [Thread-1 (]: Writing runtime sql for node "test.pipeline.test_total_game_count"
[0m13:40:31.978098 [debug] [Thread-1 (]: On test.pipeline.test_total_game_count: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_total_game_count"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
  SUM(total_games) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    ) dbt_internal_test
[0m13:40:31.978452 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:40:33.873435 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:45769ea0-7a35-4a31-a4a9-97a3a055ce31&page=queryresults
[0m13:40:34.607845 [error] [Thread-1 (]: 1 of 1 FAIL 1 test_total_game_count ............................................ [[31mFAIL 1[0m in 2.66s]
[0m13:40:34.608894 [debug] [Thread-1 (]: Finished running node test.pipeline.test_total_game_count
[0m13:40:34.610836 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:40:34.611750 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:40:34.612247 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_universal' was properly closed.
[0m13:40:34.612690 [debug] [MainThread]: Connection 'list_checkmate-453316_dev_staging' was properly closed.
[0m13:40:34.613110 [debug] [MainThread]: Connection 'test.pipeline.test_total_game_count' was properly closed.
[0m13:40:34.613578 [info ] [MainThread]: 
[0m13:40:34.614062 [info ] [MainThread]: Finished running 1 test in 0 hours 0 minutes and 4.76 seconds (4.76s).
[0m13:40:34.615047 [debug] [MainThread]: Command end result
[0m13:40:34.647041 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m13:40:34.648610 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m13:40:34.653862 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m13:40:34.654150 [info ] [MainThread]: 
[0m13:40:34.654467 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:40:34.654761 [info ] [MainThread]: 
[0m13:40:34.655066 [error] [MainThread]: [31mFailure in test test_total_game_count (tests/test_total_game_count.sql)[0m
[0m13:40:34.655354 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m13:40:34.655589 [info ] [MainThread]: 
[0m13:40:34.655864 [info ] [MainThread]:   compiled code at target/compiled/pipeline/tests/test_total_game_count.sql
[0m13:40:34.656096 [info ] [MainThread]: 
[0m13:40:34.656351 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:40:34.656973 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 8.549215, "process_in_blocks": "277120", "process_kernel_time": 0.563289, "process_mem_max_rss": "230288", "process_out_blocks": "3384", "process_user_time": 3.552215}
[0m13:40:34.657313 [debug] [MainThread]: Command `dbt test` failed at 13:40:34.657236 after 8.55 seconds
[0m13:40:34.657621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff0d844d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff4777e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcff1087e50>]}
[0m13:40:34.657925 [debug] [MainThread]: Flushing usage events
[0m13:40:35.007384 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:45.766225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1317ab890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd12ed92bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd12efca490>]}


============================== 14:19:45.769240 | d0d1826e-bea6-41c9-80e5-a191365fb7c7 ==============================
[0m14:19:45.769240 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:19:45.769730 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --full-refresh', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:19:46.414688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f70e6b90>]}
[0m14:19:46.536100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1317cd950>]}
[0m14:19:46.536708 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:19:46.655177 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:19:46.765622 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:19:46.765964 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:19:46.812702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f6f91910>]}
[0m14:19:46.890802 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:19:46.892895 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:19:46.901760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f6fa8bd0>]}
[0m14:19:46.902196 [info ] [MainThread]: Found 1 seed, 4 models, 1 test, 1 source, 492 macros
[0m14:19:46.902480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd131238590>]}
[0m14:19:46.903907 [info ] [MainThread]: 
[0m14:19:46.904193 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:19:46.904439 [info ] [MainThread]: 
[0m14:19:46.904832 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:19:46.908488 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:19:46.908986 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:19:46.909608 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m14:19:46.910063 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:46.910496 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:46.911012 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:47.919478 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m14:19:47.920125 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m14:19:47.920629 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m14:19:47.921025 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:19:47.921640 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:19:47.922036 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:19:48.129094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f707cc50>]}
[0m14:19:48.129842 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:19:48.133293 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m14:19:48.133732 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m14:19:48.134277 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m14:19:48.134784 [info ] [Thread-2 (]: 2 of 4 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m14:19:48.135255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m14:19:48.135636 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m14:19:48.136078 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m14:19:48.136432 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m14:19:48.145081 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m14:19:48.158540 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m14:19:48.159288 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m14:19:48.159667 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m14:19:48.195716 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:48.221987 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:19:48.408251 [debug] [Thread-1 (]: Writing runtime sql for node "model.pipeline.calendar"
[0m14:19:48.419600 [debug] [Thread-2 (]: Writing runtime sql for node "model.pipeline.stg__player_games"
[0m14:19:48.421049 [debug] [Thread-1 (]: On model.pipeline.calendar: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months/quarters/years"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                                                                      AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m14:19:48.422939 [debug] [Thread-2 (]: On model.pipeline.stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.stg__player_games"} */

  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games`
      
    partition by game_date
    

    OPTIONS(
      description="""Stage model that transforms raw chess game data into a player-level view,  with one row per player per game (both white and black perspectives).  Includes standardizations for result classification (win/loss/draw),  basic game metadata, and parsed opening names. This model supports  incremental loads by game date for efficient backfills.\n"""
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  

    );
  
[0m14:19:49.027532 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d67b491f-e8bb-4d21-8109-29ba06344712&page=queryresults
[0m14:19:49.050016 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:00f07f26-4de7-44dc-aa3f-315bc9ca70a1&page=queryresults
[0m14:19:51.566398 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f6a7b010>]}
[0m14:19:51.567488 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 3.43s]
[0m14:19:51.568122 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m14:19:59.356759 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f6d44e50>]}
[0m14:19:59.357751 [info ] [Thread-2 (]: 2 of 4 OK created sql incremental model dev_staging.stg__player_games .......... [[32mCREATE TABLE (4.1m rows, 262.8 MiB processed)[0m in 11.22s]
[0m14:19:59.358673 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m14:19:59.359813 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m14:19:59.360413 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m14:19:59.361099 [info ] [Thread-4 (]: 3 of 4 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m14:19:59.361746 [info ] [Thread-3 (]: 4 of 4 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m14:19:59.362422 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m14:19:59.362917 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.weekly_chess_player_metrics)
[0m14:19:59.363533 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m14:19:59.364098 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m14:19:59.370241 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:19:59.377775 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m14:19:59.378690 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m14:19:59.379224 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m14:19:59.384590 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:19:59.388814 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:19:59.569111 [debug] [Thread-3 (]: Writing runtime sql for node "model.pipeline.weekly_chess_player_metrics"
[0m14:19:59.572201 [debug] [Thread-3 (]: On model.pipeline.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.weekly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      
    partition by week_start_date
    cluster by username, time_class

    OPTIONS(
      description="""Weekly aggregate statistics of rated standard chess games by player, piece color, and time class. This model calculates performance metrics such as win/loss/draw counts, accuracy, and rating, separated by white and black games. It uses the calendar table to align games to ISO week start dates and is designed for incremental refresh with weekly partitioning and clustering.\n"""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:19:59.601679 [debug] [Thread-4 (]: Writing runtime sql for node "model.pipeline.quarterly_chess_player_metrics"
[0m14:19:59.603028 [debug] [Thread-4 (]: On model.pipeline.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "model.pipeline.quarterly_chess_player_metrics"} */

  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    OPTIONS(
      description="""Aggregates player game statistics by quarter, time class, and opening archetype. This model provides breakdowns of game outcomes (win/loss/draw) and average accuracy by piece color (white/black) for rated standard chess games. It uses calendar and opening mapping tables to enrich the game data, and filters to only include recent games based on configurable date logic.\n"""
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            COALESCE(map.opening_archetype, "Mapping Failed")         AS opening_archetype,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        wagg.opening_archetype,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
[0m14:19:59.995507 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e033efb8-407c-461c-99ee-f46b30b72765&page=queryresults
[0m14:20:00.067262 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0f78a034-07a4-4980-b725-f1b3dfdf2117&page=queryresults
[0m14:20:05.191776 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0ded64e90>]}
[0m14:20:05.192633 [info ] [Thread-4 (]: 3 of 4 OK created sql incremental model dev_marts.quarterly_chess_player_metrics  [[32mCREATE TABLE (888.8k rows, 323.1 MiB processed)[0m in 5.83s]
[0m14:20:05.193307 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m14:20:05.650933 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0d1826e-bea6-41c9-80e5-a191365fb7c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0f6a78310>]}
[0m14:20:05.651922 [info ] [Thread-3 (]: 4 of 4 OK created sql incremental model dev_marts.weekly_chess_player_metrics .. [[32mCREATE TABLE (660.4k rows, 234.6 MiB processed)[0m in 6.29s]
[0m14:20:05.652856 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m14:20:05.654693 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:20:05.655629 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:20:05.656085 [debug] [MainThread]: Connection 'model.pipeline.calendar' was properly closed.
[0m14:20:05.656536 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m14:20:05.656963 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m14:20:05.657402 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m14:20:05.657978 [info ] [MainThread]: 
[0m14:20:05.658513 [info ] [MainThread]: Finished running 3 incremental models, 1 table model in 0 hours 0 minutes and 18.75 seconds (18.75s).
[0m14:20:05.660287 [debug] [MainThread]: Command end result
[0m14:20:05.699301 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:20:05.701377 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:20:05.708415 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:20:05.708687 [info ] [MainThread]: 
[0m14:20:05.708982 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:20:05.709221 [info ] [MainThread]: 
[0m14:20:05.709493 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4
[0m14:20:05.709999 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.990948, "process_in_blocks": "40", "process_kernel_time": 0.340131, "process_mem_max_rss": "230404", "process_out_blocks": "2520", "process_user_time": 3.563342}
[0m14:20:05.710338 [debug] [MainThread]: Command `dbt run` succeeded at 14:20:05.710263 after 19.99 seconds
[0m14:20:05.710643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd12ed801d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd12ed77e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd132777d10>]}
[0m14:20:05.710926 [debug] [MainThread]: Flushing usage events
[0m14:20:06.153891 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:56:02.549814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5150198bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f51501f2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f51501f3c50>]}


============================== 15:56:02.552423 | bc5b2134-e410-4d12-9a70-4c102d7d9d4b ==============================
[0m15:56:02.552423 [info ] [MainThread]: Running with dbt=1.9.4
[0m15:56:02.552895 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt compile', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:56:03.217674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f511e9e0dd0>]}
[0m15:56:03.326212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5152bdd850>]}
[0m15:56:03.326758 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m15:56:03.444910 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m15:56:03.534329 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:56:03.534745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f51508c22d0>]}
[0m15:56:04.732428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f511c1d3850>]}
[0m15:56:04.815632 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:56:04.817876 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:56:04.826864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5117cd7510>]}
[0m15:56:04.827323 [info ] [MainThread]: Found 4 models, 1 test, 1 seed, 1 source, 492 macros
[0m15:56:04.827632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5117c03490>]}
[0m15:56:04.829154 [info ] [MainThread]: 
[0m15:56:04.829455 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:56:04.829708 [info ] [MainThread]: 
[0m15:56:04.830108 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m15:56:04.834157 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m15:56:04.835185 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m15:56:04.836013 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m15:56:04.836482 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:56:04.837004 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:56:04.837508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:56:05.755978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc5b2134-e410-4d12-9a70-4c102d7d9d4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f511c4912d0>]}
[0m15:56:05.756362 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:56:05.758897 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m15:56:05.759484 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m15:56:05.760032 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m15:56:05.760715 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m15:56:05.761265 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m15:56:05.761780 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now seed.pipeline.opening_mapping)
[0m15:56:05.762373 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m15:56:05.762876 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m15:56:05.763333 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m15:56:05.777371 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m15:56:05.786835 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m15:56:05.790293 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m15:56:05.791299 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:56:05.792875 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m15:56:05.793508 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m15:56:05.794070 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m15:56:05.794816 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:56:05.795409 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:56:05.796629 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m15:56:05.797812 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m15:56:05.798897 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m15:56:05.799573 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m15:56:05.800275 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m15:56:05.800878 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m15:56:05.801460 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m15:56:05.802004 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m15:56:05.810104 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m15:56:05.817923 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m15:56:05.819013 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m15:56:05.819644 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:56:05.820202 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m15:56:05.821476 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m15:56:05.822051 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:56:05.823350 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m15:56:05.824314 [debug] [Thread-1 (]: Began running node test.pipeline.test-stg__player_games-game_count
[0m15:56:05.824916 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test-stg__player_games-game_count)
[0m15:56:05.825466 [debug] [Thread-1 (]: Began compiling node test.pipeline.test-stg__player_games-game_count
[0m15:56:05.830687 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test-stg__player_games-game_count"
[0m15:56:05.831479 [debug] [Thread-1 (]: Began executing node test.pipeline.test-stg__player_games-game_count
[0m15:56:05.832027 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:56:05.833219 [debug] [Thread-1 (]: Finished running node test.pipeline.test-stg__player_games-game_count
[0m15:56:05.834684 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:56:05.835218 [debug] [MainThread]: Connection 'test.pipeline.test-stg__player_games-game_count' was properly closed.
[0m15:56:05.835664 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m15:56:05.835987 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m15:56:05.836274 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m15:56:05.837659 [debug] [MainThread]: Command end result
[0m15:56:05.954920 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m15:56:05.956230 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m15:56:05.962043 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m15:56:05.962658 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.4611738, "process_in_blocks": "144", "process_kernel_time": 0.32563, "process_mem_max_rss": "232808", "process_out_blocks": "3528", "process_user_time": 4.450688}
[0m15:56:05.963036 [debug] [MainThread]: Command `dbt compile` succeeded at 15:56:05.962951 after 3.46 seconds
[0m15:56:05.963356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f51504fc590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5150eab5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5153b77d10>]}
[0m15:56:05.963687 [debug] [MainThread]: Flushing usage events
[0m15:56:06.331602 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:03:55.670624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f3347e610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f33474790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f3347f950>]}


============================== 16:03:55.673138 | cedf222d-c7b5-4c8a-80b2-6042ee8c4492 ==============================
[0m16:03:55.673138 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:03:55.673620 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt clean', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:03:55.764662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cedf222d-c7b5-4c8a-80b2-6042ee8c4492', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f334d4b90>]}
[0m16:03:55.775025 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.15056208, "process_in_blocks": "16", "process_kernel_time": 0.17033, "process_mem_max_rss": "102048", "process_out_blocks": "16", "process_user_time": 1.172881}
[0m16:03:55.775422 [debug] [MainThread]: Command `dbt clean` succeeded at 16:03:55.775326 after 0.15 seconds
[0m16:03:55.775732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f3347fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f36f17e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f36f17f10>]}
[0m16:03:55.776059 [debug] [MainThread]: Flushing usage events
[0m16:03:56.138128 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:04:04.924781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3798494f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37987f8510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37984952d0>]}


============================== 16:04:04.928322 | 1447f4d4-bb29-4930-9b41-f2109c30f501 ==============================
[0m16:04:04.928322 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:04:04.928845 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'True'}
[0m16:04:05.600333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3766480dd0>]}
[0m16:04:05.723891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379aecd510>]}
[0m16:04:05.724506 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:04:05.848949 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:04:05.849528 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:04:05.849835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3798515710>]}
[0m16:04:07.176441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37640ba590>]}
[0m16:04:07.264347 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:04:07.266574 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:04:07.275719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3764156810>]}
[0m16:04:07.276175 [info ] [MainThread]: Found 4 models, 1 test, 1 seed, 1 source, 492 macros
[0m16:04:07.276489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3764258250>]}
[0m16:04:07.278013 [info ] [MainThread]: 
[0m16:04:07.278314 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:04:07.278587 [info ] [MainThread]: 
[0m16:04:07.278999 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:04:07.283071 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:04:07.284255 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:04:07.284782 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:04:07.285432 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:04:07.285857 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:04:07.286603 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:04:08.662632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1447f4d4-bb29-4930-9b41-f2109c30f501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f376481d510>]}
[0m16:04:08.663361 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:04:08.666707 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:04:08.667333 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:04:08.667903 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m16:04:08.668341 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m16:04:08.668877 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m16:04:08.669261 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now seed.pipeline.opening_mapping)
[0m16:04:08.669674 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:04:08.670206 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:04:08.670569 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m16:04:08.684357 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:04:08.694864 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:04:08.698107 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m16:04:08.698955 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:04:08.699948 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m16:04:08.700708 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:04:08.701279 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:04:08.702338 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:04:08.702779 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:04:08.703480 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:04:08.704652 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:04:08.705549 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:04:08.705863 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:04:08.706225 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m16:04:08.706554 [debug] [Thread-1 (]: Began running node test.pipeline.test-stg__player_games-game_count
[0m16:04:08.707074 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m16:04:08.707702 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:04:08.708191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test-stg__player_games-game_count)
[0m16:04:08.708677 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:04:08.718246 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:04:08.718968 [debug] [Thread-1 (]: Began compiling node test.pipeline.test-stg__player_games-game_count
[0m16:04:08.727221 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:04:08.732539 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test-stg__player_games-game_count"
[0m16:04:08.733459 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:04:08.734011 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:04:08.734754 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:04:08.735360 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:04:08.736764 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:04:08.737297 [debug] [Thread-1 (]: Began executing node test.pipeline.test-stg__player_games-game_count
[0m16:04:08.738482 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:04:08.739207 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:04:08.740488 [debug] [Thread-1 (]: Finished running node test.pipeline.test-stg__player_games-game_count
[0m16:04:08.741880 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:04:08.742430 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m16:04:08.742868 [debug] [MainThread]: Connection 'test.pipeline.test-stg__player_games-game_count' was properly closed.
[0m16:04:08.743292 [debug] [MainThread]: Connection 'model.pipeline.stg__player_games' was properly closed.
[0m16:04:08.743730 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m16:04:08.745748 [debug] [MainThread]: Command end result
[0m16:04:08.780716 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:04:08.782901 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:04:08.792492 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:04:08.793189 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.9243171, "process_in_blocks": "0", "process_kernel_time": 0.331899, "process_mem_max_rss": "232760", "process_out_blocks": "3512", "process_user_time": 4.477477}
[0m16:04:08.793573 [debug] [MainThread]: Command `dbt compile` succeeded at 16:04:08.793490 after 3.92 seconds
[0m16:04:08.793892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379848c1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3798483f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3798483ed0>]}
[0m16:04:08.794205 [debug] [MainThread]: Flushing usage events
[0m16:04:09.138500 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:14:54.065591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904aa00d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904a98550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904a98850>]}


============================== 16:14:54.068574 | 79529b87-cd5a-472f-bffe-5e6762bfd920 ==============================
[0m16:14:54.068574 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:14:54.069012 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt compile', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:14:54.732776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '79529b87-cd5a-472f-bffe-5e6762bfd920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904ab4dd0>]}
[0m16:14:54.855458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '79529b87-cd5a-472f-bffe-5e6762bfd920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9073d5950>]}
[0m16:14:54.856005 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:14:54.987880 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:14:55.107202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m16:14:55.107675 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test-quarterly_chess_player_metrics-game_count.sql
[0m16:14:55.108006 [debug] [MainThread]: Partial parsing: updated file: pipeline://tests/test-stg__player_games-game_count.sql
[0m16:14:55.273817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79529b87-cd5a-472f-bffe-5e6762bfd920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8d09018d0>]}
[0m16:14:55.350889 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:14:55.353069 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:14:55.361642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79529b87-cd5a-472f-bffe-5e6762bfd920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8d0698b50>]}
[0m16:14:55.362083 [info ] [MainThread]: Found 4 models, 1 seed, 2 data tests, 1 source, 492 macros
[0m16:14:55.362360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79529b87-cd5a-472f-bffe-5e6762bfd920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb906e229d0>]}
[0m16:14:55.363841 [info ] [MainThread]: 
[0m16:14:55.364142 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:14:55.364392 [info ] [MainThread]: 
[0m16:14:55.364776 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:14:55.368642 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:14:55.369555 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:14:55.370034 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:55.370802 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:14:55.371234 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:55.371865 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:56.372003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79529b87-cd5a-472f-bffe-5e6762bfd920', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8d08b6610>]}
[0m16:14:56.372700 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:14:56.376039 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:14:56.376546 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:14:56.377126 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m16:14:56.377668 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m16:14:56.378076 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m16:14:56.378584 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now seed.pipeline.opening_mapping)
[0m16:14:56.379079 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:14:56.379528 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:14:56.380086 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m16:14:56.388843 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:14:56.402631 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:14:56.405663 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m16:14:56.406486 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:14:56.406960 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:14:56.407473 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:14:56.407810 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:14:56.409018 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m16:14:56.409782 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:14:56.410144 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:14:56.411539 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:14:56.412469 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:14:56.412787 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:14:56.413174 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m16:14:56.413577 [debug] [Thread-1 (]: Began running node test.pipeline.test-stg__player_games-game_count
[0m16:14:56.414049 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m16:14:56.414381 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:14:56.414843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test-stg__player_games-game_count)
[0m16:14:56.415301 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:14:56.420243 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:14:56.420736 [debug] [Thread-1 (]: Began compiling node test.pipeline.test-stg__player_games-game_count
[0m16:14:56.425293 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:14:56.430362 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test-stg__player_games-game_count"
[0m16:14:56.430987 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:14:56.431489 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:14:56.432679 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:14:56.433267 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:14:56.434054 [debug] [Thread-1 (]: Began executing node test.pipeline.test-stg__player_games-game_count
[0m16:14:56.434816 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:14:56.435426 [debug] [Thread-2 (]: Began running node test.pipeline.test-quarterly_chess_player_metrics-game_count
[0m16:14:56.436015 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:14:56.437257 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:14:56.437877 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now test.pipeline.test-quarterly_chess_player_metrics-game_count)
[0m16:14:56.439076 [debug] [Thread-1 (]: Finished running node test.pipeline.test-stg__player_games-game_count
[0m16:14:56.439826 [debug] [Thread-2 (]: Began compiling node test.pipeline.test-quarterly_chess_player_metrics-game_count
[0m16:14:56.445138 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test-quarterly_chess_player_metrics-game_count"
[0m16:14:56.445925 [debug] [Thread-2 (]: Began executing node test.pipeline.test-quarterly_chess_player_metrics-game_count
[0m16:14:56.446495 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:14:56.447677 [debug] [Thread-2 (]: Finished running node test.pipeline.test-quarterly_chess_player_metrics-game_count
[0m16:14:56.449242 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:14:56.449791 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m16:14:56.450224 [debug] [MainThread]: Connection 'test.pipeline.test-stg__player_games-game_count' was properly closed.
[0m16:14:56.450670 [debug] [MainThread]: Connection 'test.pipeline.test-quarterly_chess_player_metrics-game_count' was properly closed.
[0m16:14:56.451094 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m16:14:56.453438 [debug] [MainThread]: Command end result
[0m16:14:56.492085 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:14:56.494234 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:14:56.502945 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:14:56.503572 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.4851923, "process_in_blocks": "0", "process_kernel_time": 0.332662, "process_mem_max_rss": "225492", "process_out_blocks": "3544", "process_user_time": 3.456942}
[0m16:14:56.503924 [debug] [MainThread]: Command `dbt compile` succeeded at 16:14:56.503845 after 2.49 seconds
[0m16:14:56.504260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904afa190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904afad10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb904a8fd10>]}
[0m16:14:56.504577 [debug] [MainThread]: Flushing usage events
[0m16:14:56.886877 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:28:14.557542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e3c86950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e3c87010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e3c87a10>]}


============================== 16:28:14.560611 | 91776e61-ba8e-415a-b805-85f33dfc7ac6 ==============================
[0m16:28:14.560611 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:28:14.561048 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt clean', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:28:14.644101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91776e61-ba8e-415a-b805-85f33dfc7ac6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e3d5d190>]}
[0m16:28:14.651589 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.14128497, "process_in_blocks": "0", "process_kernel_time": 0.156614, "process_mem_max_rss": "102348", "process_out_blocks": "8", "process_user_time": 1.194535}
[0m16:28:14.651950 [debug] [MainThread]: Command `dbt clean` succeeded at 16:28:14.651873 after 0.14 seconds
[0m16:28:14.652244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e3c7c0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e3cd6650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99e7677e50>]}
[0m16:28:14.652562 [debug] [MainThread]: Flushing usage events
[0m16:28:15.020627 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:28:20.109412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7664984450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7664bc6490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7664984790>]}


============================== 16:28:20.112353 | 7b47ab7b-c425-45fe-8f04-7bafab255875 ==============================
[0m16:28:20.112353 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:28:20.112800 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:28:20.753689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7630c4cd10>]}
[0m16:28:20.873020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7667375650>]}
[0m16:28:20.873621 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:28:20.992142 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:28:20.992722 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:28:20.993022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7630aee5d0>]}
[0m16:28:22.277521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76304b2d50>]}
[0m16:28:22.363979 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:28:22.366196 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:28:22.375535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76303e2810>]}
[0m16:28:22.376006 [info ] [MainThread]: Found 4 models, 3 data tests, 1 seed, 1 source, 492 macros
[0m16:28:22.376326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7664acf810>]}
[0m16:28:22.377982 [info ] [MainThread]: 
[0m16:28:22.378290 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:28:22.378555 [info ] [MainThread]: 
[0m16:28:22.378966 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:28:22.382950 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:28:22.383828 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:28:22.384301 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:28:22.384876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:28:22.385673 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:28:22.386911 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:28:23.291078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7b47ab7b-c425-45fe-8f04-7bafab255875', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7630473910>]}
[0m16:28:23.291778 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:28:23.295248 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:28:23.295925 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:28:23.296590 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m16:28:23.297353 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.calendar)
[0m16:28:23.297962 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.stg__player_games)
[0m16:28:23.298607 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now seed.pipeline.opening_mapping)
[0m16:28:23.299212 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:28:23.299773 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:28:23.300345 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m16:28:23.324489 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:28:23.320890 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:28:23.327728 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m16:28:23.328707 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:28:23.330078 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m16:28:23.330901 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:28:23.331482 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:28:23.332465 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:28:23.332885 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:28:23.334178 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:28:23.335914 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:28:23.337137 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:28:23.337599 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:28:23.338205 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m16:28:23.338652 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m16:28:23.339249 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m16:28:23.339765 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:28:23.340241 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test_game_count-stg__player_games)
[0m16:28:23.340837 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:28:23.346903 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:28:23.347412 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m16:28:23.354728 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:28:23.359866 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m16:28:23.360730 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:28:23.361263 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:28:23.361952 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:28:23.362551 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:28:23.363882 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:28:23.364380 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m16:28:23.365607 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:28:23.366349 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:28:23.367846 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m16:28:23.368472 [debug] [Thread-2 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:28:23.369167 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:28:23.369831 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m16:28:23.370431 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.quarterly_chess_player_metrics, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m16:28:23.370982 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:28:23.371568 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:28:23.378555 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m16:28:23.383549 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m16:28:23.384420 [debug] [Thread-2 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:28:23.385038 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:28:23.385595 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:28:23.386747 [debug] [Thread-2 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:28:23.387307 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:28:23.388594 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:28:23.390054 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:28:23.390649 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-stg__player_games' was properly closed.
[0m16:28:23.391084 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m16:28:23.391521 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m16:28:23.391941 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m16:28:23.394310 [debug] [MainThread]: Command end result
[0m16:28:23.422729 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:28:23.424018 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:28:23.429881 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:28:23.430486 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.3676665, "process_in_blocks": "0", "process_kernel_time": 0.326675, "process_mem_max_rss": "232640", "process_out_blocks": "3568", "process_user_time": 4.410348}
[0m16:28:23.430867 [debug] [MainThread]: Command `dbt compile` succeeded at 16:28:23.430783 after 3.37 seconds
[0m16:28:23.431183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76649decd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f766497c290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7664bc5810>]}
[0m16:28:23.431504 [debug] [MainThread]: Flushing usage events
[0m16:28:23.774077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:50:11.662810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d37b95150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d37b94590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d37b96ed0>]}


============================== 16:50:11.665289 | 091e4058-3447-4390-bbcb-3f2c69a4b625 ==============================
[0m16:50:11.665289 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:50:11.665731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:50:12.298265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '091e4058-3447-4390-bbcb-3f2c69a4b625', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cffc608d0>]}
[0m16:50:12.407279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '091e4058-3447-4390-bbcb-3f2c69a4b625', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d3a588ad0>]}
[0m16:50:12.407840 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:50:12.526909 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:50:12.634957 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 0 files changed.
[0m16:50:12.635480 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/schema.yml
[0m16:50:12.635865 [debug] [MainThread]: Partial parsing: added file: pipeline://models/universal/schema.yml
[0m16:50:12.636144 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:50:12.636489 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/schema.yml
[0m16:50:12.781193 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two schema.yml entries for the same resource named stg__player_games. Resources and their associated columns may only be described a single time. To fix this, remove the resource entry for stg__player_games in one of these files:
   - models/staging/schema.yml
  models/schema.yml
[0m16:50:12.781875 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.1665573, "process_in_blocks": "0", "process_kernel_time": 0.302123, "process_mem_max_rss": "221744", "process_out_blocks": "8", "process_user_time": 3.127589}
[0m16:50:12.782357 [debug] [MainThread]: Command `dbt compile` failed at 16:50:12.782273 after 1.17 seconds
[0m16:50:12.782675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d37b952d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9cff76ff50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d37beef50>]}
[0m16:50:12.782986 [debug] [MainThread]: Flushing usage events
[0m16:50:13.150960 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:51:01.713282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f960a59e190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f960a594390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f960a7e2390>]}


============================== 16:51:01.716492 | bdf4cef6-4057-49e3-8ee6-1d5d5e20e39b ==============================
[0m16:51:01.716492 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:51:01.716965 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:51:02.371720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bdf4cef6-4057-49e3-8ee6-1d5d5e20e39b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f95d2eabbd0>]}
[0m16:51:02.492674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bdf4cef6-4057-49e3-8ee6-1d5d5e20e39b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f960cfcd5d0>]}
[0m16:51:02.493269 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:51:02.613458 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:51:02.732902 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 4 files added, 0 files changed.
[0m16:51:02.733492 [debug] [MainThread]: Partial parsing: added file: pipeline://models/universal/schema.yml
[0m16:51:02.733775 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:51:02.734116 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/schema.yml
[0m16:51:02.734428 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/schema.yml
[0m16:51:03.167261 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "chess_raw_games".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("chess_raw", "games").
  
  To fix this, change the name of one of these resources:
  - source.pipeline.chess_raw.games (models/marts/schema.yml)
  - source.pipeline.chess_raw.games (models/universal/schema.yml)
[0m16:51:03.167990 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.5006729, "process_in_blocks": "0", "process_kernel_time": 0.340292, "process_mem_max_rss": "225928", "process_out_blocks": "16", "process_user_time": 3.347564}
[0m16:51:03.168382 [debug] [MainThread]: Command `dbt compile` failed at 16:51:03.168296 after 1.50 seconds
[0m16:51:03.168735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f960a594390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f960a5941d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f95d2263e10>]}
[0m16:51:03.169091 [debug] [MainThread]: Flushing usage events
[0m16:51:03.509956 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:52:19.723321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2410a3a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2410a0c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2410a0650>]}


============================== 16:52:19.726352 | 610b8012-65ab-43f7-8ea7-f62ecb7aebd6 ==============================
[0m16:52:19.726352 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:52:19.726803 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:52:20.341339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '610b8012-65ab-43f7-8ea7-f62ecb7aebd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2410f2b10>]}
[0m16:52:20.448792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '610b8012-65ab-43f7-8ea7-f62ecb7aebd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe243a4da50>]}
[0m16:52:20.449336 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:52:20.567804 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:52:20.674667 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 4 files added, 0 files changed.
[0m16:52:20.675174 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/schema.yml
[0m16:52:20.675554 [debug] [MainThread]: Partial parsing: added file: pipeline://models/universal/schema.yml
[0m16:52:20.675819 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:52:20.676139 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/schema.yml
[0m16:52:21.071198 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "chess_raw_games".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("chess_raw", "games").
  
  To fix this, change the name of one of these resources:
  - source.pipeline.chess_raw.games (models/universal/schema.yml)
  - source.pipeline.chess_raw.games (models/staging/schema.yml)
[0m16:52:21.071879 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": false, "command_wall_clock_time": 1.3950832, "process_in_blocks": "0", "process_kernel_time": 0.295975, "process_mem_max_rss": "225708", "process_out_blocks": "16", "process_user_time": 3.348436}
[0m16:52:21.072238 [debug] [MainThread]: Command `dbt compile` failed at 16:52:21.072153 after 1.40 seconds
[0m16:52:21.072553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2410abd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe20cb19110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2410abf10>]}
[0m16:52:21.072862 [debug] [MainThread]: Flushing usage events
[0m16:52:21.437085 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:53:19.084434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bac94d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1baede490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bac97790>]}


============================== 16:53:19.087432 | 6fa341d0-9aec-40be-82ea-9903d2b53d78 ==============================
[0m16:53:19.087432 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:53:19.087875 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt compile', 'send_anonymous_usage_stats': 'True'}
[0m16:53:19.711457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6fa341d0-9aec-40be-82ea-9903d2b53d78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bacef950>]}
[0m16:53:19.832465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6fa341d0-9aec-40be-82ea-9903d2b53d78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bd6cd110>]}
[0m16:53:19.833065 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:53:19.951529 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:53:20.060732 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 4 files added, 0 files changed.
[0m16:53:20.061273 [debug] [MainThread]: Partial parsing: added file: pipeline://models/marts/schema.yml
[0m16:53:20.061654 [debug] [MainThread]: Partial parsing: added file: pipeline://models/universal/schema.yml
[0m16:53:20.061962 [debug] [MainThread]: Partial parsing: added file: pipeline://models/staging/schema.yml
[0m16:53:20.062226 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:53:20.496100 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' (tests/test_color_pivot_total-weekly_chess_player_metrics.sql) depends on a node named 'games_agg' in package '' which was not found
[0m16:53:20.559157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6fa341d0-9aec-40be-82ea-9903d2b53d78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff182891d50>]}
[0m16:53:20.639181 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:53:20.641324 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:53:20.649863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6fa341d0-9aec-40be-82ea-9903d2b53d78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff182670b10>]}
[0m16:53:20.650297 [info ] [MainThread]: Found 1 seed, 4 models, 3 data tests, 1 source, 492 macros
[0m16:53:20.650588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6fa341d0-9aec-40be-82ea-9903d2b53d78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1826831d0>]}
[0m16:53:20.652091 [info ] [MainThread]: 
[0m16:53:20.652376 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:53:20.652645 [info ] [MainThread]: 
[0m16:53:20.653027 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:53:20.656863 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:53:20.657794 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:53:20.658285 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:20.659087 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:53:20.659604 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:20.660458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:21.566596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6fa341d0-9aec-40be-82ea-9903d2b53d78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff18299ac50>]}
[0m16:53:21.567307 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:53:21.570741 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:53:21.571197 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:53:21.571806 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m16:53:21.572219 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m16:53:21.572609 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m16:53:21.573009 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now seed.pipeline.opening_mapping)
[0m16:53:21.573356 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:53:21.573854 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:53:21.574441 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m16:53:21.581868 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:53:21.591871 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:53:21.594096 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m16:53:21.594776 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:53:21.596175 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m16:53:21.597020 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:53:21.597612 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:53:21.598257 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:53:21.598907 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:53:21.600197 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:53:21.601492 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:53:21.602666 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:53:21.603151 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:53:21.603609 [debug] [Thread-2 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m16:53:21.604381 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m16:53:21.604934 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m16:53:21.605426 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now test.pipeline.test_game_count-stg__player_games)
[0m16:53:21.606061 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:53:21.606554 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:53:21.606972 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m16:53:21.615813 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:53:21.622987 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:53:21.628619 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m16:53:21.629774 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:53:21.630304 [debug] [Thread-2 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m16:53:21.630911 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:53:21.631505 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:53:21.632088 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:53:21.632712 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:53:21.634029 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:53:21.635241 [debug] [Thread-2 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m16:53:21.636479 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:53:21.637502 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:53:21.638307 [debug] [Thread-2 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:53:21.638918 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m16:53:21.639575 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.pipeline.test_game_count-stg__player_games, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m16:53:21.640197 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:53:21.640807 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:53:21.646165 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m16:53:21.651931 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m16:53:21.652923 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:53:21.653522 [debug] [Thread-2 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:53:21.654114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:53:21.654741 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:53:21.656075 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:53:21.657303 [debug] [Thread-2 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:53:21.659035 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:53:21.659605 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m16:53:21.660041 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m16:53:21.660477 [debug] [MainThread]: Connection 'model.pipeline.weekly_chess_player_metrics' was properly closed.
[0m16:53:21.660897 [debug] [MainThread]: Connection 'model.pipeline.quarterly_chess_player_metrics' was properly closed.
[0m16:53:21.663313 [debug] [MainThread]: Command end result
[0m16:53:21.702999 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:53:21.705039 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:53:21.712261 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:53:21.712826 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.6703293, "process_in_blocks": "0", "process_kernel_time": 0.309561, "process_mem_max_rss": "230876", "process_out_blocks": "3592", "process_user_time": 3.686395}
[0m16:53:21.713182 [debug] [MainThread]: Command `dbt compile` succeeded at 16:53:21.713103 after 2.67 seconds
[0m16:53:21.713486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bac8c1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bece1f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff1bac83e90>]}
[0m16:53:21.713795 [debug] [MainThread]: Flushing usage events
[0m16:53:22.060512 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:54:50.479587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fba5f7110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fba5f64d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fba5f6fd0>]}


============================== 16:54:50.482515 | b1109df9-2b87-4685-b419-6ff9fd6a9315 ==============================
[0m16:54:50.482515 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:54:50.482957 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt compile', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:54:51.100694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b1109df9-2b87-4685-b419-6ff9fd6a9315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f828d4810>]}
[0m16:54:51.209853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b1109df9-2b87-4685-b419-6ff9fd6a9315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fbcfcdb50>]}
[0m16:54:51.210409 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:54:51.325854 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:54:51.432227 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:54:51.432764 [debug] [MainThread]: Partial parsing: updated file: pipeline://tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:54:51.588601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b1109df9-2b87-4685-b419-6ff9fd6a9315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f823ff510>]}
[0m16:54:51.663363 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:54:51.665435 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:54:51.673894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b1109df9-2b87-4685-b419-6ff9fd6a9315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fbc200150>]}
[0m16:54:51.674327 [info ] [MainThread]: Found 1 seed, 4 models, 4 data tests, 1 source, 492 macros
[0m16:54:51.674653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b1109df9-2b87-4685-b419-6ff9fd6a9315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fbc1f7e50>]}
[0m16:54:51.676166 [info ] [MainThread]: 
[0m16:54:51.676466 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:54:51.676713 [info ] [MainThread]: 
[0m16:54:51.677107 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:54:51.680874 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:54:51.681770 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:54:51.682236 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:54:51.683004 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:54:51.683535 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:54:51.684049 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:54:52.547507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b1109df9-2b87-4685-b419-6ff9fd6a9315', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f82202490>]}
[0m16:54:52.548166 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:54:52.551093 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:54:52.551798 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:54:52.552509 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m16:54:52.553147 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m16:54:52.553821 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m16:54:52.554463 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m16:54:52.554986 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:54:52.555495 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:54:52.556087 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m16:54:52.566795 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:54:52.586138 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m16:54:52.588158 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:54:52.588980 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:54:52.589568 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:54:52.590845 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m16:54:52.591405 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:54:52.591895 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:54:52.592932 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:54:52.593410 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:54:52.594703 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:54:52.595621 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:54:52.596191 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:54:52.596768 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m16:54:52.597203 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m16:54:52.597827 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m16:54:52.598324 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:54:52.598821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test_game_count-stg__player_games)
[0m16:54:52.599512 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:54:52.605370 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:54:52.605889 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m16:54:52.613289 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:54:52.618612 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m16:54:52.619421 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:54:52.620131 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:54:52.620754 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:54:52.621342 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m16:54:52.621878 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:54:52.623163 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:54:52.623750 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:54:52.624896 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:54:52.626317 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m16:54:52.626821 [debug] [Thread-2 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:54:52.627838 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m16:54:52.628413 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:54:52.628974 [debug] [Thread-4 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:54:52.629578 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:54:52.635484 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m16:54:52.636181 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.quarterly_chess_player_metrics, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m16:54:52.636795 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.weekly_chess_player_metrics, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m16:54:52.637518 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:54:52.638145 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:54:52.642420 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m16:54:52.643004 [debug] [Thread-2 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:54:52.647898 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m16:54:52.648690 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:54:52.649306 [debug] [Thread-4 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:54:52.650645 [debug] [Thread-2 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:54:52.651267 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:54:52.651789 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:54:52.653105 [debug] [Thread-4 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:54:52.653686 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:54:52.655085 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:54:52.656612 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:54:52.657147 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m16:54:52.657569 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m16:54:52.657892 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-stg__player_games' was properly closed.
[0m16:54:52.658205 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' was properly closed.
[0m16:54:52.660136 [debug] [MainThread]: Command end result
[0m16:54:52.681289 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:54:52.682416 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:54:52.687800 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:54:52.688336 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.255658, "process_in_blocks": "0", "process_kernel_time": 0.306903, "process_mem_max_rss": "225360", "process_out_blocks": "3600", "process_user_time": 3.375529}
[0m16:54:52.688704 [debug] [MainThread]: Command `dbt compile` succeeded at 16:54:52.688626 after 2.26 seconds
[0m16:54:52.688998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fba594110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fba5f62d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fba8fc210>]}
[0m16:54:52.689332 [debug] [MainThread]: Flushing usage events
[0m16:54:53.055176 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:58:58.242258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9015a84c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9015a84690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9015a85350>]}


============================== 16:58:58.245123 | 3bad45e5-35c1-476b-ace1-e977016ad4c5 ==============================
[0m16:58:58.245123 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:58:58.246776 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt compile', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:58:58.908085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3bad45e5-35c1-476b-ace1-e977016ad4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fde7821d0>]}
[0m16:58:59.027617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3bad45e5-35c1-476b-ace1-e977016ad4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f90184cd890>]}
[0m16:58:59.028157 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:58:59.151587 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:58:59.269331 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m16:58:59.269850 [debug] [MainThread]: Partial parsing: added file: pipeline://tests/test_color_pivot_total-quarterly_chess_player_metrics.sql
[0m16:58:59.437209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3bad45e5-35c1-476b-ace1-e977016ad4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fddc1b010>]}
[0m16:58:59.520536 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:58:59.522610 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:58:59.531228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3bad45e5-35c1-476b-ace1-e977016ad4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fdd623fd0>]}
[0m16:58:59.531670 [info ] [MainThread]: Found 1 seed, 4 models, 5 data tests, 1 source, 492 macros
[0m16:58:59.531950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3bad45e5-35c1-476b-ace1-e977016ad4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fdd7bf950>]}
[0m16:58:59.533485 [info ] [MainThread]: 
[0m16:58:59.533762 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:58:59.533997 [info ] [MainThread]: 
[0m16:58:59.534378 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:58:59.537993 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:58:59.538644 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:58:59.539242 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:58:59.539571 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:59.540029 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:59.540341 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:00.386332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3bad45e5-35c1-476b-ace1-e977016ad4c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8fdda2e090>]}
[0m16:59:00.386727 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:59:00.389924 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m16:59:00.390516 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m16:59:00.391072 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m16:59:00.391690 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.pipeline.calendar)
[0m16:59:00.392306 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.stg__player_games)
[0m16:59:00.392861 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now seed.pipeline.opening_mapping)
[0m16:59:00.393317 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m16:59:00.393886 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m16:59:00.394317 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m16:59:00.413156 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m16:59:00.424970 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m16:59:00.429195 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m16:59:00.430236 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:59:00.430843 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m16:59:00.431357 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m16:59:00.432619 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m16:59:00.433196 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:59:00.433770 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:59:00.435099 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m16:59:00.436259 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m16:59:00.437062 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m16:59:00.437961 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.pipeline.test_game_count-stg__player_games'
[0m16:59:00.438606 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m16:59:00.439106 [debug] [Thread-2 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m16:59:00.444160 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m16:59:00.444828 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m16:59:00.445402 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now model.pipeline.quarterly_chess_player_metrics)
[0m16:59:00.446304 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m16:59:00.446980 [debug] [Thread-2 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m16:59:00.447593 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m16:59:00.448108 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m16:59:00.456093 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m16:59:00.456712 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:59:00.464016 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m16:59:00.465646 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m16:59:00.466563 [debug] [Thread-2 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m16:59:00.467120 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m16:59:00.467747 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:59:00.468351 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:59:00.469554 [debug] [Thread-2 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m16:59:00.470693 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m16:59:00.471652 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:00.472341 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.weekly_chess_player_metrics, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m16:59:00.473101 [debug] [Thread-4 (]: Began running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:00.473623 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:00.474172 [debug] [Thread-2 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:00.474761 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:00.475304 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.pipeline.test_game_count-stg__player_games, now test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics)
[0m16:59:00.480645 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m16:59:00.481320 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.quarterly_chess_player_metrics, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m16:59:00.481928 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m16:59:00.482532 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:00.483207 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:00.483827 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:00.484408 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:00.488646 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m16:59:00.492767 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m16:59:00.497611 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m16:59:00.498258 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:59:00.499112 [debug] [Thread-4 (]: Began executing node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:00.500568 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:00.501184 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:59:00.501710 [debug] [Thread-2 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:00.502225 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:00.503569 [debug] [Thread-4 (]: Finished running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:00.504141 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:59:00.504720 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:59:00.506016 [debug] [Thread-2 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:00.507352 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:00.509015 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:59:00.509630 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m16:59:00.510118 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m16:59:00.510598 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' was properly closed.
[0m16:59:00.511058 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics' was properly closed.
[0m16:59:00.514183 [debug] [MainThread]: Command end result
[0m16:59:00.555712 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:59:00.557565 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:59:00.563360 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:59:00.563927 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.3679416, "process_in_blocks": "0", "process_kernel_time": 0.32296, "process_mem_max_rss": "225568", "process_out_blocks": "3640", "process_user_time": 3.498722}
[0m16:59:00.564274 [debug] [MainThread]: Command `dbt compile` succeeded at 16:59:00.564195 after 2.37 seconds
[0m16:59:00.564574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9015ae6850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9015a7c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f901679af50>]}
[0m16:59:00.564875 [debug] [MainThread]: Flushing usage events
[0m16:59:00.921550 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:59:10.751368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b189bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b189b990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b1898350>]}


============================== 16:59:10.754371 | 2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978 ==============================
[0m16:59:10.754371 [info ] [MainThread]: Running with dbt=1.9.4
[0m16:59:10.754811 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m16:59:11.375472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b27052d0>]}
[0m16:59:11.483553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b42cdb10>]}
[0m16:59:11.484094 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m16:59:11.601886 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m16:59:11.707931 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:59:11.708272 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:59:11.752713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1479812bd0>]}
[0m16:59:11.829478 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:59:11.831544 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:59:11.847256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f147947b250>]}
[0m16:59:11.847712 [info ] [MainThread]: Found 1 seed, 4 models, 5 data tests, 1 source, 492 macros
[0m16:59:11.847986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1479631ed0>]}
[0m16:59:11.849542 [info ] [MainThread]: 
[0m16:59:11.849842 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:59:11.850078 [info ] [MainThread]: 
[0m16:59:11.850478 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m16:59:11.854291 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m16:59:11.855226 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m16:59:11.855741 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:11.856624 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m16:59:11.857094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:11.857912 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:12.737007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2dd00cbf-eb6d-4ef0-952c-c3ffdedd3978', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14798b2210>]}
[0m16:59:12.737707 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:59:12.741063 [debug] [Thread-1 (]: Began running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:12.741695 [debug] [Thread-2 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:12.742240 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:12.743257 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m16:59:12.742879 [info ] [Thread-1 (]: 1 of 5 START test test_color_pivot_total-quarterly_chess_player_metrics ........ [RUN]
[0m16:59:12.743766 [info ] [Thread-2 (]: 2 of 5 START test test_color_pivot_total-weekly_chess_player_metrics ........... [RUN]
[0m16:59:12.744409 [info ] [Thread-3 (]: 3 of 5 START test test_game_count-quarterly_chess_player_metrics ............... [RUN]
[0m16:59:12.744914 [info ] [Thread-4 (]: 4 of 5 START test test_game_count-stg__player_games ............................ [RUN]
[0m16:59:12.745333 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics)
[0m16:59:12.745729 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m16:59:12.746162 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m16:59:12.746702 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.pipeline.test_game_count-stg__player_games'
[0m16:59:12.746989 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:12.747270 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:12.747686 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:12.748116 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m16:59:12.757378 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m16:59:12.760695 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m16:59:12.766566 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m16:59:12.771627 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m16:59:12.772629 [debug] [Thread-1 (]: Began executing node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:12.773075 [debug] [Thread-2 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:12.794283 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:12.814207 [debug] [Thread-2 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m16:59:12.814857 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m16:59:12.817005 [debug] [Thread-1 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m16:59:12.820437 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m16:59:12.823895 [debug] [Thread-4 (]: Writing runtime sql for node "test.pipeline.test_game_count-stg__player_games"
[0m16:59:12.825286 [debug] [Thread-3 (]: On test.pipeline.test_game_count-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
WHERE 1=1
  AND quarter_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
      
    ) dbt_internal_test
[0m16:59:12.826057 [debug] [Thread-2 (]: On test.pipeline.test_color_pivot_total-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date, username, time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND total_games != white_games + black_games
      
    ) dbt_internal_test
[0m16:59:12.826781 [debug] [Thread-4 (]: On test.pipeline.test_game_count-stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-stg__player_games"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')

EXCEPT DISTINCT

SELECT
    CAST(COUNT(*)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_staging`.`stg__player_games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')
      
    ) dbt_internal_test
[0m16:59:12.827516 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:59:12.828208 [debug] [Thread-1 (]: On test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date, username, time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND total_games != white_games + black_games
      
    ) dbt_internal_test
[0m16:59:12.828855 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m16:59:12.829478 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:59:12.830558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:59:13.189649 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6072d2b8-828a-468a-adcc-5a4f702fb26b&page=queryresults
[0m16:59:13.190473 [error] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6072d2b8-828a-468a-adcc-5a4f702fb26b&page=queryresults
[0m16:59:13.237283 [debug] [Thread-1 (]: Database Error in test test_color_pivot_total-quarterly_chess_player_metrics (tests/test_color_pivot_total-quarterly_chess_player_metrics.sql)
  Unrecognized name: white_games at [12:22]
  compiled code at target/run/pipeline/tests/test_color_pivot_total-quarterly_chess_player_metrics.sql
[0m16:59:13.238171 [error] [Thread-1 (]: 1 of 5 ERROR test_color_pivot_total-quarterly_chess_player_metrics ............. [[31mERROR[0m in 0.49s]
[0m16:59:13.239087 [debug] [Thread-1 (]: Finished running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m16:59:13.239698 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:13.240479 [debug] [Thread-7 (]: Marking all children of 'test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in test test_color_pivot_total-quarterly_chess_player_metrics (tests/test_color_pivot_total-quarterly_chess_player_metrics.sql)
  Unrecognized name: white_games at [12:22]
  compiled code at target/run/pipeline/tests/test_color_pivot_total-quarterly_chess_player_metrics.sql.
[0m16:59:13.241134 [info ] [Thread-1 (]: 5 of 5 START test test_game_count-weekly_chess_player_metrics .................. [RUN]
[0m16:59:13.242792 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m16:59:13.243351 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:13.248940 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m16:59:13.250463 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:13.251234 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0b10ed20-42cd-413d-be6b-cba90fdafb13&page=queryresults
[0m16:59:13.254643 [debug] [Thread-1 (]: Writing runtime sql for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m16:59:13.255384 [error] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0b10ed20-42cd-413d-be6b-cba90fdafb13&page=queryresults
[0m16:59:13.258583 [debug] [Thread-1 (]: On test.pipeline.test_game_count-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
      
    ) dbt_internal_test
[0m16:59:13.260963 [debug] [Thread-2 (]: Database Error in test test_color_pivot_total-weekly_chess_player_metrics (tests/test_color_pivot_total-weekly_chess_player_metrics.sql)
  Unrecognized name: white_games at [12:22]
  compiled code at target/run/pipeline/tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:59:13.261532 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:59:13.262172 [error] [Thread-2 (]: 2 of 5 ERROR test_color_pivot_total-weekly_chess_player_metrics ................ [[31mERROR[0m in 0.52s]
[0m16:59:13.264441 [debug] [Thread-2 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m16:59:13.265369 [debug] [Thread-7 (]: Marking all children of 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' to be skipped because of status 'error'.  Reason: Database Error in test test_color_pivot_total-weekly_chess_player_metrics (tests/test_color_pivot_total-weekly_chess_player_metrics.sql)
  Unrecognized name: white_games at [12:22]
  compiled code at target/run/pipeline/tests/test_color_pivot_total-weekly_chess_player_metrics.sql.
[0m16:59:13.900028 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0b7aed39-14f4-4702-9994-d492aae64acf&page=queryresults
[0m16:59:13.954869 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:9a51b7cf-5758-48c3-af24-ec0cef24f73c&page=queryresults
[0m16:59:14.124168 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:3c5e9eb7-42d8-44f3-8592-cc89cab9ccfd&page=queryresults
[0m16:59:14.365896 [info ] [Thread-3 (]: 3 of 5 PASS test_game_count-quarterly_chess_player_metrics ..................... [[32mPASS[0m in 1.62s]
[0m16:59:14.366913 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m16:59:14.388301 [info ] [Thread-4 (]: 4 of 5 PASS test_game_count-stg__player_games .................................. [[32mPASS[0m in 1.64s]
[0m16:59:14.388997 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m16:59:14.497503 [info ] [Thread-1 (]: 5 of 5 PASS test_game_count-weekly_chess_player_metrics ........................ [[32mPASS[0m in 1.25s]
[0m16:59:14.498507 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m16:59:14.500277 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:59:14.501085 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:59:14.501557 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m16:59:14.501986 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' was properly closed.
[0m16:59:14.502420 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m16:59:14.502849 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-stg__player_games' was properly closed.
[0m16:59:14.503333 [info ] [MainThread]: 
[0m16:59:14.503823 [info ] [MainThread]: Finished running 5 data tests in 0 hours 0 minutes and 2.65 seconds (2.65s).
[0m16:59:14.505730 [debug] [MainThread]: Command end result
[0m16:59:14.545679 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m16:59:14.547737 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m16:59:14.554577 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m16:59:14.554834 [info ] [MainThread]: 
[0m16:59:14.555124 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m16:59:14.555375 [info ] [MainThread]: 
[0m16:59:14.555686 [error] [MainThread]:   Database Error in test test_color_pivot_total-quarterly_chess_player_metrics (tests/test_color_pivot_total-quarterly_chess_player_metrics.sql)
  Unrecognized name: white_games at [12:22]
  compiled code at target/run/pipeline/tests/test_color_pivot_total-quarterly_chess_player_metrics.sql
[0m16:59:14.555927 [info ] [MainThread]: 
[0m16:59:14.556214 [error] [MainThread]:   Database Error in test test_color_pivot_total-weekly_chess_player_metrics (tests/test_color_pivot_total-weekly_chess_player_metrics.sql)
  Unrecognized name: white_games at [12:22]
  compiled code at target/run/pipeline/tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m16:59:14.556463 [info ] [MainThread]: 
[0m16:59:14.556709 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 TOTAL=5
[0m16:59:14.557218 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 3.8528757, "process_in_blocks": "544", "process_kernel_time": 0.358691, "process_mem_max_rss": "231364", "process_out_blocks": "2416", "process_user_time": 3.414716}
[0m16:59:14.557563 [debug] [MainThread]: Command `dbt test` failed at 16:59:14.557487 after 3.85 seconds
[0m16:59:14.557874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b18f5010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b1893f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14b5277d10>]}
[0m16:59:14.558180 [debug] [MainThread]: Flushing usage events
[0m16:59:14.897765 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:07:18.316770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6e9d83990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6e9d81810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6e9d83a10>]}


============================== 17:07:18.319529 | 7f4a430b-0465-465e-8a26-f68771d33eb8 ==============================
[0m17:07:18.319529 [info ] [MainThread]: Running with dbt=1.9.4
[0m17:07:18.319963 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:07:18.999471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f4a430b-0465-465e-8a26-f68771d33eb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6b1e984d0>]}
[0m17:07:19.116220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f4a430b-0465-465e-8a26-f68771d33eb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6ec785350>]}
[0m17:07:19.116836 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m17:07:19.246676 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m17:07:19.366689 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:07:19.367255 [debug] [MainThread]: Partial parsing: updated file: pipeline://tests/test_color_pivot_total-quarterly_chess_player_metrics.sql
[0m17:07:19.367616 [debug] [MainThread]: Partial parsing: updated file: pipeline://tests/test_color_pivot_total-weekly_chess_player_metrics.sql
[0m17:07:19.549894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f4a430b-0465-465e-8a26-f68771d33eb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6b1ca3fd0>]}
[0m17:07:19.631544 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m17:07:19.633862 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m17:07:19.643331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f4a430b-0465-465e-8a26-f68771d33eb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6b198d310>]}
[0m17:07:19.643852 [info ] [MainThread]: Found 1 seed, 4 models, 5 data tests, 1 source, 492 macros
[0m17:07:19.644161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f4a430b-0465-465e-8a26-f68771d33eb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6b1c1efd0>]}
[0m17:07:19.645837 [info ] [MainThread]: 
[0m17:07:19.646143 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:07:19.646413 [info ] [MainThread]: 
[0m17:07:19.646827 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:07:19.650798 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m17:07:19.651311 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m17:07:19.651790 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m17:07:19.652182 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:19.652647 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:19.653002 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:20.601956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f4a430b-0465-465e-8a26-f68771d33eb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6b1b649d0>]}
[0m17:07:20.602645 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:07:20.605894 [debug] [Thread-1 (]: Began running node model.pipeline.calendar
[0m17:07:20.606372 [debug] [Thread-2 (]: Began running node model.pipeline.stg__player_games
[0m17:07:20.606992 [debug] [Thread-3 (]: Began running node seed.pipeline.opening_mapping
[0m17:07:20.607491 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.pipeline.calendar)
[0m17:07:20.607953 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.pipeline.stg__player_games)
[0m17:07:20.608579 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now seed.pipeline.opening_mapping)
[0m17:07:20.609063 [debug] [Thread-1 (]: Began compiling node model.pipeline.calendar
[0m17:07:20.609483 [debug] [Thread-2 (]: Began compiling node model.pipeline.stg__player_games
[0m17:07:20.610063 [debug] [Thread-3 (]: Began compiling node seed.pipeline.opening_mapping
[0m17:07:20.619394 [debug] [Thread-1 (]: Writing injected SQL for node "model.pipeline.calendar"
[0m17:07:20.632408 [debug] [Thread-3 (]: Began executing node seed.pipeline.opening_mapping
[0m17:07:20.639752 [debug] [Thread-2 (]: Writing injected SQL for node "model.pipeline.stg__player_games"
[0m17:07:20.640564 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:07:20.641146 [debug] [Thread-1 (]: Began executing node model.pipeline.calendar
[0m17:07:20.642500 [debug] [Thread-3 (]: Finished running node seed.pipeline.opening_mapping
[0m17:07:20.643090 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:07:20.643652 [debug] [Thread-2 (]: Began executing node model.pipeline.stg__player_games
[0m17:07:20.644999 [debug] [Thread-1 (]: Finished running node model.pipeline.calendar
[0m17:07:20.645516 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:07:20.646789 [debug] [Thread-2 (]: Finished running node model.pipeline.stg__player_games
[0m17:07:20.647883 [debug] [Thread-4 (]: Began running node model.pipeline.quarterly_chess_player_metrics
[0m17:07:20.648427 [debug] [Thread-3 (]: Began running node model.pipeline.weekly_chess_player_metrics
[0m17:07:20.649039 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.pipeline.quarterly_chess_player_metrics'
[0m17:07:20.649517 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m17:07:20.650210 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly seed.pipeline.opening_mapping, now model.pipeline.weekly_chess_player_metrics)
[0m17:07:20.650858 [debug] [Thread-4 (]: Began compiling node model.pipeline.quarterly_chess_player_metrics
[0m17:07:20.651318 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.pipeline.calendar, now test.pipeline.test_game_count-stg__player_games)
[0m17:07:20.651737 [debug] [Thread-3 (]: Began compiling node model.pipeline.weekly_chess_player_metrics
[0m17:07:20.659871 [debug] [Thread-4 (]: Writing injected SQL for node "model.pipeline.quarterly_chess_player_metrics"
[0m17:07:20.660487 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m17:07:20.667689 [debug] [Thread-3 (]: Writing injected SQL for node "model.pipeline.weekly_chess_player_metrics"
[0m17:07:20.672883 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m17:07:20.673620 [debug] [Thread-4 (]: Began executing node model.pipeline.quarterly_chess_player_metrics
[0m17:07:20.674467 [debug] [Thread-3 (]: Began executing node model.pipeline.weekly_chess_player_metrics
[0m17:07:20.675040 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m17:07:20.675706 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:07:20.677033 [debug] [Thread-4 (]: Finished running node model.pipeline.quarterly_chess_player_metrics
[0m17:07:20.677541 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m17:07:20.678719 [debug] [Thread-3 (]: Finished running node model.pipeline.weekly_chess_player_metrics
[0m17:07:20.679572 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:07:20.680173 [debug] [Thread-2 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:20.681517 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m17:07:20.682240 [debug] [Thread-3 (]: Began running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:20.682784 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.pipeline.stg__player_games, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m17:07:20.683307 [debug] [Thread-4 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:20.683914 [debug] [Thread-1 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:20.684542 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.pipeline.weekly_chess_player_metrics, now test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics)
[0m17:07:20.685116 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:20.685717 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.pipeline.quarterly_chess_player_metrics, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m17:07:20.686316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.pipeline.test_game_count-stg__player_games, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m17:07:20.686884 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:20.692110 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m17:07:20.692716 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:20.693182 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:20.697505 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:07:20.699980 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:07:20.704827 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m17:07:20.705647 [debug] [Thread-2 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:20.706442 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:07:20.707665 [debug] [Thread-2 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:20.708502 [debug] [Thread-3 (]: Began executing node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:20.709062 [debug] [Thread-1 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:20.709596 [debug] [Thread-4 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:20.710146 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:07:20.710725 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:07:20.711308 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m17:07:20.712497 [debug] [Thread-3 (]: Finished running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:20.713649 [debug] [Thread-1 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:20.715100 [debug] [Thread-4 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:20.716948 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:07:20.717303 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m17:07:20.717636 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m17:07:20.717949 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics' was properly closed.
[0m17:07:20.718272 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' was properly closed.
[0m17:07:20.720332 [debug] [MainThread]: Command end result
[0m17:07:20.743360 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m17:07:20.744534 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m17:07:20.753883 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m17:07:20.754595 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 2.480672, "process_in_blocks": "0", "process_kernel_time": 0.344276, "process_mem_max_rss": "225240", "process_out_blocks": "3640", "process_user_time": 3.504543}
[0m17:07:20.755053 [debug] [MainThread]: Command `dbt compile` succeeded at 17:07:20.754951 after 2.48 seconds
[0m17:07:20.755484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6e9de0dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6ed9ffd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6ed9fffd0>]}
[0m17:07:20.755972 [debug] [MainThread]: Flushing usage events
[0m17:07:21.122540 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:07:24.881254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76aa5835d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76aa583190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76aa581f50>]}


============================== 17:07:24.884301 | e1dec1d4-c636-419a-8af9-27b7b88ad4b1 ==============================
[0m17:07:24.884301 [info ] [MainThread]: Running with dbt=1.9.4
[0m17:07:24.884752 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m17:07:25.556099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e1dec1d4-c636-419a-8af9-27b7b88ad4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7672e31a90>]}
[0m17:07:25.680333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e1dec1d4-c636-419a-8af9-27b7b88ad4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76acf758d0>]}
[0m17:07:25.680985 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m17:07:25.813037 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m17:07:25.934606 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:07:25.934998 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:07:25.987911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e1dec1d4-c636-419a-8af9-27b7b88ad4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7672423290>]}
[0m17:07:26.077121 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m17:07:26.079423 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m17:07:26.096957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e1dec1d4-c636-419a-8af9-27b7b88ad4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76725026d0>]}
[0m17:07:26.097450 [info ] [MainThread]: Found 1 seed, 4 models, 5 data tests, 1 source, 492 macros
[0m17:07:26.097765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1dec1d4-c636-419a-8af9-27b7b88ad4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76727255d0>]}
[0m17:07:26.099477 [info ] [MainThread]: 
[0m17:07:26.099786 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:07:26.100048 [info ] [MainThread]: 
[0m17:07:26.100490 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:07:26.104548 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m17:07:26.105099 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m17:07:26.105661 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m17:07:26.106082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:26.106551 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:26.106910 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:27.010158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1dec1d4-c636-419a-8af9-27b7b88ad4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76724ccb90>]}
[0m17:07:27.010877 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:07:27.014378 [debug] [Thread-1 (]: Began running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:27.014886 [debug] [Thread-2 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:27.015445 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:27.016326 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m17:07:27.015978 [info ] [Thread-1 (]: 1 of 5 START test test_color_pivot_total-quarterly_chess_player_metrics ........ [RUN]
[0m17:07:27.016908 [info ] [Thread-2 (]: 2 of 5 START test test_color_pivot_total-weekly_chess_player_metrics ........... [RUN]
[0m17:07:27.017602 [info ] [Thread-3 (]: 3 of 5 START test test_game_count-quarterly_chess_player_metrics ............... [RUN]
[0m17:07:27.018150 [info ] [Thread-4 (]: 4 of 5 START test test_game_count-stg__player_games ............................ [RUN]
[0m17:07:27.018684 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics)
[0m17:07:27.019120 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m17:07:27.019664 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m17:07:27.020345 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.pipeline.test_game_count-stg__player_games'
[0m17:07:27.020761 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:27.021119 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:27.021611 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:27.022095 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m17:07:27.031636 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:07:27.036001 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:07:27.042002 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m17:07:27.047371 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m17:07:27.048280 [debug] [Thread-1 (]: Began executing node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:27.049031 [debug] [Thread-2 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:27.054684 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:27.072372 [debug] [Thread-1 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:07:27.075740 [debug] [Thread-2 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:07:27.076414 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m17:07:27.079640 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m17:07:27.083268 [debug] [Thread-4 (]: Writing runtime sql for node "test.pipeline.test_game_count-stg__player_games"
[0m17:07:27.083959 [debug] [Thread-1 (]: On test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
      
    ) dbt_internal_test
[0m17:07:27.084735 [debug] [Thread-2 (]: On test.pipeline.test_color_pivot_total-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
      
    ) dbt_internal_test
[0m17:07:27.085516 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:07:27.086102 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:07:27.087047 [debug] [Thread-3 (]: On test.pipeline.test_game_count-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
WHERE 1=1
  AND quarter_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
      
    ) dbt_internal_test
[0m17:07:27.087910 [debug] [Thread-4 (]: On test.pipeline.test_game_count-stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-stg__player_games"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')

EXCEPT DISTINCT

SELECT
    CAST(COUNT(*)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_staging`.`stg__player_games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')
      
    ) dbt_internal_test
[0m17:07:27.091694 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:07:27.092154 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m17:07:27.734198 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d08b0306-ec2d-45d1-aa7b-374bee7a90e4&page=queryresults
[0m17:07:27.827738 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7c7e6445-057b-4d65-bb4f-1256e37254f9&page=queryresults
[0m17:07:28.193896 [info ] [Thread-3 (]: 3 of 5 PASS test_game_count-quarterly_chess_player_metrics ..................... [[32mPASS[0m in 1.17s]
[0m17:07:28.194672 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m17:07:28.195102 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:28.195591 [info ] [Thread-3 (]: 5 of 5 START test test_game_count-weekly_chess_player_metrics .................. [RUN]
[0m17:07:28.196083 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.pipeline.test_game_count-quarterly_chess_player_metrics, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m17:07:28.196560 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:28.201276 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m17:07:28.201950 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:28.204352 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m17:07:28.204872 [debug] [Thread-3 (]: On test.pipeline.test_game_count-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
      
    ) dbt_internal_test
[0m17:07:28.205239 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:07:28.247939 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:5b547b9e-85a4-4ef9-b5b3-15d57a6df86f&page=queryresults
[0m17:07:28.260017 [info ] [Thread-4 (]: 4 of 5 PASS test_game_count-stg__player_games .................................. [[32mPASS[0m in 1.24s]
[0m17:07:28.261009 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m17:07:28.386142 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bf588931-e65c-4b7e-94e2-e76252b9491d&page=queryresults
[0m17:07:28.626322 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:aa7580fc-50b6-463d-b603-8f1b74768138&page=queryresults
[0m17:07:28.701656 [info ] [Thread-1 (]: 1 of 5 PASS test_color_pivot_total-quarterly_chess_player_metrics .............. [[32mPASS[0m in 1.68s]
[0m17:07:28.702675 [debug] [Thread-1 (]: Finished running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:07:28.828384 [info ] [Thread-2 (]: 2 of 5 PASS test_color_pivot_total-weekly_chess_player_metrics ................. [[32mPASS[0m in 1.81s]
[0m17:07:28.829528 [debug] [Thread-2 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m17:07:29.009217 [info ] [Thread-3 (]: 5 of 5 PASS test_game_count-weekly_chess_player_metrics ........................ [[32mPASS[0m in 0.81s]
[0m17:07:29.009796 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m17:07:29.011286 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:07:29.012167 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:07:29.012636 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m17:07:29.013075 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-weekly_chess_player_metrics' was properly closed.
[0m17:07:29.013523 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics' was properly closed.
[0m17:07:29.013949 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-stg__player_games' was properly closed.
[0m17:07:29.014480 [info ] [MainThread]: 
[0m17:07:29.014968 [info ] [MainThread]: Finished running 5 data tests in 0 hours 0 minutes and 2.91 seconds (2.91s).
[0m17:07:29.016883 [debug] [MainThread]: Command end result
[0m17:07:29.057900 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m17:07:29.059951 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m17:07:29.066697 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m17:07:29.066969 [info ] [MainThread]: 
[0m17:07:29.067272 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:07:29.067520 [info ] [MainThread]: 
[0m17:07:29.067776 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m17:07:29.068287 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 4.2344146, "process_in_blocks": "0", "process_kernel_time": 0.327354, "process_mem_max_rss": "230792", "process_out_blocks": "2416", "process_user_time": 3.522127}
[0m17:07:29.068639 [debug] [MainThread]: Command `dbt test` succeeded at 17:07:29.068562 after 4.23 seconds
[0m17:07:29.068927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76aa578b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76aa578050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f76aa57bf90>]}
[0m17:07:29.069236 [debug] [MainThread]: Flushing usage events
[0m17:07:29.419062 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:10:59.560999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa715c97710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa715c972d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa715c97b10>]}


============================== 14:10:59.572787 | 9ae02592-0c89-4503-9e36-45abdeb5cdef ==============================
[0m14:10:59.572787 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:10:59.573271 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:11:02.359871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ae02592-0c89-4503-9e36-45abdeb5cdef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6df495fd0>]}
[0m14:11:02.466933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ae02592-0c89-4503-9e36-45abdeb5cdef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7186cce50>]}
[0m14:11:02.467486 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:11:02.617038 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:11:02.867922 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:11:02.868548 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/schema.yml
[0m14:11:03.228928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ae02592-0c89-4503-9e36-45abdeb5cdef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6dd8e6810>]}
[0m14:11:03.313972 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:11:03.317885 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:11:03.365430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ae02592-0c89-4503-9e36-45abdeb5cdef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6dd58e850>]}
[0m14:11:03.365865 [info ] [MainThread]: Found 1 seed, 4 models, 6 data tests, 1 source, 492 macros
[0m14:11:03.366139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ae02592-0c89-4503-9e36-45abdeb5cdef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6dd84c110>]}
[0m14:11:03.367728 [info ] [MainThread]: 
[0m14:11:03.368009 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:11:03.368248 [info ] [MainThread]: 
[0m14:11:03.368616 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:11:03.372928 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m14:11:03.373837 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m14:11:03.374288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:11:03.374883 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m14:11:03.375351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:11:03.375826 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:11:05.453821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ae02592-0c89-4503-9e36-45abdeb5cdef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6dd963150>]}
[0m14:11:05.454506 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:11:05.464447 [debug] [Thread-1 (]: Began running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:11:05.465073 [debug] [Thread-2 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:11:05.465673 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:11:05.466712 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m14:11:05.466290 [info ] [Thread-1 (]: 1 of 6 START test test_color_pivot_total-quarterly_chess_player_metrics ........ [RUN]
[0m14:11:05.467339 [info ] [Thread-2 (]: 2 of 6 START test test_color_pivot_total-weekly_chess_player_metrics ........... [RUN]
[0m14:11:05.467999 [info ] [Thread-3 (]: 3 of 6 START test test_game_count-quarterly_chess_player_metrics ............... [RUN]
[0m14:11:05.468601 [info ] [Thread-4 (]: 4 of 6 START test test_game_count-stg__player_games ............................ [RUN]
[0m14:11:05.469158 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics)
[0m14:11:05.469688 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m14:11:05.470165 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now test.pipeline.test_game_count-quarterly_chess_player_metrics)
[0m14:11:05.470774 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.pipeline.test_game_count-stg__player_games'
[0m14:11:05.471239 [debug] [Thread-1 (]: Began compiling node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:11:05.471714 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:11:05.472160 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:11:05.472640 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m14:11:05.486663 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m14:11:05.488322 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m14:11:05.493322 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m14:11:05.498502 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m14:11:05.499831 [debug] [Thread-2 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:11:05.500371 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m14:11:05.500851 [debug] [Thread-1 (]: Began executing node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:11:05.506424 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:11:05.539079 [debug] [Thread-4 (]: Writing runtime sql for node "test.pipeline.test_game_count-stg__player_games"
[0m14:11:05.545194 [debug] [Thread-2 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m14:11:05.548442 [debug] [Thread-1 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m14:11:05.551666 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m14:11:05.553165 [debug] [Thread-1 (]: On test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
      
    ) dbt_internal_test
[0m14:11:05.553961 [debug] [Thread-2 (]: On test.pipeline.test_color_pivot_total-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
      
    ) dbt_internal_test
[0m14:11:05.554576 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:11:05.555198 [debug] [Thread-3 (]: On test.pipeline.test_game_count-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
WHERE 1=1
  AND quarter_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
      
    ) dbt_internal_test
[0m14:11:05.555910 [debug] [Thread-4 (]: On test.pipeline.test_game_count-stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-stg__player_games"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')

EXCEPT DISTINCT

SELECT
    CAST(COUNT(*)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_staging`.`stg__player_games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')
      
    ) dbt_internal_test
[0m14:11:05.556531 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:11:05.557651 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:11:05.558263 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:11:06.165718 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bc1b393c-84a3-49ee-939f-2abc8de84054&page=queryresults
[0m14:11:06.170833 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:4b7ff907-eda4-49d4-828a-2c24ca5fd556&page=queryresults
[0m14:11:06.182165 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:07de301a-b5d4-416e-9c5c-4233a6f84924&page=queryresults
[0m14:11:06.226373 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:2ffefc73-ce7e-40be-8afe-9c5e1443ae47&page=queryresults
[0m14:11:06.679432 [info ] [Thread-2 (]: 2 of 6 PASS test_color_pivot_total-weekly_chess_player_metrics ................. [[32mPASS[0m in 1.21s]
[0m14:11:06.680468 [debug] [Thread-2 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:11:06.681091 [debug] [Thread-2 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:11:06.681753 [info ] [Thread-2 (]: 5 of 6 START test test_game_count-weekly_chess_player_metrics .................. [RUN]
[0m14:11:06.682430 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.pipeline.test_color_pivot_total-weekly_chess_player_metrics, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m14:11:06.683031 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:11:06.688729 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m14:11:06.689564 [debug] [Thread-2 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:11:06.693107 [debug] [Thread-2 (]: Writing runtime sql for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m14:11:06.694097 [debug] [Thread-2 (]: On test.pipeline.test_game_count-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
      
    ) dbt_internal_test
[0m14:11:06.694767 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:11:06.701353 [info ] [Thread-3 (]: 3 of 6 PASS test_game_count-quarterly_chess_player_metrics ..................... [[32mPASS[0m in 1.23s]
[0m14:11:06.702388 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:11:06.703063 [debug] [Thread-3 (]: Began running node test.pipeline.unique_stg__player_games_game_id.9c0d3fe971
[0m14:11:06.703765 [info ] [Thread-3 (]: 6 of 6 START test unique_stg__player_games_game_id ............................. [RUN]
[0m14:11:06.704482 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.pipeline.test_game_count-quarterly_chess_player_metrics, now test.pipeline.unique_stg__player_games_game_id.9c0d3fe971)
[0m14:11:06.705084 [debug] [Thread-3 (]: Began compiling node test.pipeline.unique_stg__player_games_game_id.9c0d3fe971
[0m14:11:06.721294 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.unique_stg__player_games_game_id.9c0d3fe971"
[0m14:11:06.722671 [debug] [Thread-3 (]: Began executing node test.pipeline.unique_stg__player_games_game_id.9c0d3fe971
[0m14:11:06.726551 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.unique_stg__player_games_game_id.9c0d3fe971"
[0m14:11:06.727644 [debug] [Thread-3 (]: On test.pipeline.unique_stg__player_games_game_id.9c0d3fe971: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.unique_stg__player_games_game_id.9c0d3fe971"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select game_id as unique_field
  from `checkmate-453316`.`dev_staging`.`stg__player_games`
  where game_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m14:11:06.728932 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:11:06.732276 [info ] [Thread-1 (]: 1 of 6 PASS test_color_pivot_total-quarterly_chess_player_metrics .............. [[32mPASS[0m in 1.26s]
[0m14:11:06.732987 [debug] [Thread-1 (]: Finished running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:11:06.739717 [info ] [Thread-4 (]: 4 of 6 PASS test_game_count-stg__player_games .................................. [[32mPASS[0m in 1.27s]
[0m14:11:06.740682 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m14:11:07.257282 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:88d60363-65e0-4382-8635-96856092d8e3&page=queryresults
[0m14:11:07.718093 [info ] [Thread-2 (]: 5 of 6 PASS test_game_count-weekly_chess_player_metrics ........................ [[32mPASS[0m in 1.04s]
[0m14:11:07.719107 [debug] [Thread-2 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:11:08.586587 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bb01b091-602c-4874-b2ce-210ee2b01ac5&page=queryresults
[0m14:11:09.155014 [error] [Thread-3 (]: 6 of 6 FAIL 2060718 unique_stg__player_games_game_id ........................... [[31mFAIL 2060718[0m in 2.45s]
[0m14:11:09.156027 [debug] [Thread-3 (]: Finished running node test.pipeline.unique_stg__player_games_game_id.9c0d3fe971
[0m14:11:09.157930 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:11:09.158821 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:11:09.159287 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics' was properly closed.
[0m14:11:09.159726 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m14:11:09.160154 [debug] [MainThread]: Connection 'test.pipeline.unique_stg__player_games_game_id.9c0d3fe971' was properly closed.
[0m14:11:09.160587 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-stg__player_games' was properly closed.
[0m14:11:09.161075 [info ] [MainThread]: 
[0m14:11:09.161569 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 5.79 seconds (5.79s).
[0m14:11:09.163762 [debug] [MainThread]: Command end result
[0m14:11:09.203371 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:11:09.205324 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:11:09.212266 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:11:09.212522 [info ] [MainThread]: 
[0m14:11:09.212806 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:11:09.213053 [info ] [MainThread]: 
[0m14:11:09.213361 [error] [MainThread]: [31mFailure in test unique_stg__player_games_game_id (models/staging/schema.yml)[0m
[0m14:11:09.213649 [error] [MainThread]:   Got 2060718 results, configured to fail if != 0
[0m14:11:09.213871 [info ] [MainThread]: 
[0m14:11:09.214144 [info ] [MainThread]:   compiled code at target/compiled/pipeline/models/staging/schema.yml/unique_stg__player_games_game_id.sql
[0m14:11:09.214401 [info ] [MainThread]: 
[0m14:11:09.214654 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
[0m14:11:09.215171 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 9.698752, "process_in_blocks": "277672", "process_kernel_time": 0.48164, "process_mem_max_rss": "235708", "process_out_blocks": "3576", "process_user_time": 3.965332}
[0m14:11:09.215517 [debug] [MainThread]: Command `dbt test` failed at 14:11:09.215442 after 9.70 seconds
[0m14:11:09.215811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa715c8c390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa715c95010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa715c95110>]}
[0m14:11:09.216108 [debug] [MainThread]: Flushing usage events
[0m14:11:09.581077 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:15:13.670128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f74587ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f745875d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f74584050>]}


============================== 14:15:13.673115 | 7a3d0510-0518-4458-aa0a-66966199a1c5 ==============================
[0m14:15:13.673115 [info ] [MainThread]: Running with dbt=1.9.4
[0m14:15:13.673562 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'log_path': '/home/filpill/projects/chess_analysis/dbt/pipeline/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:15:14.293446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a3d0510-0518-4458-aa0a-66966199a1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f40e31a90>]}
[0m14:15:14.400658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a3d0510-0518-4458-aa0a-66966199a1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f76f89650>]}
[0m14:15:14.401192 [info ] [MainThread]: Registered adapter: bigquery=1.9.1
[0m14:15:14.518622 [debug] [MainThread]: checksum: b5597158181700c50f5bc1c71fe28b37a6fb85633dc5323885263678b6bc782c, vars: {}, profile: , target: , version: 1.9.4
[0m14:15:14.643053 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:15:14.643888 [debug] [MainThread]: Partial parsing: updated file: pipeline://models/staging/schema.yml
[0m14:15:15.172138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a3d0510-0518-4458-aa0a-66966199a1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f401ebd10>]}
[0m14:15:15.268100 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:15:15.270321 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:15:15.287570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a3d0510-0518-4458-aa0a-66966199a1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f3bd3f090>]}
[0m14:15:15.288055 [info ] [MainThread]: Found 1 seed, 4 models, 6 data tests, 1 source, 492 macros
[0m14:15:15.288374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a3d0510-0518-4458-aa0a-66966199a1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f3bef4310>]}
[0m14:15:15.290103 [info ] [MainThread]: 
[0m14:15:15.290414 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:15:15.290705 [info ] [MainThread]: 
[0m14:15:15.291116 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m14:15:15.295366 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m14:15:15.296115 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m14:15:15.296693 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m14:15:15.297040 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:15.297553 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:15.297915 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:16.127029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a3d0510-0518-4458-aa0a-66966199a1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f40131490>]}
[0m14:15:16.127472 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:15:16.130414 [debug] [Thread-1 (]: Began running node test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13
[0m14:15:16.131011 [debug] [Thread-2 (]: Began running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:15:16.131539 [debug] [Thread-3 (]: Began running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:15:16.132396 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:15:16.132045 [info ] [Thread-1 (]: 1 of 6 START test source_unique_chess_raw_games_game_id ........................ [RUN]
[0m14:15:16.132891 [info ] [Thread-2 (]: 2 of 6 START test test_color_pivot_total-quarterly_chess_player_metrics ........ [RUN]
[0m14:15:16.133436 [info ] [Thread-3 (]: 3 of 6 START test test_color_pivot_total-weekly_chess_player_metrics ........... [RUN]
[0m14:15:16.134052 [info ] [Thread-4 (]: 4 of 6 START test test_game_count-quarterly_chess_player_metrics ............... [RUN]
[0m14:15:16.134540 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13)
[0m14:15:16.134906 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics)
[0m14:15:16.135425 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.pipeline.test_color_pivot_total-weekly_chess_player_metrics)
[0m14:15:16.136052 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.pipeline.test_game_count-quarterly_chess_player_metrics'
[0m14:15:16.136428 [debug] [Thread-1 (]: Began compiling node test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13
[0m14:15:16.136781 [debug] [Thread-2 (]: Began compiling node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:15:16.137265 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:15:16.137754 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:15:16.149081 [debug] [Thread-1 (]: Writing injected SQL for node "test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13"
[0m14:15:16.152887 [debug] [Thread-2 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m14:15:16.157109 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m14:15:16.163878 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m14:15:16.164679 [debug] [Thread-1 (]: Began executing node test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13
[0m14:15:16.165532 [debug] [Thread-2 (]: Began executing node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:15:16.188110 [debug] [Thread-1 (]: Writing runtime sql for node "test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13"
[0m14:15:16.191460 [debug] [Thread-2 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"
[0m14:15:16.192211 [debug] [Thread-3 (]: Began executing node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:15:16.192761 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:15:16.196158 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"
[0m14:15:16.196749 [debug] [Thread-1 (]: On test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select game_id as unique_field
  from `checkmate-453316`.`chess_raw`.`games`
  where game_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m14:15:16.200479 [debug] [Thread-4 (]: Writing runtime sql for node "test.pipeline.test_game_count-quarterly_chess_player_metrics"
[0m14:15:16.201190 [debug] [Thread-2 (]: On test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
      
    ) dbt_internal_test
[0m14:15:16.201965 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:15:16.202652 [debug] [Thread-3 (]: On test.pipeline.test_color_pivot_total-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_color_pivot_total-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
      
    ) dbt_internal_test
[0m14:15:16.203508 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:15:16.204703 [debug] [Thread-4 (]: On test.pipeline.test_game_count-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-quarterly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
WHERE 1=1
  AND quarter_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
      
    ) dbt_internal_test
[0m14:15:16.205402 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:16.208833 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:15:16.770487 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:127c303f-e42e-423c-bef8-f2b9bc486180&page=queryresults
[0m14:15:16.815054 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:0e082ad6-c91e-41a0-973b-5f2a15747b29&page=queryresults
[0m14:15:16.854858 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:663cc7c9-580c-42ef-a8ad-c924a76e4888&page=queryresults
[0m14:15:17.271808 [info ] [Thread-3 (]: 3 of 6 PASS test_color_pivot_total-weekly_chess_player_metrics ................. [[32mPASS[0m in 1.14s]
[0m14:15:17.272849 [debug] [Thread-3 (]: Finished running node test.pipeline.test_color_pivot_total-weekly_chess_player_metrics
[0m14:15:17.273479 [debug] [Thread-3 (]: Began running node test.pipeline.test_game_count-stg__player_games
[0m14:15:17.274135 [info ] [Thread-3 (]: 5 of 6 START test test_game_count-stg__player_games ............................ [RUN]
[0m14:15:17.274790 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.pipeline.test_color_pivot_total-weekly_chess_player_metrics, now test.pipeline.test_game_count-stg__player_games)
[0m14:15:17.275385 [debug] [Thread-3 (]: Began compiling node test.pipeline.test_game_count-stg__player_games
[0m14:15:17.280976 [debug] [Thread-3 (]: Writing injected SQL for node "test.pipeline.test_game_count-stg__player_games"
[0m14:15:17.281816 [debug] [Thread-3 (]: Began executing node test.pipeline.test_game_count-stg__player_games
[0m14:15:17.285345 [debug] [Thread-3 (]: Writing runtime sql for node "test.pipeline.test_game_count-stg__player_games"
[0m14:15:17.286310 [debug] [Thread-3 (]: On test.pipeline.test_game_count-stg__player_games: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-stg__player_games"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')

EXCEPT DISTINCT

SELECT
    CAST(COUNT(*)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_staging`.`stg__player_games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')
      
    ) dbt_internal_test
[0m14:15:17.286954 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:15:17.293372 [info ] [Thread-4 (]: 4 of 6 PASS test_game_count-quarterly_chess_player_metrics ..................... [[32mPASS[0m in 1.16s]
[0m14:15:17.294402 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-quarterly_chess_player_metrics
[0m14:15:17.295071 [debug] [Thread-4 (]: Began running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:15:17.295783 [info ] [Thread-4 (]: 6 of 6 START test test_game_count-weekly_chess_player_metrics .................. [RUN]
[0m14:15:17.296469 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.pipeline.test_game_count-quarterly_chess_player_metrics, now test.pipeline.test_game_count-weekly_chess_player_metrics)
[0m14:15:17.297072 [debug] [Thread-4 (]: Began compiling node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:15:17.303236 [debug] [Thread-4 (]: Writing injected SQL for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m14:15:17.304280 [debug] [Thread-4 (]: Began executing node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:15:17.309384 [debug] [Thread-4 (]: Writing runtime sql for node "test.pipeline.test_game_count-weekly_chess_player_metrics"
[0m14:15:17.310476 [debug] [Thread-4 (]: On test.pipeline.test_game_count-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.9.4", "profile_name": "pipeline", "target_name": "dev", "node_id": "test.pipeline.test_game_count-weekly_chess_player_metrics"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
      
    ) dbt_internal_test
[0m14:15:17.311216 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:15:17.418933 [info ] [Thread-2 (]: 2 of 6 PASS test_color_pivot_total-quarterly_chess_player_metrics .............. [[32mPASS[0m in 1.28s]
[0m14:15:17.419937 [debug] [Thread-2 (]: Finished running node test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics
[0m14:15:17.752961 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:ef93c61d-3fd1-484f-b1ba-24c2662c362d&page=queryresults
[0m14:15:17.886033 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:7437bd2d-1fd9-45b3-a78e-65a2675755d6&page=queryresults
[0m14:15:18.177241 [info ] [Thread-1 (]: 1 of 6 PASS source_unique_chess_raw_games_game_id .............................. [[32mPASS[0m in 2.04s]
[0m14:15:18.178246 [debug] [Thread-1 (]: Finished running node test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13
[0m14:15:18.214137 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:705f27b9-03fb-4a79-ad64-df98a402443d&page=queryresults
[0m14:15:18.331775 [info ] [Thread-3 (]: 5 of 6 PASS test_game_count-stg__player_games .................................. [[32mPASS[0m in 1.06s]
[0m14:15:18.332781 [debug] [Thread-3 (]: Finished running node test.pipeline.test_game_count-stg__player_games
[0m14:15:18.604782 [info ] [Thread-4 (]: 6 of 6 PASS test_game_count-weekly_chess_player_metrics ........................ [[32mPASS[0m in 1.31s]
[0m14:15:18.605777 [debug] [Thread-4 (]: Finished running node test.pipeline.test_game_count-weekly_chess_player_metrics
[0m14:15:18.607843 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:15:18.608727 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:15:18.609170 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-stg__player_games' was properly closed.
[0m14:15:18.609614 [debug] [MainThread]: Connection 'test.pipeline.source_unique_chess_raw_games_game_id.3c388e3e13' was properly closed.
[0m14:15:18.610036 [debug] [MainThread]: Connection 'test.pipeline.test_color_pivot_total-quarterly_chess_player_metrics' was properly closed.
[0m14:15:18.610467 [debug] [MainThread]: Connection 'test.pipeline.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m14:15:18.610951 [info ] [MainThread]: 
[0m14:15:18.611474 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 3.32 seconds (3.32s).
[0m14:15:18.613605 [debug] [MainThread]: Command end result
[0m14:15:18.651709 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/manifest.json
[0m14:15:18.653734 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/chess_analysis/dbt/pipeline/target/semantic_manifest.json
[0m14:15:18.661391 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/chess_analysis/dbt/pipeline/target/run_results.json
[0m14:15:18.661644 [info ] [MainThread]: 
[0m14:15:18.661939 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:15:18.662182 [info ] [MainThread]: 
[0m14:15:18.662447 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m14:15:18.662955 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 5.039287, "process_in_blocks": "0", "process_kernel_time": 0.350689, "process_mem_max_rss": "238428", "process_out_blocks": "3576", "process_user_time": 3.970478}
[0m14:15:18.663306 [debug] [MainThread]: Command `dbt test` succeeded at 14:15:18.663217 after 5.04 seconds
[0m14:15:18.663590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f781ffd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f781fffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5f77f77d90>]}
[0m14:15:18.663885 [debug] [MainThread]: Flushing usage events
[0m14:15:19.031910 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:33:35.127854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc98facfd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc99034ef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc98facc950>]}


============================== 17:33:35.163551 | 54092368-658b-4eb8-9002-4db50212f3eb ==============================
[0m17:33:35.163551 [info ] [MainThread]: Running with dbt=1.10.5
[0m17:33:35.163999 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/dbt_chess/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:33:35.166484 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'dbt_chess'
[0m17:33:35.167056 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.09427225, "process_in_blocks": "25304", "process_kernel_time": 0.262815, "process_mem_max_rss": "127916", "process_out_blocks": "33200", "process_user_time": 2.647945}
[0m17:33:35.167432 [debug] [MainThread]: Command `dbt test` failed at 17:33:35.167350 after 0.09 seconds
[0m17:33:35.167703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc990169640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc98f7fe1e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc98fb49760>]}
[0m17:33:35.167993 [debug] [MainThread]: Flushing usage events
[0m17:33:35.523610 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:35:52.232238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb16231b710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb161b66fc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1613b2a80>]}


============================== 17:35:52.235943 | 5082328c-9c58-4613-a101-8b0b9d535d06 ==============================
[0m17:35:52.235943 [info ] [MainThread]: Running with dbt=1.10.5
[0m17:35:52.236410 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/filpill/projects/dbt_chess/logs', 'debug': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:36:01.495983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb163867320>]}
[0m17:36:01.551201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1239c7740>]}
[0m17:36:01.551739 [info ] [MainThread]: Registered adapter: bigquery=1.10.0
[0m17:36:01.758950 [debug] [MainThread]: checksum: fa4c2685620973c5adba0a491aa9c06e8cf48c9113a5280b45b6efa706ae8f4b, vars: {}, profile: , target: , version: 1.10.5
[0m17:36:01.875140 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:36:01.875561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb12362d6d0>]}
[0m17:36:03.510096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1231a9e20>]}
[0m17:36:03.606767 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/dbt_chess/target/manifest.json
[0m17:36:03.608279 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/dbt_chess/target/semantic_manifest.json
[0m17:36:03.778621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb122fab770>]}
[0m17:36:03.779048 [info ] [MainThread]: Found 4 models, 6 data tests, 1 seed, 1 source, 495 macros
[0m17:36:03.779374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb123173c50>]}
[0m17:36:03.781044 [info ] [MainThread]: 
[0m17:36:03.781362 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:36:03.781624 [info ] [MainThread]: 
[0m17:36:03.782039 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:36:03.785911 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m17:36:03.786471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:03.787300 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m17:36:03.787927 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:03.788631 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m17:36:03.789389 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:05.936210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5082328c-9c58-4613-a101-8b0b9d535d06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb163745070>]}
[0m17:36:05.936892 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:36:05.942680 [debug] [Thread-1 (]: Began running node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:05.943365 [debug] [Thread-2 (]: Began running node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:05.944439 [debug] [Thread-3 (]: Began running node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:05.943877 [info ] [Thread-1 (]: 1 of 6 START test source_unique_chess_raw_games_game_id ........................ [RUN]
[0m17:36:05.945621 [debug] [Thread-4 (]: Began running node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:05.945021 [info ] [Thread-2 (]: 2 of 6 START test test_color_pivot_total-quarterly_chess_player_metrics ........ [RUN]
[0m17:36:05.946413 [info ] [Thread-3 (]: 3 of 6 START test test_color_pivot_total-weekly_chess_player_metrics ........... [RUN]
[0m17:36:05.947160 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13)
[0m17:36:05.947795 [info ] [Thread-4 (]: 4 of 6 START test test_game_count-quarterly_chess_player_metrics ............... [RUN]
[0m17:36:05.948408 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics)
[0m17:36:05.948907 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics)
[0m17:36:05.949556 [debug] [Thread-1 (]: Began compiling node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:05.950312 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.dbt_chess.test_game_count-quarterly_chess_player_metrics'
[0m17:36:05.950829 [debug] [Thread-2 (]: Began compiling node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:05.951310 [debug] [Thread-3 (]: Began compiling node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:05.962111 [debug] [Thread-4 (]: Began compiling node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:05.975601 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13"
[0m17:36:05.980157 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:36:05.984921 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:36:05.992565 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_chess.test_game_count-quarterly_chess_player_metrics"
[0m17:36:05.993877 [debug] [Thread-2 (]: Began executing node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:05.999631 [debug] [Thread-4 (]: Began executing node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:06.015525 [debug] [Thread-3 (]: Began executing node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:06.095173 [debug] [Thread-1 (]: Began executing node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:06.104481 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:36:06.105396 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:36:06.106108 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_chess.test_game_count-quarterly_chess_player_metrics"
[0m17:36:06.110404 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13"
[0m17:36:06.112050 [debug] [Thread-2 (]: On test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
  
  
      
    ) dbt_internal_test
[0m17:36:06.112627 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:36:06.114390 [debug] [Thread-3 (]: On test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
  
  
      
    ) dbt_internal_test
[0m17:36:06.115057 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:36:06.117049 [debug] [Thread-4 (]: On test.dbt_chess.test_game_count-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_game_count-quarterly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
WHERE 1=1
  AND quarter_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  
  
      
    ) dbt_internal_test
[0m17:36:06.117717 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m17:36:06.119648 [debug] [Thread-1 (]: On test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select game_id as unique_field
  from `checkmate-453316`.`chess_raw`.`games`
  where game_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m17:36:06.120309 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:36:06.821548 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:f2a57b20-e0c6-499e-ac05-f7086907b76a&page=queryresults
[0m17:36:06.824373 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:26b39f4d-585d-4648-9424-49689d4b340f&page=queryresults
[0m17:36:07.172048 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d15e167e-1728-4031-a67c-5133365bb076&page=queryresults
[0m17:36:07.329230 [info ] [Thread-2 (]: 2 of 6 PASS test_color_pivot_total-quarterly_chess_player_metrics .............. [[32mPASS[0m in 1.38s]
[0m17:36:07.331093 [debug] [Thread-2 (]: Finished running node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:07.331886 [debug] [Thread-2 (]: Began running node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:07.332685 [info ] [Thread-2 (]: 5 of 6 START test test_game_count-stg__player_games ............................ [RUN]
[0m17:36:07.334764 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics, now test.dbt_chess.test_game_count-stg__player_games)
[0m17:36:07.335438 [debug] [Thread-2 (]: Began compiling node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:07.342195 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_chess.test_game_count-stg__player_games"
[0m17:36:07.343094 [debug] [Thread-2 (]: Began executing node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:07.347033 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_chess.test_game_count-stg__player_games"
[0m17:36:07.347969 [debug] [Thread-2 (]: On test.dbt_chess.test_game_count-stg__player_games: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_game_count-stg__player_games"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')

EXCEPT DISTINCT

SELECT
    CAST(COUNT(*)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_staging`.`stg__player_games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')
  
  
      
    ) dbt_internal_test
[0m17:36:07.348646 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:36:07.383806 [info ] [Thread-3 (]: 3 of 6 PASS test_color_pivot_total-weekly_chess_player_metrics ................. [[32mPASS[0m in 1.43s]
[0m17:36:07.384969 [debug] [Thread-3 (]: Finished running node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:07.385671 [debug] [Thread-3 (]: Began running node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:07.386306 [info ] [Thread-3 (]: 6 of 6 START test test_game_count-weekly_chess_player_metrics .................. [RUN]
[0m17:36:07.387189 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics, now test.dbt_chess.test_game_count-weekly_chess_player_metrics)
[0m17:36:07.387796 [debug] [Thread-3 (]: Began compiling node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:07.394165 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_chess.test_game_count-weekly_chess_player_metrics"
[0m17:36:07.395111 [debug] [Thread-3 (]: Began executing node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:07.399238 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_chess.test_game_count-weekly_chess_player_metrics"
[0m17:36:07.400250 [debug] [Thread-3 (]: On test.dbt_chess.test_game_count-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_game_count-weekly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  
  
      
    ) dbt_internal_test
[0m17:36:07.401027 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:36:07.591801 [info ] [Thread-4 (]: 4 of 6 PASS test_game_count-quarterly_chess_player_metrics ..................... [[32mPASS[0m in 1.64s]
[0m17:36:07.592668 [debug] [Thread-4 (]: Finished running node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:07.619036 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:279d538d-e6e7-452d-ada5-7e80eaa4480d&page=queryresults
[0m17:36:08.022273 [info ] [Thread-1 (]: 1 of 6 PASS source_unique_chess_raw_games_game_id .............................. [[32mPASS[0m in 2.07s]
[0m17:36:08.023519 [debug] [Thread-1 (]: Finished running node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:08.271979 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:af99dc9d-186e-481d-a77c-622e1ceed48c&page=queryresults
[0m17:36:08.333632 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:1ad4f45a-c58e-4a7b-b850-e9305356deb3&page=queryresults
[0m17:36:08.692658 [info ] [Thread-3 (]: 6 of 6 PASS test_game_count-weekly_chess_player_metrics ........................ [[32mPASS[0m in 1.31s]
[0m17:36:08.693738 [debug] [Thread-3 (]: Finished running node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:08.730365 [info ] [Thread-2 (]: 5 of 6 PASS test_game_count-stg__player_games .................................. [[32mPASS[0m in 1.40s]
[0m17:36:08.731440 [debug] [Thread-2 (]: Finished running node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:08.733471 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:36:08.734398 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:36:08.734911 [debug] [MainThread]: Connection 'test.dbt_chess.test_game_count-stg__player_games' was properly closed.
[0m17:36:08.735419 [debug] [MainThread]: Connection 'test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13' was properly closed.
[0m17:36:08.735897 [debug] [MainThread]: Connection 'test.dbt_chess.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m17:36:08.736398 [debug] [MainThread]: Connection 'test.dbt_chess.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m17:36:08.736936 [info ] [MainThread]: 
[0m17:36:08.737514 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 4.95 seconds (4.95s).
[0m17:36:08.743297 [debug] [MainThread]: Command end result
[0m17:36:08.788449 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/dbt_chess/target/manifest.json
[0m17:36:08.790104 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/dbt_chess/target/semantic_manifest.json
[0m17:36:08.796130 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/dbt_chess/target/run_results.json
[0m17:36:08.796454 [info ] [MainThread]: 
[0m17:36:08.796766 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:36:08.797024 [info ] [MainThread]: 
[0m17:36:08.797315 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m17:36:08.797893 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 16.622337, "process_in_blocks": "96048", "process_kernel_time": 0.797639, "process_mem_max_rss": "439332", "process_out_blocks": "179416", "process_user_time": 13.434796}
[0m17:36:08.798267 [debug] [MainThread]: Command `dbt test` succeeded at 17:36:08.798182 after 16.62 seconds
[0m17:36:08.798560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb163a7e810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1620333b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb162032e70>]}
[0m17:36:08.798855 [debug] [MainThread]: Flushing usage events
[0m17:36:09.161459 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:36:15.460535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f466052f200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f466068acf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f466068b320>]}


============================== 17:36:15.464030 | 4e40104d-9316-45a4-b6d2-573a1a6fbabc ==============================
[0m17:36:15.464030 [info ] [MainThread]: Running with dbt=1.10.5
[0m17:36:15.464477 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/filpill/projects/dbt_chess/logs', 'fail_fast': 'False', 'profiles_dir': '/home/filpill/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:36:17.606128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e40104d-9316-45a4-b6d2-573a1a6fbabc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4622be8a10>]}
[0m17:36:17.669588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e40104d-9316-45a4-b6d2-573a1a6fbabc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4660912660>]}
[0m17:36:17.670188 [info ] [MainThread]: Registered adapter: bigquery=1.10.0
[0m17:36:17.888258 [debug] [MainThread]: checksum: fa4c2685620973c5adba0a491aa9c06e8cf48c9113a5280b45b6efa706ae8f4b, vars: {}, profile: , target: , version: 1.10.5
[0m17:36:18.035561 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:36:18.035929 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:36:18.100186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e40104d-9316-45a4-b6d2-573a1a6fbabc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4622554bc0>]}
[0m17:36:18.346533 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/dbt_chess/target/manifest.json
[0m17:36:18.348104 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/dbt_chess/target/semantic_manifest.json
[0m17:36:18.367194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e40104d-9316-45a4-b6d2-573a1a6fbabc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46228eacf0>]}
[0m17:36:18.367605 [info ] [MainThread]: Found 4 models, 6 data tests, 1 seed, 1 source, 495 macros
[0m17:36:18.367895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e40104d-9316-45a4-b6d2-573a1a6fbabc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4660e7ad80>]}
[0m17:36:18.369557 [info ] [MainThread]: 
[0m17:36:18.369851 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:36:18.370098 [info ] [MainThread]: 
[0m17:36:18.370508 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:36:18.374570 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_universal'
[0m17:36:18.375325 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:18.376063 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_marts'
[0m17:36:18.376827 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:18.377519 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316_dev_staging'
[0m17:36:18.378044 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:19.190719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e40104d-9316-45a4-b6d2-573a1a6fbabc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4622745cd0>]}
[0m17:36:19.191468 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:36:19.195070 [debug] [Thread-1 (]: Began running node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:19.195789 [debug] [Thread-2 (]: Began running node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:19.196922 [debug] [Thread-3 (]: Began running node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:19.197485 [debug] [Thread-4 (]: Began running node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:19.196396 [info ] [Thread-1 (]: 1 of 6 START test source_unique_chess_raw_games_game_id ........................ [RUN]
[0m17:36:19.198032 [info ] [Thread-2 (]: 2 of 6 START test test_color_pivot_total-quarterly_chess_player_metrics ........ [RUN]
[0m17:36:19.198636 [info ] [Thread-3 (]: 3 of 6 START test test_color_pivot_total-weekly_chess_player_metrics ........... [RUN]
[0m17:36:19.199664 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13)
[0m17:36:19.199043 [info ] [Thread-4 (]: 4 of 6 START test test_game_count-quarterly_chess_player_metrics ............... [RUN]
[0m17:36:19.200351 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics)
[0m17:36:19.200903 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics)
[0m17:36:19.201436 [debug] [Thread-1 (]: Began compiling node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:19.201983 [debug] [Thread-4 (]: Acquiring new bigquery connection 'test.dbt_chess.test_game_count-quarterly_chess_player_metrics'
[0m17:36:19.202402 [debug] [Thread-2 (]: Began compiling node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:19.202931 [debug] [Thread-3 (]: Began compiling node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:19.213591 [debug] [Thread-4 (]: Began compiling node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:19.230778 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:36:19.233854 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13"
[0m17:36:19.238405 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:36:19.244295 [debug] [Thread-4 (]: Writing injected SQL for node "test.dbt_chess.test_game_count-quarterly_chess_player_metrics"
[0m17:36:19.245107 [debug] [Thread-1 (]: Began executing node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:19.266171 [debug] [Thread-3 (]: Began executing node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:19.278174 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics"
[0m17:36:19.278849 [debug] [Thread-2 (]: Began executing node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:19.279460 [debug] [Thread-4 (]: Began executing node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:19.281095 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13"
[0m17:36:19.284717 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics"
[0m17:36:19.288487 [debug] [Thread-4 (]: Writing runtime sql for node "test.dbt_chess.test_game_count-quarterly_chess_player_metrics"
[0m17:36:19.289622 [debug] [Thread-3 (]: On test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
  
  
      
    ) dbt_internal_test
[0m17:36:19.290354 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:36:19.290880 [debug] [Thread-1 (]: On test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with dbt_test__target as (

  select game_id as unique_field
  from `checkmate-453316`.`chess_raw`.`games`
  where game_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m17:36:19.291393 [debug] [Thread-4 (]: On test.dbt_chess.test_game_count-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_game_count-quarterly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
WHERE 1=1
  AND quarter_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  
  
      
    ) dbt_internal_test
[0m17:36:19.293523 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:36:19.294034 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m17:36:19.295618 [debug] [Thread-2 (]: On test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
  week_start_date,
  username,
  time_class
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), QUARTER)
  AND total_games != (white_win_count + white_loss_count + white_draw_count + black_win_count + black_loss_count + black_draw_count)
  
  
      
    ) dbt_internal_test
[0m17:36:19.297243 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:36:19.781723 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:6c22b374-b2b3-4cea-9f81-4dda5824d169&page=queryresults
[0m17:36:19.784345 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:bbeb06e5-114f-48f4-ae19-a2bcea8bf440&page=queryresults
[0m17:36:19.837918 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:50aa67d2-f8fd-4fb0-8526-277480858d51&page=queryresults
[0m17:36:19.893861 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:32a752ee-ddbb-4c7c-9c13-5205e6e019bf&page=queryresults
[0m17:36:20.185246 [info ] [Thread-3 (]: 3 of 6 PASS test_color_pivot_total-weekly_chess_player_metrics ................. [[32mPASS[0m in 0.98s]
[0m17:36:20.186961 [debug] [Thread-3 (]: Finished running node test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics
[0m17:36:20.187706 [debug] [Thread-3 (]: Began running node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:20.188313 [info ] [Thread-3 (]: 5 of 6 START test test_game_count-stg__player_games ............................ [RUN]
[0m17:36:20.189723 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.dbt_chess.test_color_pivot_total-weekly_chess_player_metrics, now test.dbt_chess.test_game_count-stg__player_games)
[0m17:36:20.190313 [debug] [Thread-3 (]: Began compiling node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:20.197301 [debug] [Thread-3 (]: Writing injected SQL for node "test.dbt_chess.test_game_count-stg__player_games"
[0m17:36:20.191212 [info ] [Thread-2 (]: 2 of 6 PASS test_color_pivot_total-quarterly_chess_player_metrics .............. [[32mPASS[0m in 0.99s]
[0m17:36:20.198526 [debug] [Thread-2 (]: Finished running node test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics
[0m17:36:20.199200 [debug] [Thread-2 (]: Began running node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:20.199783 [info ] [Thread-2 (]: 6 of 6 START test test_game_count-weekly_chess_player_metrics .................. [RUN]
[0m17:36:20.200616 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.dbt_chess.test_color_pivot_total-quarterly_chess_player_metrics, now test.dbt_chess.test_game_count-weekly_chess_player_metrics)
[0m17:36:20.201183 [debug] [Thread-2 (]: Began compiling node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:20.201782 [debug] [Thread-3 (]: Began executing node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:20.207611 [debug] [Thread-2 (]: Writing injected SQL for node "test.dbt_chess.test_game_count-weekly_chess_player_metrics"
[0m17:36:20.211199 [debug] [Thread-3 (]: Writing runtime sql for node "test.dbt_chess.test_game_count-stg__player_games"
[0m17:36:20.212192 [debug] [Thread-2 (]: Began executing node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:20.212873 [debug] [Thread-3 (]: On test.dbt_chess.test_game_count-stg__player_games: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_game_count-stg__player_games"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')

EXCEPT DISTINCT

SELECT
    CAST(COUNT(*)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_staging`.`stg__player_games`
WHERE 1=1
  AND game_date >= DATE('2025-05-01')
  
  
      
    ) dbt_internal_test
[0m17:36:20.216960 [debug] [Thread-2 (]: Writing runtime sql for node "test.dbt_chess.test_game_count-weekly_chess_player_metrics"
[0m17:36:20.218280 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:36:20.221580 [info ] [Thread-4 (]: 4 of 6 PASS test_game_count-quarterly_chess_player_metrics ..................... [[32mPASS[0m in 1.02s]
[0m17:36:20.222644 [debug] [Thread-4 (]: Finished running node test.dbt_chess.test_game_count-quarterly_chess_player_metrics
[0m17:36:20.223620 [debug] [Thread-2 (]: On test.dbt_chess.test_game_count-weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "test.dbt_chess.test_game_count-weekly_chess_player_metrics"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  SELECT
    COUNT(*) AS raw_count
FROM `checkmate-453316`.`chess_raw`.`games`
WHERE 1=1
  AND game_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  AND rated = TRUE
  AND rules = "chess"

EXCEPT DISTINCT

SELECT
    CAST(SUM(total_games)/2 AS INT64) AS agg_count
FROM `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
WHERE 1=1
  AND week_start_date >= DATE_TRUNC(DATE('2025-05-01'), ISOWEEK)
  
  
      
    ) dbt_internal_test
[0m17:36:20.224343 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:36:20.328017 [info ] [Thread-1 (]: 1 of 6 PASS source_unique_chess_raw_games_game_id .............................. [[32mPASS[0m in 1.13s]
[0m17:36:20.329078 [debug] [Thread-1 (]: Finished running node test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13
[0m17:36:20.839633 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:c0cbdc7a-1ff6-4dc5-92c8-5de214d20cc9&page=queryresults
[0m17:36:20.867493 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:8354f818-7979-4ad6-8501-59314324eb10&page=queryresults
[0m17:36:21.185954 [info ] [Thread-3 (]: 5 of 6 PASS test_game_count-stg__player_games .................................. [[32mPASS[0m in 1.00s]
[0m17:36:21.187046 [debug] [Thread-3 (]: Finished running node test.dbt_chess.test_game_count-stg__player_games
[0m17:36:21.210102 [info ] [Thread-2 (]: 6 of 6 PASS test_game_count-weekly_chess_player_metrics ........................ [[32mPASS[0m in 1.01s]
[0m17:36:21.210702 [debug] [Thread-2 (]: Finished running node test.dbt_chess.test_game_count-weekly_chess_player_metrics
[0m17:36:21.212366 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:36:21.213282 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:36:21.213753 [debug] [MainThread]: Connection 'test.dbt_chess.test_game_count-weekly_chess_player_metrics' was properly closed.
[0m17:36:21.214204 [debug] [MainThread]: Connection 'test.dbt_chess.source_unique_chess_raw_games_game_id.3c388e3e13' was properly closed.
[0m17:36:21.214735 [debug] [MainThread]: Connection 'test.dbt_chess.test_game_count-stg__player_games' was properly closed.
[0m17:36:21.215204 [debug] [MainThread]: Connection 'test.dbt_chess.test_game_count-quarterly_chess_player_metrics' was properly closed.
[0m17:36:21.215730 [info ] [MainThread]: 
[0m17:36:21.216295 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 2.85 seconds (2.85s).
[0m17:36:21.219405 [debug] [MainThread]: Command end result
[0m17:36:21.265557 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/dbt_chess/target/manifest.json
[0m17:36:21.267061 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/dbt_chess/target/semantic_manifest.json
[0m17:36:21.272944 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/dbt_chess/target/run_results.json
[0m17:36:21.273279 [info ] [MainThread]: 
[0m17:36:21.273587 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:36:21.273841 [info ] [MainThread]: 
[0m17:36:21.274117 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m17:36:21.274687 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 5.870868, "process_in_blocks": "0", "process_kernel_time": 0.549951, "process_mem_max_rss": "387312", "process_out_blocks": "2496", "process_user_time": 5.206513}
[0m17:36:21.275041 [debug] [MainThread]: Command `dbt test` succeeded at 17:36:21.274961 after 5.87 seconds
[0m17:36:21.275331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4660e9e5a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46255406b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f462277c050>]}
[0m17:36:21.275617 [debug] [MainThread]: Flushing usage events
[0m17:36:22.149599 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:36:28.855742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4798932f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc479b6acf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc479890f20>]}


============================== 17:36:28.859540 | f221f068-8ec0-40fd-b9e1-96861355063b ==============================
[0m17:36:28.859540 [info ] [MainThread]: Running with dbt=1.10.5
[0m17:36:28.860027 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/filpill/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/filpill/projects/dbt_chess/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:36:30.907741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43bdb8770>]}
[0m17:36:30.969523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc47b2ae4b0>]}
[0m17:36:30.970108 [info ] [MainThread]: Registered adapter: bigquery=1.10.0
[0m17:36:31.180553 [debug] [MainThread]: checksum: fa4c2685620973c5adba0a491aa9c06e8cf48c9113a5280b45b6efa706ae8f4b, vars: {}, profile: , target: , version: 1.10.5
[0m17:36:31.327278 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:36:31.327643 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:36:31.389596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43bca2ba0>]}
[0m17:36:31.633307 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/dbt_chess/target/manifest.json
[0m17:36:31.635028 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/dbt_chess/target/semantic_manifest.json
[0m17:36:31.647140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43bdbb650>]}
[0m17:36:31.647584 [info ] [MainThread]: Found 4 models, 6 data tests, 1 seed, 1 source, 495 macros
[0m17:36:31.647876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43c272870>]}
[0m17:36:31.649495 [info ] [MainThread]: 
[0m17:36:31.649803 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:36:31.650063 [info ] [MainThread]: 
[0m17:36:31.650500 [debug] [MainThread]: Acquiring new bigquery connection 'master'
[0m17:36:31.654789 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m17:36:31.655573 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:31.656426 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m17:36:31.657063 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:31.657865 [debug] [ThreadPool]: Acquiring new bigquery connection 'list_checkmate-453316'
[0m17:36:31.658377 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:32.727265 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_universal)
[0m17:36:32.728169 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_marts)
[0m17:36:32.728715 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:36:32.729578 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_checkmate-453316, now list_checkmate-453316_dev_staging)
[0m17:36:32.730169 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:36:32.731968 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:36:32.918897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43b816ae0>]}
[0m17:36:32.919604 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:36:32.922943 [debug] [Thread-1 (]: Began running node model.dbt_chess.calendar
[0m17:36:32.923505 [debug] [Thread-2 (]: Began running node model.dbt_chess.stg__player_games
[0m17:36:32.924353 [info ] [Thread-1 (]: 1 of 4 START sql table model dev_universal.calendar ............................ [RUN]
[0m17:36:32.925199 [info ] [Thread-2 (]: 2 of 4 START sql incremental model dev_staging.stg__player_games ............... [RUN]
[0m17:36:32.925937 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_marts, now model.dbt_chess.calendar)
[0m17:36:32.926570 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_staging, now model.dbt_chess.stg__player_games)
[0m17:36:32.927165 [debug] [Thread-1 (]: Began compiling node model.dbt_chess.calendar
[0m17:36:32.927747 [debug] [Thread-2 (]: Began compiling node model.dbt_chess.stg__player_games
[0m17:36:32.950337 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_chess.calendar"
[0m17:36:32.964985 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_chess.stg__player_games"
[0m17:36:32.965898 [debug] [Thread-2 (]: Began executing node model.dbt_chess.stg__player_games
[0m17:36:32.966516 [debug] [Thread-1 (]: Began executing node model.dbt_chess.calendar
[0m17:36:33.031918 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:36:33.058381 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m17:36:33.242569 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_chess.calendar"
[0m17:36:33.254120 [debug] [Thread-1 (]: On model.dbt_chess.calendar: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "model.dbt_chess.calendar"} */

  
    

    create or replace table `checkmate-453316`.`dev_universal`.`calendar`
      
    
    

    
    OPTIONS(
      description="""Calendar table with wide-combination of values and formats accounting for days/weeks/months/quarters/years"""
    )
    as (
      


WITH cte_date_array AS (
    SELECT
      DATE_SUB(CURRENT_DATE(), INTERVAL x DAY) AS cal_date
    FROM
      UNNEST(GENERATE_ARRAY(0, 365 *10)) AS x
),

cte_apply_formatting AS (
    SELECT

          /* Extracting Basic Attibutes */
          cal_date
        , EXTRACT(DAY          FROM cal_date)                                                   AS day
        , EXTRACT(WEEK(SUNDAY) FROM cal_date)                                                   AS week
        , EXTRACT(ISOWEEK      FROM cal_date)                                                   AS iso_week
        , EXTRACT(MONTH        FROM cal_date)                                                   AS month
        , EXTRACT(QUARTER      FROM cal_date)                                                   AS quarter
        , EXTRACT(YEAR         FROM cal_date)                                                   AS year
        , FORMAT_DATE('%B', cal_date)                                                           AS month_name

        /*Quarterly Format*/
        , FORMAT_DATE('%Y-Q%Q', cal_date)                                                       AS year_quarter
        , DATE_TRUNC(cal_date, QUARTER)                                                         AS quarter_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 QUARTER), QUARTER),
              INTERVAL 1 DAY
          )                                                                                     AS quarter_end_date

        /* Monthly Formats */
        , DATE_TRUNC(cal_date, MONTH)                                                           AS month_start_date
        , DATE_SUB(
              DATE_TRUNC(DATE_ADD(cal_date, INTERVAL 1 MONTH), MONTH),
              INTERVAL 1 DAY
          )                                                                                     AS month_end_date
        , FORMAT_DATE('%b-%y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_short
        , FORMAT_DATE('%B %Y', DATE_TRUNC(cal_date, MONTH))                                     AS month_year_full

          /* Weekly Formats (ISO) - Mon to Sun */
        , "Week " || EXTRACT(ISOWEEK FROM cal_date)                                             AS iso_week_desc
        , DATE_TRUNC(cal_date, ISOWEEK)                                                         AS iso_week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, ISOWEEK), INTERVAL 6 DAY)                               AS iso_week_end_date

          /* Weekly Formats (Default) - Sun to Sat */
        , "Week " || EXTRACT(WEEK(SUNDAY)  FROM cal_date)                                       AS week_number_desc
        , DATE_TRUNC(cal_date, WEEK(SUNDAY))                                                    AS week_start_date
        , DATE_ADD(DATE_TRUNC(cal_date, WEEK(SUNDAY)), INTERVAL 6 DAY)                          AS week_end_date


        /* Yearly Boolean Date Flags */
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM CURRENT_DATE())                                                                           AS flag_current_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR))                                                AS flag_1st_previous_year
        , EXTRACT(YEAR FROM cal_date) = EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 YEAR))                                                AS flag_2nd_previous_year
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH) AND CURRENT_DATE()                                                           AS flag_current_last_12_months
        , cal_date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH) AND DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH)                              AS flag_previous_last_12_months


        /* Quarterly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_current_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), QUARTER), INTERVAL 1 DAY)                                                             AS flag_1st_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)                               AS flag_2nd_previous_quarter

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM CURRENT_DATE())                                                         AS flag_current_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 QUARTER))                           AS flag_1st_previous_quarter_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 QUARTER), QUARTER)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 QUARTER), QUARTER), INTERVAL 1 DAY)
                  AND EXTRACT(QUARTER FROM cal_date) = EXTRACT(QUARTER FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 QUARTER))                           AS flag_2nd_previous_quarter_last_year

        /* Monthly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)                                   AS flag_current_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH), MONTH) 
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)                                                               AS flag_1st_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH) 
                      AND LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                                                                      AS flag_2nd_previous_month

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM CURRENT_DATE())                                                         AS flag_current_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))                             AS flag_1st_previous_month_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 MONTH), MONTH)
                        AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), MONTH), INTERVAL 1 DAY)
                      AND EXTRACT(MONTH FROM cal_date) = EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH))                             AS flag_2nd_previous_month_last_year

        /* Weekly (Starting Sunday) Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_current_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK), INTERVAL 1 DAY)                                                                AS flag_1st_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)                                     AS flag_2nd_previous_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM CURRENT_DATE())                                                               AS flag_current_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                                    AS flag_1st_previous_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK)
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK), INTERVAL 1 DAY)
                  AND EXTRACT(WEEK FROM cal_date) = EXTRACT(WEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                                    AS flag_2nd_previous_week_last_year

        /* ISO Weekly Boolean Date Flags */
        , cal_date BETWEEN DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_current_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(CURRENT_DATE(), WEEK(MONDAY)), INTERVAL 1 DAY)                                                        AS flag_1st_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)                             AS flag_2nd_previous_iso_week

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM CURRENT_DATE())                                                         AS flag_current_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 WEEK))                              AS flag_1st_previous_iso_week_last_year

        , cal_date BETWEEN DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 2 WEEK), WEEK(MONDAY))
                      AND DATE_SUB(DATE_TRUNC(DATE_SUB(DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR), INTERVAL 1 WEEK), WEEK(MONDAY)), INTERVAL 1 DAY)
                  AND EXTRACT(ISOWEEK FROM cal_date) = EXTRACT(ISOWEEK FROM DATE_SUB(CURRENT_DATE(), INTERVAL 2 WEEK))                              AS flag_2nd_previous_iso_week_last_year

    FROM cte_date_array
)

SELECT * FROM cte_apply_formatting
ORDER BY cal_date DESC
    );
  
[0m17:36:33.297314 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_chess.stg__player_games"
[0m17:36:33.298119 [debug] [Thread-2 (]: On model.dbt_chess.stg__player_games: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "model.dbt_chess.stg__player_games"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_staging`.`stg__player_games`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      
    partition by game_date
    

    
    OPTIONS(
      description="""Stage model that transforms raw chess game data into a player-level view,  with one row per player per game (both white and black perspectives).  Includes standardizations for result classification (win/loss/draw),  basic game metadata, and parsed opening names. This model supports  incremental loads by game date for efficient backfills.\n""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_white_black_union AS (

    SELECT
        game_id,
        game_date,
        "white"                                                AS piece_color,
        white.username                                         AS username,
        white.rating                                           AS rating,
        white.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.white                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games` 

      UNION ALL

    SELECT
        game_id,
        game_date,
        "black"                                                AS piece_color,
        black.username                                         AS username,
        black.rating                                           AS rating,
        black.result                                           AS result,
        rated,
        time_class,
        time_control,
        rules,
        accuracies.black                                       AS accuracy,
        opening                                                AS opening_line,
    FROM `checkmate-453316`.`chess_raw`.`games`
)

SELECT
    t.game_id,
    t.game_date,
    t.username,
    t.rating,
    t.piece_color,
    t.time_class,
    t.rules,
    t.result                                               AS raw_result,
    t.rated,
    CASE
        WHEN t.result = "win"                 THEN "win"
        WHEN t.result = "timeout"             THEN "loss"
        WHEN t.result = "threecheck"          THEN "loss"
        WHEN t.result = "resigned"            THEN "loss"
        WHEN t.result = "kingofthehill"       THEN "loss"
        WHEN t.result = "checkmated"          THEN "loss"
        WHEN t.result = "bughousepartnerlose" THEN "loss"
        WHEN t.result = "abandoned"           THEN "loss"
        WHEN t.result = "timevsinsufficient"  THEN "draw"
        WHEN t.result = "stalemate"           THEN "draw"
        WHEN t.result = "repetition"          THEN "draw"
        WHEN t.result = "insufficient"        THEN "draw"
        WHEN t.result = "agreed"              THEN "draw"
        WHEN t.result = "50move"              THEN "draw"
    END                                                   AS win_loss_draw,
    t.opening_line                                        AS opening_line,
    TRIM(
        REGEXP_REPLACE(REGEXP_REPLACE(t.opening_line , r'\d.*$', ''), r'\.{3,}\s*$', '')
    )                                                     AS opening,
    t.accuracy,
FROM cte_white_black_union t

WHERE 1=1 
  
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(game_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_staging`.`stg__player_games` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.game_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)
    values
        (`game_id`, `game_date`, `username`, `rating`, `piece_color`, `time_class`, `rules`, `raw_result`, `rated`, `win_loss_draw`, `opening_line`, `opening`, `accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_staging`.`stg__player_games__dbt_tmp`

  


  

    
[0m17:36:33.445441 [debug] [Thread-2 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:d493921d-53e9-43af-b9f8-1347fb19ec9a&page=queryresults
[0m17:36:33.522787 [debug] [Thread-1 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:e8a2407a-8d36-452b-8b81-f5b0db419512&page=queryresults
[0m17:36:35.681407 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43ebfd040>]}
[0m17:36:35.682510 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dev_universal.calendar ....................... [[32mCREATE TABLE (3.7k rows, 0 processed)[0m in 2.75s]
[0m17:36:35.683568 [debug] [Thread-1 (]: Finished running node model.dbt_chess.calendar
[0m17:36:40.565231 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43b8a63f0>]}
[0m17:36:40.566256 [info ] [Thread-2 (]: 2 of 4 OK created sql incremental model dev_staging.stg__player_games .......... [[32mSCRIPT (11.6 MiB processed)[0m in 7.64s]
[0m17:36:40.567233 [debug] [Thread-2 (]: Finished running node model.dbt_chess.stg__player_games
[0m17:36:40.568451 [debug] [Thread-4 (]: Began running node model.dbt_chess.quarterly_chess_player_metrics
[0m17:36:40.569129 [debug] [Thread-3 (]: Began running node model.dbt_chess.weekly_chess_player_metrics
[0m17:36:40.569931 [info ] [Thread-4 (]: 3 of 4 START sql incremental model dev_marts.quarterly_chess_player_metrics .... [RUN]
[0m17:36:40.570799 [info ] [Thread-3 (]: 4 of 4 START sql incremental model dev_marts.weekly_chess_player_metrics ....... [RUN]
[0m17:36:40.571667 [debug] [Thread-4 (]: Acquiring new bigquery connection 'model.dbt_chess.quarterly_chess_player_metrics'
[0m17:36:40.572278 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly list_checkmate-453316_dev_universal, now model.dbt_chess.weekly_chess_player_metrics)
[0m17:36:40.572892 [debug] [Thread-4 (]: Began compiling node model.dbt_chess.quarterly_chess_player_metrics
[0m17:36:40.573559 [debug] [Thread-3 (]: Began compiling node model.dbt_chess.weekly_chess_player_metrics
[0m17:36:40.583975 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_chess.quarterly_chess_player_metrics"
[0m17:36:40.593138 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_chess.weekly_chess_player_metrics"
[0m17:36:40.594247 [debug] [Thread-4 (]: Began executing node model.dbt_chess.quarterly_chess_player_metrics
[0m17:36:40.597637 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m17:36:40.599628 [debug] [Thread-3 (]: Began executing node model.dbt_chess.weekly_chess_player_metrics
[0m17:36:40.606074 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m17:36:40.745840 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_chess.quarterly_chess_player_metrics"
[0m17:36:40.747282 [debug] [Thread-4 (]: On model.dbt_chess.quarterly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "model.dbt_chess.quarterly_chess_player_metrics"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      
    partition by quarter_start_date
    cluster by username, opening_archetype, time_class

    
    OPTIONS(
      description="""Aggregates player game statistics by quarter, time class, and opening archetype. This model provides breakdowns of game outcomes (win/loss/draw) and average accuracy by piece color (white/black) for rated standard chess games. It uses calendar and opening mapping tables to enrich the game data, and filters to only include recent games based on configurable date logic.\n""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.quarter_start_date,
            cal.year_quarter,
            t.username,
            t.piece_color,
            t.time_class,
            COALESCE(map.opening_archetype, "Mapping Failed")         AS opening_archetype,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      LEFT JOIN `checkmate-453316`.`dev_universal`.`opening_mapping` map
          ON t.opening = map.opening
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.quarter_start_date,
        wagg.year_quarter,
        wagg.username,
        wagg.time_class,
        wagg.opening_archetype,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      quarter_start_date,
      year_quarter,
      username,
      time_class,
      opening_archetype,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(quarter_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.quarter_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)
    values
        (`quarter_start_date`, `year_quarter`, `username`, `time_class`, `opening_archetype`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`quarterly_chess_player_metrics__dbt_tmp`

  


  

    
[0m17:36:40.765995 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_chess.weekly_chess_player_metrics"
[0m17:36:40.766990 [debug] [Thread-3 (]: On model.dbt_chess.weekly_chess_player_metrics: /* {"app": "dbt", "dbt_version": "1.10.5", "profile_name": "dbt_chess", "target_name": "dev", "node_id": "model.dbt_chess.weekly_chess_player_metrics"} */

   
      -- generated script to merge partitions into `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics`
      declare dbt_partitions_for_replacement array<date>;

      
      
       -- 1. create a temp table with model data
        
  
    

    create or replace table `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`
      
    partition by week_start_date
    cluster by username, time_class

    
    OPTIONS(
      description="""Weekly aggregate statistics of rated standard chess games by player, piece color, and time class. This model calculates performance metrics such as win/loss/draw counts, accuracy, and rating, separated by white and black games. It uses the calendar table to align games to ISO week start dates and is designed for incremental refresh with weekly partitioning and clustering.\n""",
    
      expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 hour)
    )
    as (
      



WITH cte_date_aggregate AS (
      SELECT
            cal.iso_week_start_date                                   AS week_start_date,
            cal.iso_week_desc                                         AS week_number,
            t.username,
            t.piece_color,
            t.time_class,
            AVG(t.rating)                                             AS avg_rating,
            AVG(t.accuracy)                                           AS avg_accuracy,
            SUM(CASE WHEN t.win_loss_draw = "win"  THEN 1 ELSE 0 END) AS win_count,
            SUM(CASE WHEN t.win_loss_draw = "loss" THEN 1 ELSE 0 END) AS loss_count,
            SUM(CASE WHEN t.win_loss_draw = "draw" THEN 1 ELSE 0 END) AS draw_count,
            COUNT(*)                                                  AS total_games
      FROM `checkmate-453316`.`dev_staging`.`stg__player_games` t
      LEFT JOIN `checkmate-453316`.`dev_universal`.`calendar` cal
        ON t.game_date = cal.cal_date
      WHERE 1=1
        
  
    AND t.game_date BETWEEN CURRENT_DATE() - 30 AND CURRENT_DATE()
  

        AND t.rated = TRUE
        AND t.rules = "chess"
      GROUP BY ALL
),

cte_pivot_piece_color AS (
    SELECT
        wagg.week_start_date,
        wagg.week_number,
        wagg.username,
        wagg.time_class,
        AVG(wagg.avg_rating)                                                            AS avg_rating,
        SUM(wagg.total_games)                                                           AS total_games,

        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.win_count  ELSE 0 END)       AS white_win_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.loss_count ELSE 0 END)       AS white_loss_count,
        SUM(CASE WHEN wagg.piece_color = "white" THEN wagg.draw_count ELSE 0 END)       AS white_draw_count,
        AVG(CASE WHEN wagg.piece_color = "white" THEN wagg.avg_accuracy ELSE NULL END)  AS white_accuracy,

        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.win_count  ELSE 0 END)       AS black_win_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.loss_count ELSE 0 END)       AS black_loss_count,
        SUM(CASE WHEN wagg.piece_color = "black" THEN wagg.draw_count ELSE 0 END)       AS black_draw_count,
        AVG(CASE WHEN wagg.piece_color = "black" THEN wagg.avg_accuracy ELSE NULL END)  AS black_accuracy,

    FROM cte_date_aggregate wagg
    GROUP BY ALL
)

SELECT
      week_start_date,
      week_number,
      username,
      time_class,
      avg_rating,
      total_games,
      white_win_count,
      white_loss_count,
      white_draw_count,
      white_accuracy,
      black_win_count,
      black_loss_count,
      black_draw_count,
      black_accuracy
FROM cte_pivot_piece_color
    );
  
      -- 2. define partitions to update
      set (dbt_partitions_for_replacement) = (
          select as struct
              -- IGNORE NULLS: this needs to be aligned to _dbt_max_partition, which ignores null
              array_agg(distinct date(week_start_date) IGNORE NULLS)
          from `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`
      );

      -- 3. run the merge statement
      

    merge into `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics` as DBT_INTERNAL_DEST
        using (
        select
        * from `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`
      ) as DBT_INTERNAL_SOURCE
        on FALSE

    when not matched by source
         and date(DBT_INTERNAL_DEST.week_start_date) in unnest(dbt_partitions_for_replacement) 
        then delete

    when not matched then insert
        (`week_start_date`, `week_number`, `username`, `time_class`, `avg_rating`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)
    values
        (`week_start_date`, `week_number`, `username`, `time_class`, `avg_rating`, `total_games`, `white_win_count`, `white_loss_count`, `white_draw_count`, `white_accuracy`, `black_win_count`, `black_loss_count`, `black_draw_count`, `black_accuracy`)

;

      -- 4. clean up the temp table
      drop table if exists `checkmate-453316`.`dev_marts`.`weekly_chess_player_metrics__dbt_tmp`

  


  

    
[0m17:36:40.923143 [debug] [Thread-3 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:48fd0d67-f04a-4e0e-b5c5-e7c02382dc64&page=queryresults
[0m17:36:41.027177 [debug] [Thread-4 (]: BigQuery adapter: https://console.cloud.google.com/bigquery?project=checkmate-453316&j=bq:EU:9dee52c5-d94a-488b-9ec9-d455482eeddb&page=queryresults
[0m17:36:47.670782 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43841c770>]}
[0m17:36:47.671782 [info ] [Thread-3 (]: 4 of 4 OK created sql incremental model dev_marts.weekly_chess_player_metrics .. [[32mSCRIPT (3.4 MiB processed)[0m in 7.10s]
[0m17:36:47.672755 [debug] [Thread-3 (]: Finished running node model.dbt_chess.weekly_chess_player_metrics
[0m17:36:48.523347 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f221f068-8ec0-40fd-b9e1-96861355063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43b8aa720>]}
[0m17:36:48.524345 [info ] [Thread-4 (]: 3 of 4 OK created sql incremental model dev_marts.quarterly_chess_player_metrics  [[32mSCRIPT (16.2 MiB processed)[0m in 7.95s]
[0m17:36:48.525944 [debug] [Thread-4 (]: Finished running node model.dbt_chess.quarterly_chess_player_metrics
[0m17:36:48.527863 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:36:48.528826 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:36:48.529308 [debug] [MainThread]: Connection 'model.dbt_chess.stg__player_games' was properly closed.
[0m17:36:48.529751 [debug] [MainThread]: Connection 'model.dbt_chess.weekly_chess_player_metrics' was properly closed.
[0m17:36:48.530194 [debug] [MainThread]: Connection 'model.dbt_chess.calendar' was properly closed.
[0m17:36:48.530632 [debug] [MainThread]: Connection 'model.dbt_chess.quarterly_chess_player_metrics' was properly closed.
[0m17:36:48.531199 [info ] [MainThread]: 
[0m17:36:48.531715 [info ] [MainThread]: Finished running 3 incremental models, 1 table model in 0 hours 0 minutes and 16.88 seconds (16.88s).
[0m17:36:48.533651 [debug] [MainThread]: Command end result
[0m17:36:48.578274 [debug] [MainThread]: Wrote artifact WritableManifest to /home/filpill/projects/dbt_chess/target/manifest.json
[0m17:36:48.580013 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/filpill/projects/dbt_chess/target/semantic_manifest.json
[0m17:36:48.585676 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/filpill/projects/dbt_chess/target/run_results.json
[0m17:36:48.585966 [info ] [MainThread]: 
[0m17:36:48.586302 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:36:48.586571 [info ] [MainThread]: 
[0m17:36:48.586871 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m17:36:48.587497 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.78875, "process_in_blocks": "0", "process_kernel_time": 0.485926, "process_mem_max_rss": "388612", "process_out_blocks": "2664", "process_user_time": 5.332213}
[0m17:36:48.587878 [debug] [MainThread]: Command `dbt run` succeeded at 17:36:48.587789 after 19.79 seconds
[0m17:36:48.588184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc479dcda00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc43863a6f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc47b6fce90>]}
[0m17:36:48.588489 [debug] [MainThread]: Flushing usage events
[0m17:36:49.059786 [debug] [MainThread]: An error was encountered while trying to flush usage events
